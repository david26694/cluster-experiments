{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#cluster-experiments","title":"cluster-experiments","text":"<p><code>cluster-experiments</code> is a comprehensive Python library for end-to-end A/B testing workflows, from experiment design to statistical analysis.</p>"},{"location":"index.html#what-is-cluster-experiments","title":"\ud83d\udcd6 What is cluster-experiments?","text":"<p><code>cluster-experiments</code> provides a complete toolkit for designing, running, and analyzing experiments, with particular strength in handling clustered randomization and complex experimental designs. Originally developed to address challenges in switchback experiments and scenarios with network effects where standard randomization isn't feasible, it has evolved into a general-purpose experimentation framework supporting both simple A/B tests and other randomization designs.</p>"},{"location":"index.html#why-cluster","title":"Why \"cluster\"?","text":"<p>The name reflects the library's origins in handling cluster-randomized experiments, where randomization happens at a group level (e.g., stores, cities, time periods) rather than at the individual level. This is critical when:</p> <ul> <li>Spillover/Network Effects: Treatment of one unit affects others (e.g., testing driver incentives in ride-sharing)</li> <li>Operational Constraints: You can't randomize individuals (e.g., testing restaurant menu changes)</li> <li>Switchback Designs: Treatment alternates over time periods within the same unit</li> </ul> <p>While the library is aimed at these scenarios, it's equally capable of handling standard A/B tests with individual-level randomization.</p>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#experiment-design","title":"Experiment Design","text":"Power Analysis &amp; Sample Size Calculation <ul> <li>Simulation-based (Monte Carlo) for any design complexity</li> <li>Analytical (CLT-based) for standard designs</li> <li>Minimum Detectable Effect (MDE) estimation</li> </ul> Multiple Experimental Designs <ul> <li>Standard A/B tests with individual randomization</li> <li>Cluster-randomized experiments</li> <li>Switchback/crossover experiments</li> <li>Stratified randomization</li> <li>Observational studies with Synthetic Control</li> </ul>"},{"location":"index.html#statistical-methods","title":"Statistical Methods","text":"Multiple Analysis Methods <ul> <li>OLS and Clustered OLS regression</li> <li>GEE (Generalized Estimating Equations)</li> <li>Mixed Linear Models (MLM)</li> <li>Delta Method for ratio metrics</li> <li>Synthetic Control for observational data</li> </ul> Variance Reduction Techniques <ul> <li>CUPED (Controlled-experiment Using Pre-Experiment Data)</li> <li>CUPAC (Control Using Predictions As Covariates)</li> <li>Covariate adjustment</li> </ul>"},{"location":"index.html#analysis-workflow","title":"Analysis Workflow","text":"Scorecard &amp; Multi-dimensional Analysis <ul> <li>Scorecard Generation: Analyze multiple metrics simultaneously</li> <li>Multi-dimensional Slicing: Break down results by segments</li> <li>Multiple Treatment Arms: Compare several treatments at once</li> <li>Ratio Metrics: Built-in support for conversion rates, averages, etc.</li> <li>Relative Lift: Analyze effects as percentage changes rather than absolute differences</li> </ul>"},{"location":"index.html#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install cluster-experiments\n</code></pre>"},{"location":"index.html#quick-example","title":"\u26a1 Quick Example","text":"<p>Here's how to run an analysis in just a few lines:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom cluster_experiments import AnalysisPlan, Variant\n\nnp.random.seed(42)\n\n# 0. Create simple data\nN = 1_000\ndf = pd.DataFrame({\n    \"variant\": np.random.choice([\"control\", \"treatment\"], N),\n    \"orders\": np.random.poisson(10, N),\n    \"visits\": np.random.poisson(100, N),\n})\ndf[\"converted\"] = (df[\"orders\"] &gt; 0).astype(int)\n\n\n# 1. Define your analysis plan\nplan = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"name\": \"orders\", \"alias\": \"revenue\", \"metric_type\": \"simple\"},\n        {\"name\": \"converted\", \"alias\": \"conversion\", \"metric_type\": \"ratio\", \"numerator\": \"converted\", \"denominator\": \"visits\"}\n    ],\n    \"variants\": [\n        {\"name\": \"control\", \"is_control\": True},\n        {\"name\": \"treatment\", \"is_control\": False}\n    ],\n    \"variant_col\": \"variant\",\n    \"analysis_type\": \"ols\"\n})\n\n# 2. Run analysis on your dataframe\nresults = plan.analyze(df)\nprint(results.to_dataframe().head())\n</code></pre> <p>Output Example:</p> <pre><code>  metric_alias control_variant_name treatment_variant_name  control_variant_mean  treatment_variant_mean analysis_type           ate  ate_ci_lower  ate_ci_upper   p_value     std_error     dimension_name dimension_value  alpha\n0      revenue              control              treatment              10.08554                9.941061           ols -1.444788e-01 -5.446603e-01  2.557026e-01  0.479186  2.041780e-01  __total_dimension           total   0.05\n1   conversion              control              treatment               1.00000                1.000000           ols  1.110223e-16 -1.096504e-16  3.316950e-16  0.324097  1.125902e-16  __total_dimension           total   0.05\n</code></pre>"},{"location":"index.html#power-analysis","title":"Power Analysis","text":"<p>Design your experiment by estimating required sample size and detectable effects. Here's a complete example using analytical (CLT-based) power analysis:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom cluster_experiments import NormalPowerAnalysis\n\n# Create sample historical data\nnp.random.seed(42)\nN = 500\n\nhistorical_data = pd.DataFrame({\n    'user_id': range(N),\n    'metric': np.random.normal(100, 20, N),\n    'date': pd.to_datetime('2025-10-01') + pd.to_timedelta(np.random.randint(0, 30, N), unit='d')\n})\n\n# Initialize analytical power analysis (fast, CLT-based)\npower_analysis = NormalPowerAnalysis.from_dict({\n    'analysis': 'ols',\n    'splitter': 'non_clustered',\n    'target_col': 'metric',\n    'time_col': 'date'  # Required for mde_time_line\n})\n\n# 1. Calculate power for a given effect size\npower = power_analysis.power_analysis(historical_data, average_effect=5.0)\nprint(f\"Power for detecting +5 unit effect: {power:.1%}\")\n\n# 2. Calculate Minimum Detectable Effect (MDE) for desired power\nmde = power_analysis.mde(historical_data, power=0.8)\nprint(f\"Minimum detectable effect at 80% power: {mde:.2f}\")\n\n# 3. Power curve: How power changes with effect size\npower_curve = power_analysis.power_line(\n    historical_data,\n    average_effects=[2.0, 4.0, 6.0, 8.0, 10.0]\n)\nprint(power_curve)\n\n# 4. MDE timeline: How MDE changes with experiment length\nmde_timeline = power_analysis.mde_time_line(\n    historical_data,\n    powers=[0.8],\n    experiment_length=[7, 14, 21, 30]\n)\n</code></pre> <p>Output:</p> <pre><code>Power for detecting +5 unit effect: 72.7%\nMinimum detectable effect at 80% power: 5.46\n{2.0: 0.18, 4.0: 0.54, 6.0: 0.87, 8.0: 0.98, 10.0: 1.00}\n</code></pre> <p>Key methods:</p> <ul> <li><code>power_analysis()</code>: Calculate power for a given effect</li> <li><code>mde()</code>: Calculate minimum detectable effect</li> <li><code>power_line()</code>: Generate power curves across effect sizes</li> <li><code>mde_time_line()</code>: Calculate MDE for different experiment lengths</li> </ul> <p>For simulation-based power analysis (for complex designs), see the Power Analysis Guide.</p>"},{"location":"index.html#documentation","title":"\ud83d\udcda Documentation","text":"<p>For detailed guides, API references, and advanced examples, visit our documentation.</p>"},{"location":"index.html#core-concepts","title":"Core Concepts","text":"<p>The library is built around three main components:</p>"},{"location":"index.html#1-splitter-define-how-to-randomize","title":"1. Splitter - Define how to randomize","text":"<p>Choose how to split your data into control and treatment groups:</p> <ul> <li><code>NonClusteredSplitter</code>: Standard individual-level randomization</li> <li><code>ClusteredSplitter</code>: Cluster-level randomization</li> <li><code>SwitchbackSplitter</code>: Time-based alternating treatments</li> <li><code>StratifiedClusteredSplitter</code>: Balance randomization across strata</li> </ul>"},{"location":"index.html#2-analysis-measure-the-impact","title":"2. Analysis - Measure the impact","text":"<p>Select the appropriate statistical method for your design:</p> <ul> <li><code>OLSAnalysis</code>: Standard regression for A/B tests</li> <li><code>ClusteredOLSAnalysis</code>: Clustered standard errors for cluster-randomized designs</li> <li><code>TTestClusteredAnalysis</code>: T-tests on cluster-aggregated data</li> <li><code>GeeExperimentAnalysis</code>: GEE for correlated observations</li> <li><code>SyntheticControlAnalysis</code>: Observational studies with synthetic controls</li> </ul>"},{"location":"index.html#3-analysisplan-orchestrate-your-analysis","title":"3. AnalysisPlan - Orchestrate your analysis","text":"<p>Define your complete analysis workflow:</p> <ul> <li>Specify metrics (simple and ratio)</li> <li>Define variants and dimensions</li> <li>Configure hypothesis tests</li> <li>Generate comprehensive scorecards</li> </ul> <p>For power analysis, combine these with:</p> <ul> <li>Perturbator: Simulate treatment effects for power calculations</li> <li>PowerAnalysis: Estimate statistical power and sample sizes</li> </ul>"},{"location":"index.html#advanced-features","title":"\ud83d\udee0\ufe0f Advanced Features","text":""},{"location":"index.html#variance-reduction-cupedcupac","title":"Variance Reduction (CUPED/CUPAC)","text":"<p>Reduce variance and detect smaller effects by leveraging pre-experiment data. Use historical metrics as covariates to control for pre-existing differences between groups.</p> <p>Use cases:</p> <ul> <li>Have pre-experiment metrics for your users/clusters</li> <li>Want to detect smaller treatment effects</li> <li>Need more sensitive tests with same sample size</li> </ul> <p>See the CUPAC Example for detailed implementation.</p>"},{"location":"index.html#cluster-randomization","title":"Cluster Randomization","text":"<p>Handle experiments where randomization occurs at group level (stores, cities, regions) rather than individual level. Essential for managing spillover effects and operational constraints.</p> <p>See the Cluster Randomization Guide for details.</p>"},{"location":"index.html#switchback-experiments","title":"Switchback Experiments","text":"<p>Design and analyze time-based crossover experiments where the same units receive both control and treatment at different times.</p> <p>See the Switchback Example for implementation.</p>"},{"location":"index.html#support","title":"\ud83c\udf1f Support","text":"<ul> <li>\u2b50 Star us on GitHub</li> <li>\ud83d\udcdd Read the documentation</li> <li>\ud83d\udc1b Report issues on our issue tracker</li> <li>\ud83d\udcac Join discussions in GitHub Discussions</li> </ul>"},{"location":"index.html#citation","title":"\ud83d\udcda Citation","text":"<p>If you use cluster-experiments in your research, please cite:</p> <pre><code>@software{cluster_experiments,\n  author = {David Masip and contributors},\n  title = {cluster-experiments: A Python library for designing and analyzing experiments},\n  url = {https://github.com/david26694/cluster-experiments},\n  year = {2022}\n}\n</code></pre>"},{"location":"CONTRIBUTING.html","title":"Contributing","text":""},{"location":"CONTRIBUTING.html#contributing","title":"Contributing","text":"<p>uv is needed as package manager. If you haven't installed it, run the installation command:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"CONTRIBUTING.html#project-setup","title":"Project setup","text":"<p>Clone repo and go to the project directory:</p> <pre><code>git clone git@github.com:david26694/cluster-experiments.git\ncd cluster-experiments\n</code></pre> <p>Create virtual environment and activate it:</p> <pre><code>uv venv -p 3.10\nsource .venv/bin/activate\n</code></pre> <p>After creating the virtual environment, install the project dependencies:</p> <pre><code>make install-dev\n</code></pre>"},{"location":"aa_test.html","title":"AA Test (Clustered)","text":"<p>This notebook shows that, when using a clustered splitter, if the clusters explain a part of the variance, using a non-clustered analysis will lead to higher false positive rate than expected.</p> <p>In particular, we use a clustered splitter and:</p> <ul> <li>An OLS-clustered robust estimator, we see that it passes the AA test</li> <li>A simple OLS (without clustered standard errors), it fails the AA test as it returns a super high false positive rate</li> </ul> In\u00a0[1]: Copied! <pre>from datetime import date\n\nimport numpy as np\nfrom cluster_experiments import PowerAnalysis, ConstantPerturbator, BalancedClusteredSplitter, ExperimentAnalysis, ClusteredOLSAnalysis\nimport pandas as pd\nimport statsmodels.api as sm\n\n\n\n# Create fake data\nN = 10_000\nclusters = [f\"Cluster {i}\" for i in range(10)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 15)]\ndf = pd.DataFrame(\n    {\n        \"cluster\": np.random.choice(clusters, size=N),\n        \"date\": np.random.choice(dates, size=N),\n    }\n).assign(\n    # Target is a linear combination of cluster and day of week, plus some noise\n    cluster_id=lambda df: df[\"cluster\"].astype(\"category\").cat.codes,\n    day_of_week=lambda df: pd.to_datetime(df[\"date\"]).dt.dayofweek,\n    target=lambda df: df[\"cluster_id\"] + df[\"day_of_week\"] + np.random.normal(size=N),\n)\n</pre> from datetime import date  import numpy as np from cluster_experiments import PowerAnalysis, ConstantPerturbator, BalancedClusteredSplitter, ExperimentAnalysis, ClusteredOLSAnalysis import pandas as pd import statsmodels.api as sm    # Create fake data N = 10_000 clusters = [f\"Cluster {i}\" for i in range(10)] dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 15)] df = pd.DataFrame(     {         \"cluster\": np.random.choice(clusters, size=N),         \"date\": np.random.choice(dates, size=N),     } ).assign(     # Target is a linear combination of cluster and day of week, plus some noise     cluster_id=lambda df: df[\"cluster\"].astype(\"category\").cat.codes,     day_of_week=lambda df: pd.to_datetime(df[\"date\"]).dt.dayofweek,     target=lambda df: df[\"cluster_id\"] + df[\"day_of_week\"] + np.random.normal(size=N), )  In\u00a0[2]: Copied! <pre>df.head()\n</pre> df.head() Out[2]: cluster date cluster_id day_of_week target 0 Cluster 3 2022-01-08 3 5 7.534487 1 Cluster 2 2022-01-06 2 3 5.039041 2 Cluster 1 2022-01-14 1 4 5.341845 3 Cluster 7 2022-01-12 7 2 9.468617 4 Cluster 0 2022-01-10 0 0 -0.644678 <p>Some clusters have a higher average outcome than others</p> In\u00a0[3]: Copied! <pre>df.groupby(\"cluster\").agg({\"target\": [\"mean\", \"std\"]})\n</pre> df.groupby(\"cluster\").agg({\"target\": [\"mean\", \"std\"]}) Out[3]: target mean std cluster Cluster 0 3.027335 2.223308 Cluster 1 3.907833 2.211297 Cluster 2 4.895215 2.270596 Cluster 3 6.045043 2.269786 Cluster 4 6.902209 2.224554 Cluster 5 8.028794 2.313159 Cluster 6 9.046213 2.253462 Cluster 7 10.055748 2.226720 Cluster 8 11.048716 2.273583 Cluster 9 11.939075 2.216478 In\u00a0[4]: Copied! <pre># Simple ols to run the analysis\nclass NonClusteredOLS(ExperimentAnalysis):\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_ols = sm.OLS.from_formula(\"target ~ treatment\", data=df).fit()\n        return results_ols.pvalues[self.treatment_col]\n</pre> # Simple ols to run the analysis class NonClusteredOLS(ExperimentAnalysis):     def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:         \"\"\"Returns the p-value of the analysis         Arguments:             df: dataframe containing the data to analyze             verbose (Optional): bool, prints the regression summary if True         \"\"\"         results_ols = sm.OLS.from_formula(\"target ~ treatment\", data=df).fit()         return results_ols.pvalues[self.treatment_col]   In\u00a0[5]: Copied! <pre>cluster_cols = [\"cluster\", \"date\"]\n\nsplitter = BalancedClusteredSplitter(\n    cluster_cols=cluster_cols,\n)\n\nperturbator = ConstantPerturbator()\n\nalpha = 0.05\nn_simulations = 100\n\n# Right power analysis, we use clustered splitter and ols clustered analysis\npw_right = PowerAnalysis(\n    splitter=splitter,\n    perturbator=perturbator,\n    alpha=alpha,\n    n_simulations=n_simulations,\n    analysis=ClusteredOLSAnalysis(\n        cluster_cols=cluster_cols,\n    ),\n)\n\n# Wrong power analysis, we use clustered splitter and regular ols\npw_wrong = PowerAnalysis(\n    splitter=splitter,\n    perturbator=perturbator,\n    alpha=alpha,\n    n_simulations=n_simulations,\n    analysis=NonClusteredOLS(\n        # We pass cluster_cols here, but we don't use them!!!\n        cluster_cols=cluster_cols,\n    ),\n)\n</pre> cluster_cols = [\"cluster\", \"date\"]  splitter = BalancedClusteredSplitter(     cluster_cols=cluster_cols, )  perturbator = ConstantPerturbator()  alpha = 0.05 n_simulations = 100  # Right power analysis, we use clustered splitter and ols clustered analysis pw_right = PowerAnalysis(     splitter=splitter,     perturbator=perturbator,     alpha=alpha,     n_simulations=n_simulations,     analysis=ClusteredOLSAnalysis(         cluster_cols=cluster_cols,     ), )  # Wrong power analysis, we use clustered splitter and regular ols pw_wrong = PowerAnalysis(     splitter=splitter,     perturbator=perturbator,     alpha=alpha,     n_simulations=n_simulations,     analysis=NonClusteredOLS(         # We pass cluster_cols here, but we don't use them!!!         cluster_cols=cluster_cols,     ), )   <p>Right way of doing it: in the AA test we get a power similar to the type I error of the test</p> In\u00a0[6]: Copied! <pre>pw_right.power_analysis(df, average_effect=0.0)\n</pre> pw_right.power_analysis(df, average_effect=0.0) Out[6]: <pre>0.06</pre> <p>Wrong way of doing it: the AA test fails, we have too much power</p> In\u00a0[7]: Copied! <pre>pw_wrong.power_analysis(df, average_effect=0.0)\n</pre> pw_wrong.power_analysis(df, average_effect=0.0) Out[7]: <pre>0.79</pre>"},{"location":"analysis_with_different_hypotheses.html","title":"Power comparison under different hypotheses","text":"<p>The goal of this notebook is to understand how different hypotheses change the power of an experiment. We start from some theory, moving to a pratical perspective using simulations and the implementations from this package.</p> <p>In hypothesis testing, various hypotheses can be examined, but the most common are:</p> <ul> <li>Two-sided: the effect is different from zero (commonly noted as H0: beta = 0, H1: beta != 0)</li> <li>Less: the effect is less than zero (H0: beta &gt;= 0, H1: beta &lt; 0)</li> <li>Greater: the effect is greater than zero (H0: beta &lt;= 0, H1: beta &gt; 0)</li> </ul> <p>In most cases the one-sided (less or greater) p-value is half the two-sided p-value. So, if the two-sided p-value is 5%, the one-sided p-value is 2.5%. However, if the actual difference (effect) went opposite to the predicted direction, in this case the one-sided p-value equals one minus half the two-sided value. So if the two-sided p-value is 2.5%, the one-tailed p-value is 97.5%.</p> <p></p> In\u00a0[1]: Copied! <pre>from datetime import date\nimport numpy as np\nimport pandas as pd\nfrom cluster_experiments import PowerAnalysis\nimport matplotlib.pyplot as plt\nimport warnings\nfrom plotnine import ggplot,theme_classic, facet_wrap, geom_density, labs, geom_vline, aes, geom_line, geom_histogram, geom_text,scale_y_continuous\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\nN = 1000\nclusters = [f\"Cluster {i}\" for i in range(50)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)]\ndf = pd.DataFrame(\n    {\n        \"cluster\": np.random.choice(clusters, size=N),\n        \"target\": np.random.normal(0, 1, size=N),\n        \"date\": np.random.choice(dates, size=N),\n    }\n)\n</pre> from datetime import date import numpy as np import pandas as pd from cluster_experiments import PowerAnalysis import matplotlib.pyplot as plt import warnings from plotnine import ggplot,theme_classic, facet_wrap, geom_density, labs, geom_vline, aes, geom_line, geom_histogram, geom_text,scale_y_continuous  warnings.filterwarnings('ignore') np.random.seed(42)  N = 1000 clusters = [f\"Cluster {i}\" for i in range(50)] dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)] df = pd.DataFrame(     {         \"cluster\": np.random.choice(clusters, size=N),         \"target\": np.random.normal(0, 1, size=N),         \"date\": np.random.choice(dates, size=N),     } )   <p>Let's start with a simple OLS model, not clustered, to understand the difference in power. We will try 2 different splitters: constant and normal</p> In\u00a0[2]: Copied! <pre>splitters = ['constant', 'normal']\nresults = []\n\nfor hypothesis in [\"two-sided\", \"less\", \"greater\"]:\n        for splitter in splitters:\n                config = {\n                    \"analysis\": 'ols',\n                    \"perturbator\": splitter,\n                    \"splitter\": \"non_clustered\",\n                    \"n_simulations\": 50,\n                    \"hypothesis\": hypothesis,\n                    \"seed\":41\n                }\n                pw = PowerAnalysis.from_dict(config)\n\n                power_dict = pw.power_line(df, average_effects=list(np.linspace(0.000001, 0.5, 15)))\n                power_df = pd.DataFrame(list(power_dict.items()), columns=['average_effect', 'power'])\n\n                power_df['hypothesis'] = hypothesis\n                power_df['splitter'] = splitter\n\n                results.append(power_df)\n\n\nfinal_df = pd.concat(results, ignore_index=True)\n</pre> splitters = ['constant', 'normal'] results = []  for hypothesis in [\"two-sided\", \"less\", \"greater\"]:         for splitter in splitters:                 config = {                     \"analysis\": 'ols',                     \"perturbator\": splitter,                     \"splitter\": \"non_clustered\",                     \"n_simulations\": 50,                     \"hypothesis\": hypothesis,                     \"seed\":41                 }                 pw = PowerAnalysis.from_dict(config)                  power_dict = pw.power_line(df, average_effects=list(np.linspace(0.000001, 0.5, 15)))                 power_df = pd.DataFrame(list(power_dict.items()), columns=['average_effect', 'power'])                  power_df['hypothesis'] = hypothesis                 power_df['splitter'] = splitter                  results.append(power_df)   final_df = pd.concat(results, ignore_index=True)  In\u00a0[3]: Copied! <pre>final_df.head()\n</pre> final_df.head() Out[3]: average_effect power hypothesis splitter 0 0.000001 0.04 two-sided constant 1 0.035715 0.06 two-sided constant 2 0.071429 0.22 two-sided constant 3 0.107144 0.40 two-sided constant 4 0.142858 0.64 two-sided constant In\u00a0[4]: Copied! <pre>def plot(breakdown:str, ncol_facetting:int):\n    p = (ggplot(final_df, aes(x='average_effect', y = 'power', color='hypothesis')) \n         + geom_line() \n         + theme_classic() \n         + facet_wrap(breakdown, ncol = 1))\n\n    print(p)\n</pre> def plot(breakdown:str, ncol_facetting:int):     p = (ggplot(final_df, aes(x='average_effect', y = 'power', color='hypothesis'))           + geom_line()           + theme_classic()           + facet_wrap(breakdown, ncol = 1))      print(p)  <p>We can clearly see that using the correct side (higher, as the effect is positive) increase the power of the experiment. It's also great to see such a low power in case of hypothesis 'less'.</p> In\u00a0[5]: Copied! <pre>plot(breakdown = 'splitter', ncol_facetting = 1)\n</pre> plot(breakdown = 'splitter', ncol_facetting = 1) <pre>\n</pre> <p>Now we will quantify this difference in power between greater and two-sided</p> In\u00a0[6]: Copied! <pre>pivot_df = (\n    final_df\n    .pivot(index = [ 'splitter', 'average_effect'], columns = 'hypothesis', values = 'power')\n    .reset_index()\n    .assign(diff = lambda x: x['greater'] - x['two-sided'])\n)\n</pre> pivot_df = (     final_df     .pivot(index = [ 'splitter', 'average_effect'], columns = 'hypothesis', values = 'power')     .reset_index()     .assign(diff = lambda x: x['greater'] - x['two-sided']) ) In\u00a0[7]: Copied! <pre>mean_diff = pivot_df['diff'].mean()\nmax_count = np.max(pivot_df['diff'].value_counts())\n</pre> mean_diff = pivot_df['diff'].mean() max_count = np.max(pivot_df['diff'].value_counts())  <p>Below we plot the distribution of the difference between 2-sided and 'greater' hypotheses. The mean equals to 4% is the estimated increase in power when using a one-sided experiment. From the 30 iterations, we see that 12 times it didn't change the power, in 2 cases it actually decrease it and in 2 cases the incease in power was 15pp.</p> In\u00a0[8]: Copied! <pre>(ggplot(pivot_df, aes(x = 'diff')) \n + geom_histogram( alpha=0.5, bins = 10, position=\"identity\") \n + geom_density()\n + geom_vline(xintercept=mean_diff, color=\"red\", linetype=\"dashed\", size=1)\n + geom_text(x=mean_diff, y=np.max(pivot_df['diff'].value_counts()), label=f'Mean: {mean_diff:.2f}', va='bottom', ha='left', color=\"red\")\n + scale_y_continuous(breaks=range(0, int(max_count) + 1))\n + labs(title = 'Histogram of power difference between greater and two sided hypothesis', x='Difference in power')\n        \n)\n</pre> (ggplot(pivot_df, aes(x = 'diff'))   + geom_histogram( alpha=0.5, bins = 10, position=\"identity\")   + geom_density()  + geom_vline(xintercept=mean_diff, color=\"red\", linetype=\"dashed\", size=1)  + geom_text(x=mean_diff, y=np.max(pivot_df['diff'].value_counts()), label=f'Mean: {mean_diff:.2f}', va='bottom', ha='left', color=\"red\")  + scale_y_continuous(breaks=range(0, int(max_count) + 1))  + labs(title = 'Histogram of power difference between greater and two sided hypothesis', x='Difference in power')          ) Out[8]: <pre>&lt;Figure Size: (640 x 480)&gt;</pre> <p>Now let's move to clustered methods. To keep the notebook tidy, we will just run the constant perturbator.</p> In\u00a0[9]: Copied! <pre>results = []\n\nfor hypothesis in [\"two-sided\", \"less\", \"greater\"]:\n        for analysis in [\"ols_clustered\", \"gee\", 'ttest_clustered']:\n                config = {\n                    \"analysis\": analysis,\n                    \"perturbator\": \"constant\",\n                    \"splitter\": \"clustered\",\n                    \"n_simulations\": 50,\n                    \"hypothesis\": hypothesis,\n                    \"cluster_cols\": ['cluster', 'date'],\n                    \"seed\":41\n                }\n                pw = PowerAnalysis.from_dict(config)\n\n                power_dict = pw.power_line(df, average_effects=list(np.linspace(0.000001, 0.5, 15)))\n                power_df = pd.DataFrame(list(power_dict.items()), columns=['average_effect', 'power'])\n\n                power_df['hypothesis'] = hypothesis\n                power_df['analysis'] = analysis\n\n                results.append(power_df)\n\n\nfinal_df = pd.concat(results, ignore_index=True)\n</pre> results = []  for hypothesis in [\"two-sided\", \"less\", \"greater\"]:         for analysis in [\"ols_clustered\", \"gee\", 'ttest_clustered']:                 config = {                     \"analysis\": analysis,                     \"perturbator\": \"constant\",                     \"splitter\": \"clustered\",                     \"n_simulations\": 50,                     \"hypothesis\": hypothesis,                     \"cluster_cols\": ['cluster', 'date'],                     \"seed\":41                 }                 pw = PowerAnalysis.from_dict(config)                  power_dict = pw.power_line(df, average_effects=list(np.linspace(0.000001, 0.5, 15)))                 power_df = pd.DataFrame(list(power_dict.items()), columns=['average_effect', 'power'])                  power_df['hypothesis'] = hypothesis                 power_df['analysis'] = analysis                  results.append(power_df)   final_df = pd.concat(results, ignore_index=True)   <p>Here, again, we see an increase in power using the correct one-sided hypothesis compared to two-sided.</p> In\u00a0[10]: Copied! <pre>plot(breakdown = 'analysis', ncol_facetting=1)\n</pre> plot(breakdown = 'analysis', ncol_facetting=1) <pre>\n</pre>"},{"location":"analysis_with_different_hypotheses.html#power-comparison-under-different-hypotheses","title":"Power comparison under different hypotheses\u00b6","text":""},{"location":"analysis_with_different_hypotheses.html#generate-random-clustered-data","title":"Generate random clustered data\u00b6","text":""},{"location":"analysis_with_different_hypotheses.html#ols","title":"OLS\u00b6","text":""},{"location":"analysis_with_different_hypotheses.html#clustered-methods","title":"Clustered methods\u00b6","text":""},{"location":"create_custom_classes.html","title":"Custom Classes","text":"<p>Examples on how to create:</p> <ul> <li>a custom perturbator</li> <li>a custom splitter</li> <li>a custom hypothesis test</li> </ul> <p>The names of you custom classes don't need to be CustomX, they are completely free. The only requirement is that they inherit from the base class. For example, if you want to create a custom perturbator, you need to inherit from the Perturbator base class. The same applies to the other classes.</p> In\u00a0[1]: Copied! <pre>from cluster_experiments import ExperimentAnalysis\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\nclass CustomExperimentAnalysis(ExperimentAnalysis):\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = True) -&gt; float:\n        treatment_data = df.query(f\"{self.treatment_col} == 1\")[self.target_col]\n        control_data = df.query(f\"{self.treatment_col} == 0\")[self.target_col]\n        t_test_results = ttest_ind(treatment_data, control_data, equal_var=False)\n        return t_test_results.pvalue\n</pre> from cluster_experiments import ExperimentAnalysis import pandas as pd from scipy.stats import ttest_ind  class CustomExperimentAnalysis(ExperimentAnalysis):     def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = True) -&gt; float:         treatment_data = df.query(f\"{self.treatment_col} == 1\")[self.target_col]         control_data = df.query(f\"{self.treatment_col} == 0\")[self.target_col]         t_test_results = ttest_ind(treatment_data, control_data, equal_var=False)         return t_test_results.pvalue In\u00a0[2]: Copied! <pre>from cluster_experiments import RandomSplitter\nimport numpy as np\n\nclass CustomRandomSplitter(RandomSplitter):\n    def assign_treatment_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df.copy()\n        # Power users get treatment with 90% probability\n        df_power_users = df.query(\"power_user\")\n        df_power_users[self.treatment_col] = np.random.choice(\n            [\"A\", \"B\"], size=len(df_power_users), p=[0.1, 0.9]\n        )\n        # Non-power users get treatment with 10% probability\n        df_non_power_users = df.query(\"not power_user\")\n        df_non_power_users[self.treatment_col] = np.random.choice(\n            [\"A\", \"B\"], size=len(df_non_power_users), p=[0.9, 0.1]\n        )\n        return pd.concat([df_power_users, df_non_power_users])\n</pre> from cluster_experiments import RandomSplitter import numpy as np  class CustomRandomSplitter(RandomSplitter):     def assign_treatment_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:         df = df.copy()         # Power users get treatment with 90% probability         df_power_users = df.query(\"power_user\")         df_power_users[self.treatment_col] = np.random.choice(             [\"A\", \"B\"], size=len(df_power_users), p=[0.1, 0.9]         )         # Non-power users get treatment with 10% probability         df_non_power_users = df.query(\"not power_user\")         df_non_power_users[self.treatment_col] = np.random.choice(             [\"A\", \"B\"], size=len(df_non_power_users), p=[0.9, 0.1]         )         return pd.concat([df_power_users, df_non_power_users]) In\u00a0[3]: Copied! <pre>from cluster_experiments import Perturbator\nimport pandas as pd\n\nclass CustomPerturbator(Perturbator):\n    def perturbate(self, df: pd.DataFrame, average_effect: float) -&gt; pd.DataFrame:\n        df = df.copy().reset_index(drop=True)\n        n = (df[self.treatment_col] == self.treatment).sum()\n        df.loc[\n            df[self.treatment_col] == self.treatment, self.target_col\n        ] += np.random.normal(average_effect, 1, size=n)\n        return df\n</pre> from cluster_experiments import Perturbator import pandas as pd  class CustomPerturbator(Perturbator):     def perturbate(self, df: pd.DataFrame, average_effect: float) -&gt; pd.DataFrame:         df = df.copy().reset_index(drop=True)         n = (df[self.treatment_col] == self.treatment).sum()         df.loc[             df[self.treatment_col] == self.treatment, self.target_col         ] += np.random.normal(average_effect, 1, size=n)         return df"},{"location":"cupac_example.html","title":"CUPAC Example","text":"In\u00a0[1]: Copied! <pre>from datetime import date\n\nimport numpy as np\nimport pandas as pd\nfrom cluster_experiments import GeeExperimentAnalysis\nfrom cluster_experiments import ConstantPerturbator\nfrom cluster_experiments import PowerAnalysis\nfrom cluster_experiments import ClusteredSplitter\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\ndef generate_random_data(clusters, dates, N):\n    \"\"\"Generates target as a non-linear function of the covariates, the cluster mean and some residual\"\"\"\n\n    # Every cluster has a mean\n    df_clusters = pd.DataFrame(\n        {\n            \"cluster\": clusters,\n            \"cluster_mean\": np.random.normal(0, 0.1, size=len(clusters)),\n        }\n    )\n    # The target is the sum of: covariates, cluster mean and random residual\n    df = (\n        pd.DataFrame(\n            {\n                \"cluster\": np.random.choice(clusters, size=N),\n                \"residual\": np.random.normal(0, 1, size=N),\n                \"date\": np.random.choice(dates, size=N),\n                \"x1\": np.random.normal(0, 1, size=N),\n                \"x2\": np.random.normal(0, 1, size=N),\n                \"x3\": np.random.normal(0, 1, size=N),\n                \"x4\": np.random.normal(0, 1, size=N),\n            }\n        )\n        .merge(df_clusters, on=\"cluster\")\n        .assign(\n            target=lambda x: x[\"x1\"] * x[\"x2\"]\n            + x[\"x3\"] ** 2\n            + x[\"x4\"]\n            + x[\"cluster_mean\"]\n            + x[\"residual\"]\n        )\n    )\n\n    return df\n</pre> from datetime import date  import numpy as np import pandas as pd from cluster_experiments import GeeExperimentAnalysis from cluster_experiments import ConstantPerturbator from cluster_experiments import PowerAnalysis from cluster_experiments import ClusteredSplitter from sklearn.ensemble import HistGradientBoostingRegressor   def generate_random_data(clusters, dates, N):     \"\"\"Generates target as a non-linear function of the covariates, the cluster mean and some residual\"\"\"      # Every cluster has a mean     df_clusters = pd.DataFrame(         {             \"cluster\": clusters,             \"cluster_mean\": np.random.normal(0, 0.1, size=len(clusters)),         }     )     # The target is the sum of: covariates, cluster mean and random residual     df = (         pd.DataFrame(             {                 \"cluster\": np.random.choice(clusters, size=N),                 \"residual\": np.random.normal(0, 1, size=N),                 \"date\": np.random.choice(dates, size=N),                 \"x1\": np.random.normal(0, 1, size=N),                 \"x2\": np.random.normal(0, 1, size=N),                 \"x3\": np.random.normal(0, 1, size=N),                 \"x4\": np.random.normal(0, 1, size=N),             }         )         .merge(df_clusters, on=\"cluster\")         .assign(             target=lambda x: x[\"x1\"] * x[\"x2\"]             + x[\"x3\"] ** 2             + x[\"x4\"]             + x[\"cluster_mean\"]             + x[\"residual\"]         )     )      return df In\u00a0[2]: Copied! <pre>clusters = [f\"Cluster {i}\" for i in range(100)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)]\nexperiment_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(15, 32)]\nN = 10_000\ndf = generate_random_data(clusters, dates, N).drop(columns=[\"residual\", \"cluster_mean\"])\ndf_analysis = df.query(f\"date.isin({experiment_dates})\")\ndf_pre = df.query(f\"~date.isin({experiment_dates})\")\ndf\n</pre> clusters = [f\"Cluster {i}\" for i in range(100)] dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)] experiment_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(15, 32)] N = 10_000 df = generate_random_data(clusters, dates, N).drop(columns=[\"residual\", \"cluster_mean\"]) df_analysis = df.query(f\"date.isin({experiment_dates})\") df_pre = df.query(f\"~date.isin({experiment_dates})\") df Out[2]: cluster date x1 x2 x3 x4 target 0 Cluster 64 2022-01-26 0.407857 -0.821972 0.380603 -0.435979 0.259322 1 Cluster 64 2022-01-01 -0.338506 1.259993 -1.172503 -0.638529 0.109807 2 Cluster 64 2022-01-25 0.144844 -0.647623 1.126136 0.339519 1.672764 3 Cluster 64 2022-01-04 -0.081028 2.257395 1.786203 0.608843 4.281907 4 Cluster 64 2022-01-13 -0.381064 -0.636419 0.854746 1.072939 2.303696 ... ... ... ... ... ... ... ... 9995 Cluster 74 2022-01-05 0.781370 -0.216738 -0.786449 -0.806634 -0.552988 9996 Cluster 74 2022-01-16 -0.858006 0.801571 -1.044879 0.105053 0.638271 9997 Cluster 74 2022-01-22 0.943984 -0.975810 -0.839801 0.839736 1.674833 9998 Cluster 74 2022-01-24 1.269351 -0.037544 -1.506677 1.209883 2.423682 9999 Cluster 74 2022-01-30 -1.141438 0.977891 0.143942 -0.585707 -0.835189 <p>10000 rows \u00d7 7 columns</p> In\u00a0[3]: Copied! <pre># Splitter and perturbator\nsw = ClusteredSplitter(\n    cluster_cols=[\"cluster\", \"date\"],\n)\n\nperturbator = ConstantPerturbator(\n    average_effect=0.1,\n)\n\n# Vainilla GEE\nanalysis = GeeExperimentAnalysis(\n    cluster_cols=[\"cluster\", \"date\"],\n)\npw_vanilla = PowerAnalysis(\n    perturbator=perturbator,\n    splitter=sw,\n    analysis=analysis,\n    n_simulations=50,\n)\n\npower = pw_vanilla.power_analysis(df_analysis)\nprint(f\"Not using cupac: {power = }\")\n</pre> # Splitter and perturbator sw = ClusteredSplitter(     cluster_cols=[\"cluster\", \"date\"], )  perturbator = ConstantPerturbator(     average_effect=0.1, )  # Vainilla GEE analysis = GeeExperimentAnalysis(     cluster_cols=[\"cluster\", \"date\"], ) pw_vanilla = PowerAnalysis(     perturbator=perturbator,     splitter=sw,     analysis=analysis,     n_simulations=50, )  power = pw_vanilla.power_analysis(df_analysis) print(f\"Not using cupac: {power = }\")  <pre>Not using cupac: power = 0.5\n</pre> <p>We can see that power is not great</p> In\u00a0[4]: Copied! <pre># Cupac GEE\nanalysis = GeeExperimentAnalysis(\n    cluster_cols=[\"cluster\", \"date\"], covariates=[\"estimate_target\"]\n)\n\ngbm = HistGradientBoostingRegressor()\npw_cupac = PowerAnalysis(\n    perturbator=perturbator,\n    splitter=sw,\n    analysis=analysis,\n    n_simulations=50,\n    cupac_model=gbm,\n    features_cupac_model=[\"x1\", \"x2\", \"x3\", \"x4\"],\n)\n\npower = pw_cupac.power_analysis(df_analysis, df_pre)\nprint(f\"Using cupac: {power = }\")\n</pre> # Cupac GEE analysis = GeeExperimentAnalysis(     cluster_cols=[\"cluster\", \"date\"], covariates=[\"estimate_target\"] )  gbm = HistGradientBoostingRegressor() pw_cupac = PowerAnalysis(     perturbator=perturbator,     splitter=sw,     analysis=analysis,     n_simulations=50,     cupac_model=gbm,     features_cupac_model=[\"x1\", \"x2\", \"x3\", \"x4\"], )  power = pw_cupac.power_analysis(df_analysis, df_pre) print(f\"Using cupac: {power = }\") <pre>Using cupac: power = 0.92\n</pre> <p>Power has increased!</p>"},{"location":"cupac_example.html#cupac-tutorial","title":"Cupac tutorial\u00b6","text":"<p>In this example we demonstrate usage of CUPAC as a way to increase power.</p> <p>We compare vanilla (non-cupac) gee estimation with gee estimation with cupac.</p>"},{"location":"cupac_example.html#data-generation","title":"Data generation\u00b6","text":"<p>We have data that contains:</p> <ul> <li>Cluster</li> <li>Date</li> <li>Some covariates</li> <li>Outcome (target)</li> </ul> <p>We want to run a switchback-clustered experiment for around 15 days.</p> <p>We take data from day 15 to day 31 as the analysis data.</p> <p>We take data from day 1 to day 14 as the pre-analysis data, which is going to be used to fit the cupac model.</p>"},{"location":"cupac_example.html#vainilla-method","title":"Vainilla method\u00b6","text":"<p>Just run a regular gee estimation, no covariate adjustment.</p>"},{"location":"cupac_example.html#cupac-method","title":"Cupac method\u00b6","text":"<p>Use GBM model with covariates x1, x2, x3, x4 fitted on pre-analysis data; use the predictions of this model on analysis data to reduce variance.</p>"},{"location":"delta.html","title":"Delta","text":"<p>Delta method with CUPED should work like the following. For each randomization unit i we observe $Y_i$, $N_i$.</p> <p>Our estimator of the mean will have the form</p> <p>$$ \\frac{\\sum_{i=1}^n Y_i - \\theta (Z_i - E[Z_i])}{\\sum_{i=1}^n N_i} $$</p> <p>where $Z_i$ is the covariate for unit $i$, $E[Z]$ is the average of the covariate across all units, and $\\theta$ is a parameter that we estimate from the data.</p> <p>In order to estimate $\\theta$, we find the value that minimizes the variance of the estimator. The variance of the estimator can be expressed as:</p> <p>$$ \\begin{align} \\text{Var}\\left( \\frac{\\sum_{i=1}^n Y_i - \\theta \\sum_{i=1}^n (Z_i - \\bar{Z})}{\\sum_{i=1}^n N_i} \\right) &amp;= \\text{Var}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i} \\right) \\ &amp;\\quad +  \\theta^2 \\text{Var}\\left( \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) \\ &amp;\\quad - 2 \\theta \\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) \\end{align} $$</p> <p>By calculating the derivative of this variance with respect to $\\theta$ and setting it to zero, we can find the optimal value of $\\theta$ that minimizes the variance: $$\\begin{align} 2 \\theta \\text{Var}\\left( \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) - 2 \\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) &amp;= 0 \\end{align}$$</p> <p>This gives us the optimal $\\theta$ as: $$\\theta = \\frac{\\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right)}{\\text{Var}\\left( \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right)}$$</p> <p>Using linearisation (Taylor expansion) we can also express the optimal $\\theta$ as: $$ \\theta = \\frac{     \\text{Cov}\\left(             \\frac{\\bar{Y}}{E[N]} -             \\frac{\\bar{N} E[Y]}{E[N]^2},         \\frac{\\bar{Z}}{E[N]} - \\frac{E[Z] \\bar{N}}{E[N]^2}     \\right) }{     \\text{Var}\\left(         \\frac{\\bar{Z}}{E[N]} -         \\frac{E[Z] \\bar{N}}{E[N]^2}     \\right) } $$ Where $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i$, $\\bar{Z} = \\frac{1}{n} \\sum_{i=1}^n Z_i$, and $\\bar{N} = \\frac{1}{n} \\sum_{i=1}^n N_i$ are the sample means of $Y$, $Z$, and $N$ respectively.</p> <p>Because the user is the randomisation unit and user level observations are iid, we have: $$ \\sqrt(n) (\\bar{Y}, \\bar{N}, \\bar{Z}, \\bar{N}) \\xrightarrow{d} N(\\mu, \\Sigma) $$ Where $\\mu = (E[Y], E[N], E[Z], E[N])$ and $\\Sigma$ is the covariance matrix of $(Y, N, Z, N)$.</p> <p>If we define $$ \\beta_1 = (1 / E[N], - E[Y] / E[N]^2, 0, 0)^T $$ $$ \\beta_2 = (0, 0, 1 / E[N], - E[Z] / E[N]^2)^T $$ then we can express the optimal $\\theta$ as: $$ \\theta = \\beta_1^T \\Sigma \\beta_2 / \\beta_2^T \\Sigma \\beta_2 $$</p> <p>In this case, the variance of the estimator can be expressed as: $$\\begin{align} \\text{Var}\\left( \\frac{\\sum_{i=1}^n Y_i - \\theta \\sum_{i=1}^n (Z_i - \\bar{Z})}{\\sum_{i=1}^n N_i} \\right) &amp;= \\text{Var}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i} \\right) \\ &amp;\\quad +  \\theta^2 \\text{Var}\\left( \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) \\ &amp;\\quad - 2 \\theta \\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) \\end{align} $$</p> <p>If instead of a single covariate $Z$ we have multiple covariates, we can extend this to a vector of covariates $\\bold{Z} = (Z_1, Z_2, \\ldots, Z_k)$ and compute the covariance and variance matrices accordingly.</p> <p>In this case, the variance of the estimator becomes: $$\\begin{align} \\text{Var}\\left( \\frac{\\sum_{i=1}^n Y_i - \\theta^T \\sum_{i=1}^n (Z_i - \\bar{Z})}{\\sum_{i=1}^n N_i} \\right) &amp;= \\text{Var}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i} \\right) \\ &amp;\\quad +  \\theta^T \\text{Var}\\left( \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) \\theta \\ &amp;\\quad - 2 \\theta^T \\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n Z_i}{\\sum_{i=1}^n N_i} \\right) \\end{align} $$</p> <p>The optimal $\\theta$ in this case is given by: $$\\theta = \\text{Var}\\left( \\frac{\\sum_{i=1}^n \\bold{Z}i}{\\sum{i=1}^n N_i} \\right)^{-1} \\cdot \\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n \\bold{Z}i}{\\sum{i=1}^n N_i} \\right) $$</p> <p>This can be computed using matrix operations, where $\\theta$ is a vector of coefficients corresponding to each covariate in $\\bold{Z}$. The term $\\text{Cov}\\left( \\frac{\\sum_{i=1}^n Y_i}{\\sum_{i=1}^n N_i}, \\frac{\\sum_{i=1}^n \\bold{Z}i}{\\sum{i=1}^n N_i} \\right)$ is a vector of covariances between the outcome and each covariate, and $\\text{Var}\\left( \\frac{\\sum_{i=1}^n \\bold{Z}i}{\\sum{i=1}^n N_i} \\right)$ is the covariance matrix of the covariates.</p> <p>Using linearisation, the optimal $\\theta$ can also be expressed as: $$ \\theta = \\text{Var}\\left(\\frac{\\bar{\\bold{Z}}}{E[N]} - \\frac{E[\\bold{Z}] \\bar{N}}{E[N]^2}\\right)^{-1} \\cdot \\text{Cov}\\left(\\frac{\\bar{Y}}{E[N]} - \\frac{\\bar{N} E[Y]}{E[N]^2}, \\frac{\\bar{\\bold{Z}}}{E[N]} - \\frac{E[\\bold{Z}] \\bar{N}}{E[N]^2}\\right) $$</p> <p>If we have multiple covariates, we consider the vector $(Y, N, Z_1, Z_2, \\ldots, Z_k, N)$ where $Z_1, Z_2, \\ldots, Z_k$ are the covariates.</p> <p>For $$ \\text{Cov}\\left(\\frac{\\bar{Y}}{E[N]} - \\frac{\\bar{N} E[Y]}{E[N]^2}, \\frac{\\bar{\\bold{Z}}}{E[N]} - \\frac{E[\\bold{Z}] \\bar{N}}{E[N]^2}\\right) $$, its component $l$ is given by: $$\\text{Cov}\\left(\\frac{\\bar{Y}}{E[N]} - \\frac{\\bar{N} E[Y]}{E[N]^2}, \\frac{\\bar{Z_l}}{E[N]} - \\frac{E[Z_l] \\bar{N}}{E[N]^2}\\right) $$ By expanding the covariance, we get: $$\\begin{align} &amp;= \\frac{\\text{Cov}(\\bar{Y}, \\bar{Z_l})}{E[N]^2} - \\frac{E[Y] \\text{Cov}(\\bar{N}, \\bar{Z_l})}{E[N]^3} - \\frac{E[Z_l] \\text{Cov}(\\bar{Y}, \\bar{N})}{E[N]^3} + \\frac{E[Y] E[Z_l] \\text{Var}(\\bar{N})}{E[N]^4} \\end{align}$$</p> <p>If we define the vectors: $$cov(Y, \\bold{Z}) = (\\text{Cov}(Y, Z_1), \\text{Cov}(Y, Z_2), \\ldots, \\text{Cov}(Y, Z_k))^T$$ $$cov(N, \\bold{Z}) = (\\text{Cov}(N, Z_1), \\text{Cov}(N, Z_2), \\ldots, \\text{Cov}(N, Z_k))^T$$ $$E[\\bold{Z}] = (E[Z_1], E[Z_2], \\ldots, E[Z_k])^T$$</p> <p>Then we can express the covariance vector as: $$\\text{Cov}\\left(\\frac{\\bar{Y}}{E[N]} - \\frac{\\bar{N} E[Y]}{E[N]^2}, \\frac{\\bar{\\bold{Z}}}{E[N]} - \\frac{E[\\bold{Z}] \\bar{N}}{E[N]^2}\\right) = \\frac{cov(Y, \\bold{Z})}{E[N]^2} - \\frac{E[Y] cov(N, \\bold{Z})}{E[N]^3} - \\frac{E[\\bold{Z}] \\text{Cov}(Y, N)}{E[N]^3} + \\frac{E[Y] E[\\bold{Z}] \\text{Var}(N)}{E[N]^4} $$</p> <p>I have a matrix $A = (a_{ij})$ where $a_{ij} = \\text{Cov}(Z_i, Z_j)$ for $i, j = 1, 2, \\ldots, k$ and a vector $b = (b_i)$ where $b_i = \\text{Cov}(Z_i, N)$ for $i = 1, 2, \\ldots, k$. I build the matrix: $$ D =\\begin{bmatrix} A &amp; b \\ b^T &amp; \\text{Var}(N) \\end{bmatrix}$$</p> <p>I have a matrix $$c_{ij} = x^2 a_{ij} + y_i y_j \\text{Var}(N) - x y_j b_i - x y_i b_j$$ for some constants $x$ and a vector $\\bold{y} = (y_i)$.</p> <p>I want to get a matrix $K$ such that: $$ K D K^T = C $$</p> <p>If $$ K = \\begin{bmatrix}x I &amp; -\\bold{y} \\end{bmatrix}$$ where $I$ is the identity matrix of size $k$ and $\\bold{y}$ is a column vector of ones of size $k$, then we have: $$ K D K^T = C $$</p> <p>In our case, we have $x = 1 / E[N]$ and $\\bold{y} = E[\\bold{Z} ] / E[N]^2$.</p>"},{"location":"delta_method.html","title":"Delta Method Analysis","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom scipy.stats import norm\nfrom statsmodels.formula.api import ols\n\nfrom cluster_experiments import BinaryPerturbator\nfrom cluster_experiments import ClusteredSplitter\nfrom cluster_experiments import DeltaMethodAnalysis\nfrom cluster_experiments import PowerAnalysis\nfrom tests.utils import generate_ratio_metric_data\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt  from datetime import date from scipy.stats import norm from statsmodels.formula.api import ols  from cluster_experiments import BinaryPerturbator from cluster_experiments import ClusteredSplitter from cluster_experiments import DeltaMethodAnalysis from cluster_experiments import PowerAnalysis from tests.utils import generate_ratio_metric_data  In\u00a0[2]: Copied! <pre>def naive_ttest(data):\n    data= data.groupby(['user', 'treatment']).agg({'target':'sum', 'scale':'sum'}).reset_index()\n    data['metric']  = data['target']/data['scale']\n    stats = data.groupby('treatment').agg(\n        mean = ('metric', 'mean'),\n        var = ('metric', 'var'),\n        num_samples = ('scale', 'sum')\n        )\n\n    mean_dif = stats.loc['A', 'mean'] - stats.loc['B', 'mean']\n    var_dif = stats.loc['A', 'var']/stats.loc['A', 'num_samples'] + stats.loc['B', 'var']/stats.loc['B', 'num_samples']\n    z_score = mean_dif/np.sqrt(var_dif)\n    pval = 2 * (1 - norm.cdf(abs(z_score)))\n    return pval\n\ndef generate_data(N, num_users=2000, user_sample_mean=0.3, user_standard_error=0.5, treatment_effect=0.1):\n    exp_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(14, 32)]\n\n    user_target_means = np.random.normal(user_sample_mean, user_standard_error, num_users)\n\n    #data generation\n    data = generate_ratio_metric_data(exp_dates, N, user_target_means, num_users, treatment_effect)\n    return data\n\ndef compare_vanilla_cuped(N, num_users=2000, user_sample_mean=0.3, user_standard_error=0.5, treatment_effect=0.1):\n    #data generation\n    data = generate_data(N, num_users, user_sample_mean, user_standard_error, treatment_effect)\n\n    analysis_delta = DeltaMethodAnalysis(\n        cluster_cols=[\"user\"]\n    )\n    delta_pval = analysis_delta.get_pvalue(data)\n\n\n    #naive t-test considering the same randomization unit at user level\n    naive_ttest_pval = naive_ttest(data)\n\n    #OLS without clustered errors\n    #map treatment to binary number\n\n    data['treatment'] = data['treatment'].map({'A': 0, 'B': 1})\n\n    ols_result = ols('Y ~ treatment ', pd.DataFrame({'Y': data.target, 'treatment': data.treatment})).fit()\n    naive_ols_pval = ols_result.pvalues['treatment']\n    #OLS with clustered errors\n    ols_result_clustered = ols('Y ~ treatment ', pd.DataFrame({'Y': data.target, 'treatment': data.treatment})).fit(cov_type='cluster', cov_kwds={'groups': data['user']})\n    naive_ols_clustered_pval = ols_result_clustered.pvalues['treatment']\n\n\n    return naive_ttest_pval, naive_ols_pval, naive_ols_clustered_pval, delta_pval\n</pre> def naive_ttest(data):     data= data.groupby(['user', 'treatment']).agg({'target':'sum', 'scale':'sum'}).reset_index()     data['metric']  = data['target']/data['scale']     stats = data.groupby('treatment').agg(         mean = ('metric', 'mean'),         var = ('metric', 'var'),         num_samples = ('scale', 'sum')         )      mean_dif = stats.loc['A', 'mean'] - stats.loc['B', 'mean']     var_dif = stats.loc['A', 'var']/stats.loc['A', 'num_samples'] + stats.loc['B', 'var']/stats.loc['B', 'num_samples']     z_score = mean_dif/np.sqrt(var_dif)     pval = 2 * (1 - norm.cdf(abs(z_score)))     return pval  def generate_data(N, num_users=2000, user_sample_mean=0.3, user_standard_error=0.5, treatment_effect=0.1):     exp_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(14, 32)]      user_target_means = np.random.normal(user_sample_mean, user_standard_error, num_users)      #data generation     data = generate_ratio_metric_data(exp_dates, N, user_target_means, num_users, treatment_effect)     return data  def compare_vanilla_cuped(N, num_users=2000, user_sample_mean=0.3, user_standard_error=0.5, treatment_effect=0.1):     #data generation     data = generate_data(N, num_users, user_sample_mean, user_standard_error, treatment_effect)      analysis_delta = DeltaMethodAnalysis(         cluster_cols=[\"user\"]     )     delta_pval = analysis_delta.get_pvalue(data)       #naive t-test considering the same randomization unit at user level     naive_ttest_pval = naive_ttest(data)      #OLS without clustered errors     #map treatment to binary number      data['treatment'] = data['treatment'].map({'A': 0, 'B': 1})      ols_result = ols('Y ~ treatment ', pd.DataFrame({'Y': data.target, 'treatment': data.treatment})).fit()     naive_ols_pval = ols_result.pvalues['treatment']     #OLS with clustered errors     ols_result_clustered = ols('Y ~ treatment ', pd.DataFrame({'Y': data.target, 'treatment': data.treatment})).fit(cov_type='cluster', cov_kwds={'groups': data['user']})     naive_ols_clustered_pval = ols_result_clustered.pvalues['treatment']       return naive_ttest_pval, naive_ols_pval, naive_ols_clustered_pval, delta_pval In\u00a0[3]: Copied! <pre># Let's generate some fake switchback data (the clusters here would be city and date\nN = 200_000\n\nnaive_ttest_p_values = []\nnaive_ols_clustered_p_values = []\nnaive_ols_p_values = []\ndelta_p_values = []\n\nfor _ in range(200):\n    naive_ttest_pval, naive_ols_pval, naive_ols_clustered_pval, delta_pval = compare_vanilla_cuped(N, num_users = 5_000, treatment_effect= 0)\n    naive_ttest_p_values.append(naive_ttest_pval)\n    naive_ols_p_values.append(naive_ols_pval)\n    naive_ols_clustered_p_values.append(naive_ols_clustered_pval)\n    delta_p_values.append(delta_pval)\n\nprint(f\"Naive average p-value t-test: {np.mean(naive_ttest_p_values)}\")\nprint(f\"Naive average OLS p-value: {np.mean(naive_ols_p_values)}\")\nprint(f\"Average OLS w/ clustered errors p-value: {np.mean(naive_ols_clustered_p_values)}\")\nprint(f\"Delta Method average p-value: {np.mean(delta_p_values)}\")\n</pre> # Let's generate some fake switchback data (the clusters here would be city and date N = 200_000  naive_ttest_p_values = [] naive_ols_clustered_p_values = [] naive_ols_p_values = [] delta_p_values = []  for _ in range(200):     naive_ttest_pval, naive_ols_pval, naive_ols_clustered_pval, delta_pval = compare_vanilla_cuped(N, num_users = 5_000, treatment_effect= 0)     naive_ttest_p_values.append(naive_ttest_pval)     naive_ols_p_values.append(naive_ols_pval)     naive_ols_clustered_p_values.append(naive_ols_clustered_pval)     delta_p_values.append(delta_pval)  print(f\"Naive average p-value t-test: {np.mean(naive_ttest_p_values)}\") print(f\"Naive average OLS p-value: {np.mean(naive_ols_p_values)}\") print(f\"Average OLS w/ clustered errors p-value: {np.mean(naive_ols_clustered_p_values)}\") print(f\"Delta Method average p-value: {np.mean(delta_p_values)}\") <pre>Naive average p-value t-test: 0.08820678021915838\nNaive average OLS p-value: 0.1380091920705585\nAverage OLS w/ clustered errors p-value: 0.5071697690571216\nDelta Method average p-value: 0.5072002733744696\n</pre> In\u00a0[4]: Copied! <pre>positives_prop_ttest = np.mean([pval&lt; 0.05 for pval in naive_ttest_p_values])\npositives_prop_ols = np.mean([pval&lt; 0.05 for pval in naive_ols_p_values])\npositives_user_clustered_ols = np.mean([pval&lt; 0.05 for pval in naive_ols_clustered_p_values])\npositives_delta = np.mean([pval&lt; 0.05 for pval in delta_p_values])\n\nprint(f\"Naive false positives rate t-test: {positives_prop_ttest}\")\nprint(f\"Naive false positives rate OLS p-value: {positives_prop_ols}\")\nprint(f\"False positives rate OLS w/ clustered errors: {positives_user_clustered_ols}\")\nprint(f\"False positives rate Delta Method: {positives_delta}\")\n</pre> positives_prop_ttest = np.mean([pval&lt; 0.05 for pval in naive_ttest_p_values]) positives_prop_ols = np.mean([pval&lt; 0.05 for pval in naive_ols_p_values]) positives_user_clustered_ols = np.mean([pval&lt; 0.05 for pval in naive_ols_clustered_p_values]) positives_delta = np.mean([pval&lt; 0.05 for pval in delta_p_values])  print(f\"Naive false positives rate t-test: {positives_prop_ttest}\") print(f\"Naive false positives rate OLS p-value: {positives_prop_ols}\") print(f\"False positives rate OLS w/ clustered errors: {positives_user_clustered_ols}\") print(f\"False positives rate Delta Method: {positives_delta}\") <pre>Naive false positives rate t-test: 0.78\nNaive false positives rate OLS p-value: 0.66\nFalse positives rate OLS w/ clustered errors: 0.065\nFalse positives rate Delta Method: 0.065\n</pre> In\u00a0[5]: Copied! <pre># Do a subfigure with four plots 2x2 of the p-values add also uniform distribution line\nnum_bins = 10\nuniform_line = len(naive_ttest_p_values)/num_bins\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs[0, 0].hist(naive_ttest_p_values, bins=num_bins, alpha=0.5, label='T-test with randomization unit', color='r')\naxs[0, 0].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution')\naxs[0, 0].set_title('T-test with randomization unit')\naxs[0, 0].legend()\naxs[0, 1].hist(naive_ols_p_values, bins=num_bins, alpha=0.5, label='Naive OLS', color='y')\naxs[0, 1].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution')\naxs[0, 1].set_title('Naive OLS')\naxs[0, 1].legend()\naxs[1, 0].hist(naive_ols_clustered_p_values, bins=num_bins, alpha=0.5, label='Clustered Errors OLS', color='b')\naxs[1, 0].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution')\naxs[1, 0].set_title('Clustered Errors OLS')\naxs[1, 0].legend()\naxs[1, 1].hist(delta_p_values, bins=num_bins, alpha=0.5, label='Delta Method', color='g')\naxs[1, 1].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution')\naxs[1, 1].set_title('Delta Method')\naxs[1, 1].legend()\nplt.show()\n</pre> # Do a subfigure with four plots 2x2 of the p-values add also uniform distribution line num_bins = 10 uniform_line = len(naive_ttest_p_values)/num_bins  fig, axs = plt.subplots(2, 2, figsize=(10, 10)) axs[0, 0].hist(naive_ttest_p_values, bins=num_bins, alpha=0.5, label='T-test with randomization unit', color='r') axs[0, 0].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution') axs[0, 0].set_title('T-test with randomization unit') axs[0, 0].legend() axs[0, 1].hist(naive_ols_p_values, bins=num_bins, alpha=0.5, label='Naive OLS', color='y') axs[0, 1].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution') axs[0, 1].set_title('Naive OLS') axs[0, 1].legend() axs[1, 0].hist(naive_ols_clustered_p_values, bins=num_bins, alpha=0.5, label='Clustered Errors OLS', color='b') axs[1, 0].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution') axs[1, 0].set_title('Clustered Errors OLS') axs[1, 0].legend() axs[1, 1].hist(delta_p_values, bins=num_bins, alpha=0.5, label='Delta Method', color='g') axs[1, 1].axhline(y=uniform_line, color='black', linestyle='--', label='Uniform distribution') axs[1, 1].set_title('Delta Method') axs[1, 1].legend() plt.show() <p>In the A/A test we should expect a distribution resembling a uniform distribution. As we can see unless we are using the Delta Method or Clustered Errors this is not the case. The advantage of the Delta Method lies in its performance as compared to OLS.</p> <p>It is known that Naive t-test will have a smaller estimation of the variance, increasing the false positive rate</p> In\u00a0[6]: Copied! <pre>N = 200_000\nnum_users=5_000\nuser_sample_mean=0.3\nuser_standard_error=0.5\ntreatment_effect=0.1\n\n# data generation\ndata = generate_data(N, num_users, user_sample_mean, user_standard_error, treatment_effect)\n\n# Delta Method\nanalysis_delta = DeltaMethodAnalysis(\n    cluster_cols=[\"user\"]\n)\ndelta_pval = analysis_delta.get_pvalue(data)\n\n\nprint(f\"Vanilla Delta Method p-value: {delta_pval}\")\n</pre> N = 200_000 num_users=5_000 user_sample_mean=0.3 user_standard_error=0.5 treatment_effect=0.1  # data generation data = generate_data(N, num_users, user_sample_mean, user_standard_error, treatment_effect)  # Delta Method analysis_delta = DeltaMethodAnalysis(     cluster_cols=[\"user\"] ) delta_pval = analysis_delta.get_pvalue(data)   print(f\"Vanilla Delta Method p-value: {delta_pval}\") <pre>Vanilla Delta Method p-value: 0.0\n</pre> In\u00a0[7]: Copied! <pre># Splitter and perturbator\nsw = ClusteredSplitter(\n    cluster_cols=[\"user\"],\n)\n\nperturbator = BinaryPerturbator(\n    average_effect=0.02,\n)\n\n# Vainilla Delta Method\nanalysis = DeltaMethodAnalysis(\n    cluster_cols=[\"user\"],\n\n)\npw_vanilla = PowerAnalysis(\n    perturbator=perturbator,\n    splitter=sw,\n    analysis=analysis,\n    n_simulations=50,\n)\n\ndata = generate_data(N, num_users, user_sample_mean, user_standard_error, treatment_effect = 0)\ndata = data.drop(columns=['treatment'])\npower = pw_vanilla.power_analysis(data)\nprint(f\"Delta Method: {power = }\")\n</pre> # Splitter and perturbator sw = ClusteredSplitter(     cluster_cols=[\"user\"], )  perturbator = BinaryPerturbator(     average_effect=0.02, )  # Vainilla Delta Method analysis = DeltaMethodAnalysis(     cluster_cols=[\"user\"],  ) pw_vanilla = PowerAnalysis(     perturbator=perturbator,     splitter=sw,     analysis=analysis,     n_simulations=50, )  data = generate_data(N, num_users, user_sample_mean, user_standard_error, treatment_effect = 0) data = data.drop(columns=['treatment']) power = pw_vanilla.power_analysis(data) print(f\"Delta Method: {power = }\")   <pre>Delta Method: power = np.float64(0.56)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"delta_method.html#delta-method-analysis-tutorial","title":"Delta Method Analysis Tutorial\u00b6","text":"<p>This notebook shows how the DeltaMethodAnalysis class is performing the Delta Method for a simple ratio metric. In this case the cluster column is going to be at user level for the current example.</p> <p>We start by exemplifying why the method is useful and when it has to be used</p>"},{"location":"delta_method.html#a-a-test-for-validation","title":"A-A test for validation\u00b6","text":""},{"location":"delta_method.html#check-p-val-distributions","title":"Check p-val distributions\u00b6","text":""},{"location":"delta_method.html#example-of-usage","title":"Example of usage\u00b6","text":""},{"location":"delta_method.html#example-of-usage-in-power-analysis","title":"Example of usage in Power Analysis\u00b6","text":""},{"location":"e2e_mde.html","title":"End-to-end: Power Analysis","text":"In\u00a0[1]: Copied! <pre>import random\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom cluster_experiments import NormalPowerAnalysis\n\nnp.random.seed(42)\nrandom.seed(42)\n\n# Constants\nN = 10000  # Number of orders\nNUM_CUSTOMERS = 1000  # Unique customers\n\ndef generate_customers(num_customers):\n    \"\"\"Generate unique customers with a mean order value based on age.\"\"\"\n    customer_ids = np.arange(1, num_customers + 1)\n    customer_ages = np.random.randint(20, 60, size=num_customers)\n    mean_order_values = 50 + 0.8 * customer_ages + np.random.normal(0, 10, size=num_customers)\n\n    return pd.DataFrame({\n        \"customer_id\": customer_ids,\n        \"customer_age\": customer_ages,\n        \"mean_order_value\": mean_order_values\n    })\n\ndef sample_orders(customers, num_orders):\n    \"\"\"Sample customers and generate order-level data.\"\"\"\n    sampled_customers = np.random.choice(customers[\"customer_id\"], size=num_orders)\n    return pd.DataFrame({\"customer_id\": sampled_customers}).merge(customers, on=\"customer_id\", how=\"left\")\n\ndef generate_orders(customers, num_orders):\n    \"\"\"Full order generation pipeline using .assign() for cleaner transformations.\"\"\"\n    date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\")\n\n    return (\n        sample_orders(customers, num_orders)\n        .assign(\n            order_value=lambda df: df[\"mean_order_value\"] + np.random.normal(0, 5, size=len(df)),\n            delivery_time=lambda df: 8 + np.sin(df[\"customer_id\"] / 10) + np.random.normal(0, 0.5, size=len(df)),\n            city=lambda df: np.random.choice([\"NYC\", \"LA\"], size=len(df)),\n            date=lambda df: np.random.choice(date_range, size=len(df))\n        )\n        .drop(columns=[\"mean_order_value\"])  # Remove intermediate column\n    )\n\n\ndef plot_mdes(mdes, x_lim=40, y_value=3):\n    sns.lineplot(\n        data=pd.DataFrame(mdes),\n        x=\"experiment_length\",\n        y=\"mde\",\n    )\n\n    sns.lineplot(\n        x=[0, x_lim],\n        y=[y_value, y_value],\n        color=\"red\",\n        linestyle=\"--\",\n    )\n\ndef get_length(mdes, mde_value):\n    return min(x[\"experiment_length\"] for x in mdes if x[\"mde\"] &lt; mde_value)\n\ndef get_length_print(mdes, mde_value):\n    length = get_length(mdes, mde_value)\n    print(f\"Minimum experiment length to detect MDE of {mde_value}: {length}\")\n\n# Run the pipeline\ncustomers = generate_customers(NUM_CUSTOMERS)\nexperiment_data = generate_orders(customers, N)\n\nprint(experiment_data.head())\n</pre> import random  import numpy as np import pandas as pd import seaborn as sns  from cluster_experiments import NormalPowerAnalysis  np.random.seed(42) random.seed(42)  # Constants N = 10000  # Number of orders NUM_CUSTOMERS = 1000  # Unique customers  def generate_customers(num_customers):     \"\"\"Generate unique customers with a mean order value based on age.\"\"\"     customer_ids = np.arange(1, num_customers + 1)     customer_ages = np.random.randint(20, 60, size=num_customers)     mean_order_values = 50 + 0.8 * customer_ages + np.random.normal(0, 10, size=num_customers)      return pd.DataFrame({         \"customer_id\": customer_ids,         \"customer_age\": customer_ages,         \"mean_order_value\": mean_order_values     })  def sample_orders(customers, num_orders):     \"\"\"Sample customers and generate order-level data.\"\"\"     sampled_customers = np.random.choice(customers[\"customer_id\"], size=num_orders)     return pd.DataFrame({\"customer_id\": sampled_customers}).merge(customers, on=\"customer_id\", how=\"left\")  def generate_orders(customers, num_orders):     \"\"\"Full order generation pipeline using .assign() for cleaner transformations.\"\"\"     date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\")      return (         sample_orders(customers, num_orders)         .assign(             order_value=lambda df: df[\"mean_order_value\"] + np.random.normal(0, 5, size=len(df)),             delivery_time=lambda df: 8 + np.sin(df[\"customer_id\"] / 10) + np.random.normal(0, 0.5, size=len(df)),             city=lambda df: np.random.choice([\"NYC\", \"LA\"], size=len(df)),             date=lambda df: np.random.choice(date_range, size=len(df))         )         .drop(columns=[\"mean_order_value\"])  # Remove intermediate column     )   def plot_mdes(mdes, x_lim=40, y_value=3):     sns.lineplot(         data=pd.DataFrame(mdes),         x=\"experiment_length\",         y=\"mde\",     )      sns.lineplot(         x=[0, x_lim],         y=[y_value, y_value],         color=\"red\",         linestyle=\"--\",     )  def get_length(mdes, mde_value):     return min(x[\"experiment_length\"] for x in mdes if x[\"mde\"] &lt; mde_value)  def get_length_print(mdes, mde_value):     length = get_length(mdes, mde_value)     print(f\"Minimum experiment length to detect MDE of {mde_value}: {length}\")  # Run the pipeline customers = generate_customers(NUM_CUSTOMERS) experiment_data = generate_orders(customers, N)  print(experiment_data.head())  <pre>   customer_id  customer_age  order_value  delivery_time city       date\n0          611            39    72.199186       7.495150  NYC 2024-01-28\n1          704            23    81.011831       8.887809   LA 2024-03-31\n2          243            52    89.979294       7.851989   LA 2024-03-22\n3          831            23    65.235192       8.933512  NYC 2024-03-17\n4          561            58    91.154383       6.976415   LA 2024-03-20\n</pre> In\u00a0[2]: Copied! <pre>mde_order_split = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"non_clustered\",\n    \"analysis\": \"ols\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n})\n\nmdes = mde_order_split.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=(1, 4, 7, 14, 21, 28),\n    n_simulations=5\n)\n\nmde_value = 3\n\nplot_mdes(mdes, y_value=mde_value, x_lim=30)\n</pre> mde_order_split = NormalPowerAnalysis.from_dict({     \"splitter\": \"non_clustered\",     \"analysis\": \"ols\",     \"time_col\": \"date\",     \"target_col\": \"order_value\", })  mdes = mde_order_split.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=(1, 4, 7, 14, 21, 28),     n_simulations=5 )  mde_value = 3  plot_mdes(mdes, y_value=mde_value, x_lim=30) In\u00a0[3]: Copied! <pre>get_length_print(mdes, mde_value)\n</pre> get_length_print(mdes, mde_value) <pre>Minimum experiment length to detect MDE of 3: 7\n</pre> In\u00a0[4]: Copied! <pre>mde = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"clustered\",\n    \"analysis\": \"clustered_ols\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n    \"cluster_cols\": [\"customer_id\"]\n})\n</pre> mde = NormalPowerAnalysis.from_dict({     \"splitter\": \"clustered\",     \"analysis\": \"clustered_ols\",     \"time_col\": \"date\",     \"target_col\": \"order_value\",     \"cluster_cols\": [\"customer_id\"] }) In\u00a0[5]: Copied! <pre>mdes = mde.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=(7, 14, 21, 28),\n    n_simulations=5\n)\n</pre> mdes = mde.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=(7, 14, 21, 28),     n_simulations=5 ) In\u00a0[6]: Copied! <pre>mde_value = 3\nplot_mdes(mdes, y_value=mde_value, x_lim=30)\n</pre> mde_value = 3 plot_mdes(mdes, y_value=mde_value, x_lim=30) In\u00a0[7]: Copied! <pre>get_length_print(mdes, mde_value)\n</pre> get_length_print(mdes, mde_value) <pre>Minimum experiment length to detect MDE of 3: 21\n</pre> <p>The age of the customer is a good predictor of the orde value that is not impacted by the treatment. We can use this covariate to adjust our analysis and decrease mde. We see that mde is smaller in this case, because we are using a covariate that is not impacted by the treatment.</p> In\u00a0[8]: Copied! <pre>mde_variance_reduction = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"clustered\",\n    \"analysis\": \"clustered_ols\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n    \"cluster_cols\": [\"customer_id\"],\n    \"covariates\": [\"customer_age\"]\n})\n</pre> mde_variance_reduction = NormalPowerAnalysis.from_dict({     \"splitter\": \"clustered\",     \"analysis\": \"clustered_ols\",     \"time_col\": \"date\",     \"target_col\": \"order_value\",     \"cluster_cols\": [\"customer_id\"],     \"covariates\": [\"customer_age\"] }) In\u00a0[9]: Copied! <pre>mdes = mde_variance_reduction.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=(1, 4, 7, 14, 21, 28, 35, 42),\n    n_simulations=5\n)\n</pre> mdes = mde_variance_reduction.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=(1, 4, 7, 14, 21, 28, 35, 42),     n_simulations=5 ) In\u00a0[10]: Copied! <pre>plot_mdes(mdes, y_value=mde_value, x_lim=30)\n</pre> plot_mdes(mdes, y_value=mde_value, x_lim=30) In\u00a0[11]: Copied! <pre>get_length_print(mdes, mde_value)\n</pre> get_length_print(mdes, mde_value) <pre>Minimum experiment length to detect MDE of 3: 7\n</pre> <p>This is the last example of power analysis.</p> <p>We assume data from Feb onwards is used to run the power analysis (it's the data we take as experimental, though it is actually pre-experimental data). Data from Jan is taken as pre-experimental data. We simulate that the experiment happened after Feb. We do this because we need pre-experimental data to train the Cupac Model, which we didn't do before.</p> <p>We use cupac model with customer id and age features (order value has a non-linear relationship with customer id, that's why we don't add it a single covariate). We use the same data as before, but we train the model with pre-experimental data. We see that mde is smaller in this case, because we are using a better covariate that is not impacted by the treatment.</p> <p>In this case we cannot init by dict because we're using cupac, but happy to review a PR that includes this :) In this case we need to create splitter and analysis classes.</p> In\u00a0[12]: Copied! <pre>pre_experiment_df = experiment_data.query(\"date &lt; '2024-02-01'\")\nexperiment_df = experiment_data.query(\"date &gt;= '2024-02-01'\")\n</pre> pre_experiment_df = experiment_data.query(\"date &lt; '2024-02-01'\") experiment_df = experiment_data.query(\"date &gt;= '2024-02-01'\") In\u00a0[13]: Copied! <pre>from cluster_experiments import ClusteredSplitter, ClusteredOLSAnalysis\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\nsplitter = ClusteredSplitter(\n    cluster_cols=[\"customer_id\"],\n)\nols = ClusteredOLSAnalysis(\n    target_col=\"order_value\",\n    covariates=[\"estimate_order_value\"],\n    cluster_cols=[\"customer_id\"],\n)\n\npwr = NormalPowerAnalysis(\n    splitter=splitter,\n    analysis=ols,\n    cupac_model=HistGradientBoostingRegressor(),\n    time_col=\"date\",\n    target_col=\"order_value\",\n    features_cupac_model=[\"customer_id\", \"customer_age\"],\n)\n</pre> from cluster_experiments import ClusteredSplitter, ClusteredOLSAnalysis from sklearn.ensemble import HistGradientBoostingRegressor   splitter = ClusteredSplitter(     cluster_cols=[\"customer_id\"], ) ols = ClusteredOLSAnalysis(     target_col=\"order_value\",     covariates=[\"estimate_order_value\"],     cluster_cols=[\"customer_id\"], )  pwr = NormalPowerAnalysis(     splitter=splitter,     analysis=ols,     cupac_model=HistGradientBoostingRegressor(),     time_col=\"date\",     target_col=\"order_value\",     features_cupac_model=[\"customer_id\", \"customer_age\"], )  In\u00a0[14]: Copied! <pre>mde_cupac = pwr.mde_time_line(\n    experiment_df,\n    pre_experiment_df,\n    powers=[0.8],\n    experiment_length=range(1, 42, 2),\n    n_simulations=5,\n)\n\nplot_mdes(mde_cupac, y_value=mde_value, x_lim=30)\n</pre> mde_cupac = pwr.mde_time_line(     experiment_df,     pre_experiment_df,     powers=[0.8],     experiment_length=range(1, 42, 2),     n_simulations=5, )  plot_mdes(mde_cupac, y_value=mde_value, x_lim=30) In\u00a0[15]: Copied! <pre>get_length_print(mde_cupac, mde_value)\n</pre> get_length_print(mde_cupac, mde_value) <pre>Minimum experiment length to detect MDE of 3: 5\n</pre> In\u00a0[16]: Copied! <pre>real_experiment_data = experiment_data.query(\"date &gt;= '2024-03-01' and date &lt; '2024-03-21'\")\nreal_pre_experiment_data = experiment_data.query(\"date &lt; '2024-03-01'\")\nreal_experiment_data\n\nfrom cluster_experiments import ClusteredSplitter, ConstantPerturbator\n\n\n# Add effect on the order value\nsplitter = ClusteredSplitter(\n    cluster_cols=[\"customer_id\"],\n)\nperturbator = ConstantPerturbator(\n    target_col=\"order_value\",\n)\n\nreal_experiment_data = splitter.assign_treatment_df(real_experiment_data)\nreal_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=1.5)\n\n# Add effect on the delivery time\nperturbator = ConstantPerturbator(\n    target_col=\"delivery_time\",\n)\nreal_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=.2)\nreal_experiment_data\n</pre> real_experiment_data = experiment_data.query(\"date &gt;= '2024-03-01' and date &lt; '2024-03-21'\") real_pre_experiment_data = experiment_data.query(\"date &lt; '2024-03-01'\") real_experiment_data  from cluster_experiments import ClusteredSplitter, ConstantPerturbator   # Add effect on the order value splitter = ClusteredSplitter(     cluster_cols=[\"customer_id\"], ) perturbator = ConstantPerturbator(     target_col=\"order_value\", )  real_experiment_data = splitter.assign_treatment_df(real_experiment_data) real_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=1.5)  # Add effect on the delivery time perturbator = ConstantPerturbator(     target_col=\"delivery_time\", ) real_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=.2) real_experiment_data Out[16]: customer_id customer_age order_value delivery_time city date treatment 0 831 23 65.235192 8.933512 NYC 2024-03-17 A 1 561 58 92.654383 7.176415 LA 2024-03-20 B 2 391 31 101.486430 9.643470 LA 2024-03-16 B 3 277 57 91.538628 8.410730 LA 2024-03-12 B 4 884 30 94.140176 9.091150 NYC 2024-03-20 B ... ... ... ... ... ... ... ... 2159 13 59 91.067824 7.331142 NYC 2024-03-01 A 2160 299 43 91.388094 6.401728 NYC 2024-03-05 A 2161 787 36 81.456158 7.185609 NYC 2024-03-15 A 2162 149 44 82.405384 8.764580 LA 2024-03-20 B 2163 211 39 67.695851 8.437365 LA 2024-03-06 B <p>2164 rows \u00d7 7 columns</p> <p>We run the analysis with and without covariate adjustment. We use city as a dimension, showing results in the overall population and by city.</p> <p>We see that the effect is larger when we use covariate adjustment, because we are using a better covariate that is not impacted by the treatment. We also show how to calculate the MDE for a given power and effect size.</p> In\u00a0[17]: Copied! <pre>from cluster_experiments import AnalysisPlan, HypothesisTest, Variant, SimpleMetric\n\nplan = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"name\": \"order_value\"},\n        {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"dimensions\": [\n        {\"name\": \"city\", \"values\": [\"NYC\", \"LA\"]},\n    ],\n    \"analysis_type\": \"clustered_ols\",\n    \"analysis_config\": {\"cluster_cols\": [\"customer_id\"]},\n})\n</pre> from cluster_experiments import AnalysisPlan, HypothesisTest, Variant, SimpleMetric  plan = AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"name\": \"order_value\"},         {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"dimensions\": [         {\"name\": \"city\", \"values\": [\"NYC\", \"LA\"]},     ],     \"analysis_type\": \"clustered_ols\",     \"analysis_config\": {\"cluster_cols\": [\"customer_id\"]}, })  <p>We see that CIs contain the true effect, and CIs are pretty big.</p> In\u00a0[18]: Copied! <pre># Run the analysis plan\nplan.analyze(real_experiment_data).to_dataframe()\n</pre> # Run the analysis plan plan.analyze(real_experiment_data).to_dataframe() Out[18]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 81.929798 84.346509 clustered_ols 2.416712 0.315398 4.518025 0.024187 1.072118 __total_dimension total 0.05 1 AOV A B 81.778970 84.030982 clustered_ols 2.252012 -0.147058 4.651082 0.065794 1.224038 city NYC 0.05 2 AOV A B 82.068329 84.653585 clustered_ols 2.585256 0.198567 4.971944 0.033751 1.217721 city LA 0.05 3 delivery_time A B 8.012237 8.182237 clustered_ols 0.169999 0.057150 0.282849 0.003152 0.057578 __total_dimension total 0.05 4 delivery_time A B 7.995034 8.140337 clustered_ols 0.145303 0.012721 0.277885 0.031712 0.067645 city NYC 0.05 5 delivery_time A B 8.028038 8.223014 clustered_ols 0.194976 0.060193 0.329759 0.004579 0.068768 city LA 0.05 <p>Now we use covariate adjustment with customer age. We see that the effect in order value is closer to the true effect, but the effect in delivery time is just as bad. This is because in our data generation process we used customer age as a covariate for order value but not for delivery time.</p> In\u00a0[19]: Copied! <pre>plan_covariates = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"name\": \"order_value\"},\n        {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"dimensions\": [\n        {\"name\": \"city\", \"values\": [\"NYC\", \"LA\"]},\n    ],\n    \"analysis_type\": \"clustered_ols\",\n    \"analysis_config\": {\n        \"cluster_cols\": [\"customer_id\"],\n        \"covariates\": [\"customer_age\"],\n    },\n})\n</pre> plan_covariates = AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"name\": \"order_value\"},         {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"dimensions\": [         {\"name\": \"city\", \"values\": [\"NYC\", \"LA\"]},     ],     \"analysis_type\": \"clustered_ols\",     \"analysis_config\": {         \"cluster_cols\": [\"customer_id\"],         \"covariates\": [\"customer_age\"],     }, }) In\u00a0[20]: Copied! <pre># Run the analysis plan\nplan_covariates.analyze(real_experiment_data).to_dataframe()\n</pre> # Run the analysis plan plan_covariates.analyze(real_experiment_data).to_dataframe() Out[20]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 81.929798 84.346509 clustered_ols 2.588424 0.945550 4.231298 0.002015 0.838216 __total_dimension total 0.05 1 AOV A B 81.778970 84.030982 clustered_ols 2.112448 0.248134 3.976762 0.026363 0.951198 city NYC 0.05 2 AOV A B 82.068329 84.653585 clustered_ols 3.058242 1.166815 4.949668 0.001529 0.965031 city LA 0.05 3 delivery_time A B 8.012237 8.182237 clustered_ols 0.169061 0.056424 0.281697 0.003263 0.057469 __total_dimension total 0.05 4 delivery_time A B 7.995034 8.140337 clustered_ols 0.146025 0.014186 0.277864 0.029942 0.067266 city NYC 0.05 5 delivery_time A B 8.028038 8.223014 clustered_ols 0.192335 0.057672 0.326997 0.005120 0.068706 city LA 0.05 <p>Now we run analysis using cupac model. We see that the effect in both metrics is closer to the true effect, and the CIs are smaller.</p> In\u00a0[21]: Copied! <pre>plan_cupac = AnalysisPlan(\n    tests=[\n        HypothesisTest(\n            metric=SimpleMetric(alias=\"AOV\", name=\"order_value\"),\n            analysis_type=\"clustered_ols\",\n            analysis_config={\n                \"cluster_cols\": [\"customer_id\"],\n                \"covariates\": [\"customer_age\", \"estimate_order_value\"],\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\"],\n                \"target_col\": \"order_value\",\n            },\n        ),\n        HypothesisTest(\n            metric=SimpleMetric(alias=\"delivery_time\", name=\"delivery_time\"),\n            analysis_type=\"clustered_ols\",\n            analysis_config={\n                \"cluster_cols\": [\"customer_id\"],\n                \"covariates\": [\"customer_age\", \"estimate_delivery_time\"],\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\"],\n                \"target_col\": \"delivery_time\",\n            },\n        ),\n    ],\n    variants=[\n        Variant(name=\"A\", is_control=True),\n        Variant(name=\"B\", is_control=False),\n    ],\n    variant_col=\"treatment\",\n)\n</pre> plan_cupac = AnalysisPlan(     tests=[         HypothesisTest(             metric=SimpleMetric(alias=\"AOV\", name=\"order_value\"),             analysis_type=\"clustered_ols\",             analysis_config={                 \"cluster_cols\": [\"customer_id\"],                 \"covariates\": [\"customer_age\", \"estimate_order_value\"],             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\"],                 \"target_col\": \"order_value\",             },         ),         HypothesisTest(             metric=SimpleMetric(alias=\"delivery_time\", name=\"delivery_time\"),             analysis_type=\"clustered_ols\",             analysis_config={                 \"cluster_cols\": [\"customer_id\"],                 \"covariates\": [\"customer_age\", \"estimate_delivery_time\"],             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\"],                 \"target_col\": \"delivery_time\",             },         ),     ],     variants=[         Variant(name=\"A\", is_control=True),         Variant(name=\"B\", is_control=False),     ],     variant_col=\"treatment\", ) In\u00a0[22]: Copied! <pre>plan_cupac.analyze(\n    real_experiment_data,\n    real_pre_experiment_data,\n).to_dataframe()\n</pre> plan_cupac.analyze(     real_experiment_data,     real_pre_experiment_data, ).to_dataframe() Out[22]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 81.929798 84.346509 clustered_ols 1.803072 0.678043 2.928101 1.682500e-03 0.574005 __total_dimension total 0.05 1 delivery_time A B 8.012237 8.182237 clustered_ols 0.195599 0.150556 0.240643 1.723474e-17 0.022982 __total_dimension total 0.05 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"e2e_mde.html#end-to-end-power-analysis","title":"End-to-end: Power Analysis\u00b6","text":"<p>We'll show the different functionalities of cluster_experiments, which are:</p> <ul> <li>MDE calculation</li> <li>MDE calculation in cluster-randomized experiments</li> <li>MDE calculation with simple covariate adjustment</li> <li>MDE calculation with cupac (adjustment via ML models)</li> <li>Inference for all the cases above</li> </ul>"},{"location":"e2e_mde.html#data-generation","title":"Data generation\u00b6","text":"<p>We create some pre-experimental data that we could use to run power analysis.</p> <p>We have a dataframe with orders and customers, each customer may have many orders, and the two target metrics are delivery time and order value.</p>"},{"location":"e2e_mde.html#power-analysis","title":"Power analysis\u00b6","text":""},{"location":"e2e_mde.html#order-level-split","title":"Order-level split\u00b6","text":"<p>Assume we run an AB test randomizing at order level, in this case the dataset is at the order level, so we don't need cluster-randomized analysis, we just use regular splitter and analyse with ols. The following code shows the mde on order_value given some experiment length (1, 2, 3 and 4 weeks).</p>"},{"location":"e2e_mde.html#customer-level-split","title":"Customer-level split\u00b6","text":"<p>Assume we randomize at customer level, in this case we need to use clustered_ols to run the power analysis. The following code shows the mde on order_value given some experiment length (1, 2, 3 and 4 weeks).</p>"},{"location":"e2e_mde.html#analysis","title":"Analysis\u00b6","text":"<p>Now we run analysis assuming that the experiment run after 2024-03-01 for 3 weeks. We simulate some fake effects (1.5 in order value and 0.2 in delivery time). We use functionalities in cluster_experiments to simulate the experiment.</p>"},{"location":"e2e_mde_delta.html","title":"End-to-end: Delta Method","text":"In\u00a0[\u00a0]: Copied! <pre>import random\nimport warnings\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom cluster_experiments import NormalPowerAnalysis\n\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\nrandom.seed(42)\n\n# Constants\nN = 100000  # Number of orders\nNUM_CUSTOMERS = 6000  # Unique customers\n\ndef generate_customers(num_customers):\n    \"\"\"Generate unique customers with a mean order value based on age.\"\"\"\n    customer_ids = np.arange(1, num_customers + 1)\n    customer_ages = np.random.randint(20, 60, size=num_customers)\n    customer_historical_orders = np.random.poisson(5, size=num_customers)\n    mean_order_values = 50 + 2.5 * customer_ages - 2 * (customer_ages &lt;= 30) + 2 * (customer_historical_orders &gt;= 8) + np.random.normal(0, 15, size=num_customers)\n\n    return pd.DataFrame({\n        \"customer_id\": customer_ids,\n        \"customer_age\": customer_ages,\n        \"mean_order_value\": mean_order_values,\n        \"historical_orders\": customer_historical_orders\n    })\n\ndef sample_orders(customers, num_orders):\n    \"\"\"Sample customers and generate order-level data.\"\"\"\n    sampled_customers = np.random.choice(customers[\"customer_id\"], size=num_orders)\n    return pd.DataFrame({\"customer_id\": sampled_customers}).merge(customers, on=\"customer_id\", how=\"left\")\n\ndef generate_orders(customers, num_orders):\n    \"\"\"Full order generation pipeline using .assign() for cleaner transformations.\"\"\"\n    date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\")\n\n    return (\n        sample_orders(customers, num_orders)\n        .assign(\n            order_value=lambda df: df[\"mean_order_value\"] + np.random.normal(0, 15, size=len(df)),\n            delivery_time=lambda df: 8 + np.sin(df[\"customer_id\"] / 10) + np.random.normal(0, 0.5, size=len(df)),\n            city=lambda df: np.random.choice([\"NYC\", \"LA\"], size=len(df)),\n            date=lambda df: np.random.choice(date_range, size=len(df))\n        )\n        .drop(columns=[\"mean_order_value\"])  # Remove intermediate column\n    )\n\n\ndef plot_mdes(mdes, x_lim=40, y_value=3):\n    sns.lineplot(\n        data=pd.DataFrame(mdes),\n        x=\"experiment_length\",\n        y=\"mde\",\n    )\n\n    sns.lineplot(\n        x=[0, x_lim],\n        y=[y_value, y_value],\n        color=\"red\",\n        linestyle=\"--\",\n    )\n\ndef get_length(mdes, mde_value):\n    if any(x[\"mde\"] &lt;= mde_value for x in mdes):\n        return min(x[\"experiment_length\"] for x in mdes if x[\"mde\"] &lt; mde_value)\n    return None\n\ndef get_length_print(mdes, mde_value):\n    length = get_length(mdes, mde_value)\n    print(f\"Minimum experiment length to detect MDE of {mde_value}: {length}\")\n\n# Run the pipeline\ncustomers = generate_customers(NUM_CUSTOMERS)\nexperiment_data = generate_orders(customers, N).assign(\n    one=1\n)\n\nprint(experiment_data.head())\n</pre> import random import warnings import time  import numpy as np import pandas as pd import seaborn as sns  from cluster_experiments import NormalPowerAnalysis   warnings.filterwarnings('ignore') np.random.seed(42) random.seed(42)  # Constants N = 100000  # Number of orders NUM_CUSTOMERS = 6000  # Unique customers  def generate_customers(num_customers):     \"\"\"Generate unique customers with a mean order value based on age.\"\"\"     customer_ids = np.arange(1, num_customers + 1)     customer_ages = np.random.randint(20, 60, size=num_customers)     customer_historical_orders = np.random.poisson(5, size=num_customers)     mean_order_values = 50 + 2.5 * customer_ages - 2 * (customer_ages &lt;= 30) + 2 * (customer_historical_orders &gt;= 8) + np.random.normal(0, 15, size=num_customers)      return pd.DataFrame({         \"customer_id\": customer_ids,         \"customer_age\": customer_ages,         \"mean_order_value\": mean_order_values,         \"historical_orders\": customer_historical_orders     })  def sample_orders(customers, num_orders):     \"\"\"Sample customers and generate order-level data.\"\"\"     sampled_customers = np.random.choice(customers[\"customer_id\"], size=num_orders)     return pd.DataFrame({\"customer_id\": sampled_customers}).merge(customers, on=\"customer_id\", how=\"left\")  def generate_orders(customers, num_orders):     \"\"\"Full order generation pipeline using .assign() for cleaner transformations.\"\"\"     date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\")      return (         sample_orders(customers, num_orders)         .assign(             order_value=lambda df: df[\"mean_order_value\"] + np.random.normal(0, 15, size=len(df)),             delivery_time=lambda df: 8 + np.sin(df[\"customer_id\"] / 10) + np.random.normal(0, 0.5, size=len(df)),             city=lambda df: np.random.choice([\"NYC\", \"LA\"], size=len(df)),             date=lambda df: np.random.choice(date_range, size=len(df))         )         .drop(columns=[\"mean_order_value\"])  # Remove intermediate column     )   def plot_mdes(mdes, x_lim=40, y_value=3):     sns.lineplot(         data=pd.DataFrame(mdes),         x=\"experiment_length\",         y=\"mde\",     )      sns.lineplot(         x=[0, x_lim],         y=[y_value, y_value],         color=\"red\",         linestyle=\"--\",     )  def get_length(mdes, mde_value):     if any(x[\"mde\"] &lt;= mde_value for x in mdes):         return min(x[\"experiment_length\"] for x in mdes if x[\"mde\"] &lt; mde_value)     return None  def get_length_print(mdes, mde_value):     length = get_length(mdes, mde_value)     print(f\"Minimum experiment length to detect MDE of {mde_value}: {length}\")  # Run the pipeline customers = generate_customers(NUM_CUSTOMERS) experiment_data = generate_orders(customers, N).assign(     one=1 )  print(experiment_data.head())  <pre>   customer_id  customer_age  historical_orders  order_value  delivery_time  \\\n0         4162            49                  3   177.806483       9.875481   \n1         1076            58                  5   194.904074       8.637389   \n2         5361            38                  2   153.913243       9.020768   \n3         2036            35                  4   138.692537       8.016001   \n4          390            22                  6    92.457381       8.753228   \n\n  city       date  one  \n0   LA 2024-03-08    1  \n1   LA 2024-03-08    1  \n2  NYC 2024-02-06    1  \n3  NYC 2024-02-05    1  \n4  NYC 2024-03-29    1  \n</pre> In\u00a0[2]: Copied! <pre>mde = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"clustered\",\n    \"analysis\": \"delta\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n    \"scale_col\": \"one\",\n    \"cluster_cols\": [\"customer_id\"]\n})\n</pre> mde = NormalPowerAnalysis.from_dict({     \"splitter\": \"clustered\",     \"analysis\": \"delta\",     \"time_col\": \"date\",     \"target_col\": \"order_value\",     \"scale_col\": \"one\",     \"cluster_cols\": [\"customer_id\"] }) In\u00a0[3]: Copied! <pre>start = time.time()\nmdes = mde.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=(7, 14, 21, 28),\n    n_simulations=5\n)\nprint(f\"Time taken for MDE calculation: {time.time() - start:.2f} seconds\")\n</pre> start = time.time() mdes = mde.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=(7, 14, 21, 28),     n_simulations=5 ) print(f\"Time taken for MDE calculation: {time.time() - start:.2f} seconds\") <pre>Time taken for MDE calculation: 0.44 seconds\n</pre> In\u00a0[4]: Copied! <pre>mde_value = 1.4\nplot_mdes(mdes, y_value=mde_value, x_lim=30)\n</pre> mde_value = 1.4 plot_mdes(mdes, y_value=mde_value, x_lim=30) In\u00a0[5]: Copied! <pre>get_length_print(mdes, mde_value)\n</pre> get_length_print(mdes, mde_value) <pre>Minimum experiment length to detect MDE of 1.4: None\n</pre> <p>Check that results with clustered ols are the same</p> In\u00a0[6]: Copied! <pre>mde = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"clustered\",\n    \"analysis\": \"clustered_ols\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n    \"cluster_cols\": [\"customer_id\"]\n})\n\nstart = time.time()\nmdes = mde.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=(7, 14, 21, 28),\n    n_simulations=5\n)\nprint(f\"Time taken for OLS MDE calculation: {time.time() - start:.2f} seconds\")\n\nplot_mdes(mdes, y_value=mde_value, x_lim=30)\nget_length_print(mdes, mde_value)\n</pre> mde = NormalPowerAnalysis.from_dict({     \"splitter\": \"clustered\",     \"analysis\": \"clustered_ols\",     \"time_col\": \"date\",     \"target_col\": \"order_value\",     \"cluster_cols\": [\"customer_id\"] })  start = time.time() mdes = mde.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=(7, 14, 21, 28),     n_simulations=5 ) print(f\"Time taken for OLS MDE calculation: {time.time() - start:.2f} seconds\")  plot_mdes(mdes, y_value=mde_value, x_lim=30) get_length_print(mdes, mde_value) <pre>Time taken for OLS MDE calculation: 0.63 seconds\nMinimum experiment length to detect MDE of 1.4: None\n</pre> <p>Results with clustered ols are the same as with delta method, but the delta method is faster and more efficient.</p> <p>The age of the customer is a good predictor of the orde value that is not impacted by the treatment. We can use this covariate to adjust our analysis and decrease mde. We see that mde is smaller in this case, because we are using a covariate that is not impacted by the treatment.</p> <p>In delta method, we use this form of CUPED for variance reduction:</p> <p>$$ ATE = \\frac{\\sum_{i=1}^N T_i (Y_i - \\sum_{j=1}^m \\theta_j (Z_{ij} - E[Z_j]) N_i)}{\\sum_{i=1}^N T_i N_i} - \\frac{\\sum_{i=1}^N (1 - T_i) (Y_i - \\sum_{j=1}^m \\theta_j (Z_{ij} - E[Z_j]) N_i)}{\\sum_{i=1}^N (1 - T_i) N_i} $$</p> <p>that is, we apply variance reduction to the numerator, but not the denominator. An issue is open to add the same variance reduction to the denominator, but it is not implemented yet. This follows the same notation as the Deng et al. paper on CUPED.</p> In\u00a0[7]: Copied! <pre>mde_variance_reduction = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"clustered\",\n    \"analysis\": \"delta\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n    \"scale_col\": \"one\",\n    \"cluster_cols\": [\"customer_id\"],\n    \"covariates\": [\"customer_age\"]\n})\n</pre> mde_variance_reduction = NormalPowerAnalysis.from_dict({     \"splitter\": \"clustered\",     \"analysis\": \"delta\",     \"time_col\": \"date\",     \"target_col\": \"order_value\",     \"scale_col\": \"one\",     \"cluster_cols\": [\"customer_id\"],     \"covariates\": [\"customer_age\"] }) In\u00a0[8]: Copied! <pre>start = time.time()\nmdes = []\n# should be simplified with aggregation logic in NormalPowerAnalysis\nfor experiment_length in [7, 14, 21, 28]:\n    filtered_data = experiment_data[experiment_data[\"date\"] &lt;= pd.Timestamp(\"2024-01-01\") + pd.Timedelta(days=experiment_length)]\n    aggregated_data = filtered_data.groupby(\"customer_id\").agg({\n        \"order_value\": \"sum\",\n        \"one\": \"sum\",\n        \"customer_age\": \"mean\",\n        \"date\": \"min\"\n    }).reset_index()\n\n    mde = mde_variance_reduction.mde_time_line(\n        aggregated_data,\n        powers=[0.8],\n        experiment_length=[experiment_length],\n        n_simulations=5\n    )\n    mdes.append(mde[0])\nprint(f\"Time taken for MDE with covariate calculation: {time.time() - start:.2f} seconds\")\n</pre> start = time.time() mdes = [] # should be simplified with aggregation logic in NormalPowerAnalysis for experiment_length in [7, 14, 21, 28]:     filtered_data = experiment_data[experiment_data[\"date\"] &lt;= pd.Timestamp(\"2024-01-01\") + pd.Timedelta(days=experiment_length)]     aggregated_data = filtered_data.groupby(\"customer_id\").agg({         \"order_value\": \"sum\",         \"one\": \"sum\",         \"customer_age\": \"mean\",         \"date\": \"min\"     }).reset_index()      mde = mde_variance_reduction.mde_time_line(         aggregated_data,         powers=[0.8],         experiment_length=[experiment_length],         n_simulations=5     )     mdes.append(mde[0]) print(f\"Time taken for MDE with covariate calculation: {time.time() - start:.2f} seconds\") <pre>Time taken for MDE with covariate calculation: 0.20 seconds\n</pre> In\u00a0[9]: Copied! <pre>plot_mdes(mdes, y_value=mde_value, x_lim=30)\n</pre> plot_mdes(mdes, y_value=mde_value, x_lim=30) In\u00a0[10]: Copied! <pre>get_length_print(mdes, mde_value)\n</pre> get_length_print(mdes, mde_value) <pre>Minimum experiment length to detect MDE of 1.4: 21\n</pre> <p>Now we repeat the same using clustered standard errors with OLS. We see that the results are the same as with delta method.</p> In\u00a0[11]: Copied! <pre>mde_variance_reduction_ols = NormalPowerAnalysis.from_dict({\n    \"splitter\": \"clustered\",\n    \"analysis\": \"clustered_ols\",\n    \"time_col\": \"date\",\n    \"target_col\": \"order_value\",\n    \"cluster_cols\": [\"customer_id\"],\n    \"covariates\": [\"customer_age\"]\n})\n\nstart = time.time()\nmdes_ols_covariates = mde_variance_reduction_ols.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=[7, 14, 21, 28],\n    n_simulations=5\n)\nprint(f\"Time taken for OLS with covariate MDE calculation: {time.time() - start:.2f} seconds\")\n</pre> mde_variance_reduction_ols = NormalPowerAnalysis.from_dict({     \"splitter\": \"clustered\",     \"analysis\": \"clustered_ols\",     \"time_col\": \"date\",     \"target_col\": \"order_value\",     \"cluster_cols\": [\"customer_id\"],     \"covariates\": [\"customer_age\"] })  start = time.time() mdes_ols_covariates = mde_variance_reduction_ols.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=[7, 14, 21, 28],     n_simulations=5 ) print(f\"Time taken for OLS with covariate MDE calculation: {time.time() - start:.2f} seconds\")   <pre>Time taken for OLS with covariate MDE calculation: 0.66 seconds\n</pre> In\u00a0[12]: Copied! <pre>plot_mdes(mdes_ols_covariates, y_value=mde_value, x_lim=30)\n</pre> plot_mdes(mdes_ols_covariates, y_value=mde_value, x_lim=30) <p>This is the last example of power analysis.</p> <p>We assume data from Feb onwards is used to run the power analysis (it's the data we take as experimental, though it is actually pre-experimental data). Data from Jan is taken as pre-experimental data. We simulate that the experiment happened after Feb. We do this because we need pre-experimental data to train the Cupac Model, which we didn't do before.</p> <p>We use cupac model with customer id, historical orders and age features (order value has a non-linear relationship with customer id, that's why we don't add it a single covariate). We use the same data as before, but we train the model with pre-experimental data. We see that mde is smaller in this case, because we are using a better covariate that is not impacted by the treatment.</p> <p>In this case we cannot init by dict because we're using cupac, but happy to review a PR that includes this :) In this case we need to create splitter and analysis classes.</p> In\u00a0[13]: Copied! <pre>pre_experiment_df = experiment_data.query(\"date &lt; '2024-02-01'\")\nexperiment_df = experiment_data.query(\"date &gt;= '2024-02-01'\")\n</pre> pre_experiment_df = experiment_data.query(\"date &lt; '2024-02-01'\") experiment_df = experiment_data.query(\"date &gt;= '2024-02-01'\") In\u00a0[14]: Copied! <pre>from cluster_experiments import ClusteredSplitter, DeltaMethodAnalysis, ClusteredOLSAnalysis\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\nsplitter = ClusteredSplitter(\n    cluster_cols=[\"customer_id\"],\n)\ndelta = DeltaMethodAnalysis(\n    target_col=\"order_value\",\n    scale_col=\"one\",\n    covariates=[\"estimate_order_value\"],\n    cluster_cols=[\"customer_id\"],\n)\nols = ClusteredOLSAnalysis(\n    target_col=\"order_value\",\n    covariates=[\"estimate_order_value\"],\n    cluster_cols=[\"customer_id\"],\n)\n\npwr = NormalPowerAnalysis(\n    splitter=splitter,\n    analysis=delta,\n    cupac_model=HistGradientBoostingRegressor(),\n    time_col=\"date\",\n    target_col=\"order_value\",\n    scale_col=\"one\",\n    features_cupac_model=[\"customer_id\", \"customer_age\", \"historical_orders\"],\n)\n\npwr_ols = NormalPowerAnalysis(\n    splitter=splitter,\n    analysis=ols,\n    time_col=\"date\",\n    target_col=\"order_value\",\n    cupac_model=HistGradientBoostingRegressor(),\n    features_cupac_model=[\"customer_id\", \"customer_age\", \"historical_orders\"],\n)\n</pre> from cluster_experiments import ClusteredSplitter, DeltaMethodAnalysis, ClusteredOLSAnalysis from sklearn.ensemble import HistGradientBoostingRegressor   splitter = ClusteredSplitter(     cluster_cols=[\"customer_id\"], ) delta = DeltaMethodAnalysis(     target_col=\"order_value\",     scale_col=\"one\",     covariates=[\"estimate_order_value\"],     cluster_cols=[\"customer_id\"], ) ols = ClusteredOLSAnalysis(     target_col=\"order_value\",     covariates=[\"estimate_order_value\"],     cluster_cols=[\"customer_id\"], )  pwr = NormalPowerAnalysis(     splitter=splitter,     analysis=delta,     cupac_model=HistGradientBoostingRegressor(),     time_col=\"date\",     target_col=\"order_value\",     scale_col=\"one\",     features_cupac_model=[\"customer_id\", \"customer_age\", \"historical_orders\"], )  pwr_ols = NormalPowerAnalysis(     splitter=splitter,     analysis=ols,     time_col=\"date\",     target_col=\"order_value\",     cupac_model=HistGradientBoostingRegressor(),     features_cupac_model=[\"customer_id\", \"customer_age\", \"historical_orders\"], )   In\u00a0[15]: Copied! <pre>mdes_cupac = []\nmde_cupac_ols = []\n\nfor experiment_length in range(7, 35, 7):\n    filtered_data = experiment_df[experiment_df[\"date\"] &lt;= pd.Timestamp(\"2024-02-01\") + pd.Timedelta(days=experiment_length)]\n    aggregated_data = filtered_data.groupby(\"customer_id\").agg({\n        \"order_value\": \"sum\",\n        \"one\": \"sum\",\n        \"customer_age\": \"mean\",\n        \"historical_orders\": \"mean\",\n        \"date\": \"min\"\n    }).reset_index()\n    pre_experiment_aggregated_data = pre_experiment_df.groupby(\"customer_id\").agg({\n        \"order_value\": \"sum\",\n        \"one\": \"sum\",\n        \"customer_age\": \"mean\",\n        \"historical_orders\": \"mean\",\n        \"date\": \"min\"\n    }).reset_index()\n\n    timer_cupac_start = time.time()\n    mde = pwr.mde_time_line(\n        aggregated_data,\n        pre_experiment_aggregated_data,\n        powers=[0.8],\n        experiment_length=[experiment_length],\n        n_simulations=5\n    )\n    timer_cupac_end = time.time()\n    print(f\"Cupac MDE calculation for length {experiment_length} took {timer_cupac_end - timer_cupac_start:.2f} seconds\")\n\n    timer_ols_start = time.time()\n    mde_ols = pwr_ols.mde_time_line(\n        filtered_data,\n        pre_experiment_df,\n        powers=[0.8],\n        experiment_length=[experiment_length],\n        n_simulations=5\n    )\n    timer_ols_end = time.time()\n    print(f\"Cupac OLS MDE calculation for length {experiment_length} took {timer_ols_end - timer_ols_start:.2f} seconds\")\n    mdes_cupac.append(mde[0])\n    mde_cupac_ols.append(mde_ols[0])\n\n\nplot_mdes(mdes_cupac, y_value=mde_value, x_lim=30)\n</pre> mdes_cupac = [] mde_cupac_ols = []  for experiment_length in range(7, 35, 7):     filtered_data = experiment_df[experiment_df[\"date\"] &lt;= pd.Timestamp(\"2024-02-01\") + pd.Timedelta(days=experiment_length)]     aggregated_data = filtered_data.groupby(\"customer_id\").agg({         \"order_value\": \"sum\",         \"one\": \"sum\",         \"customer_age\": \"mean\",         \"historical_orders\": \"mean\",         \"date\": \"min\"     }).reset_index()     pre_experiment_aggregated_data = pre_experiment_df.groupby(\"customer_id\").agg({         \"order_value\": \"sum\",         \"one\": \"sum\",         \"customer_age\": \"mean\",         \"historical_orders\": \"mean\",         \"date\": \"min\"     }).reset_index()      timer_cupac_start = time.time()     mde = pwr.mde_time_line(         aggregated_data,         pre_experiment_aggregated_data,         powers=[0.8],         experiment_length=[experiment_length],         n_simulations=5     )     timer_cupac_end = time.time()     print(f\"Cupac MDE calculation for length {experiment_length} took {timer_cupac_end - timer_cupac_start:.2f} seconds\")      timer_ols_start = time.time()     mde_ols = pwr_ols.mde_time_line(         filtered_data,         pre_experiment_df,         powers=[0.8],         experiment_length=[experiment_length],         n_simulations=5     )     timer_ols_end = time.time()     print(f\"Cupac OLS MDE calculation for length {experiment_length} took {timer_ols_end - timer_ols_start:.2f} seconds\")     mdes_cupac.append(mde[0])     mde_cupac_ols.append(mde_ols[0])   plot_mdes(mdes_cupac, y_value=mde_value, x_lim=30) <pre>Cupac MDE calculation for length 7 took 0.62 seconds\nCupac OLS MDE calculation for length 7 took 0.73 seconds\nCupac MDE calculation for length 14 took 0.06 seconds\nCupac OLS MDE calculation for length 14 took 0.15 seconds\nCupac MDE calculation for length 21 took 0.06 seconds\nCupac OLS MDE calculation for length 21 took 0.21 seconds\nCupac MDE calculation for length 28 took 0.06 seconds\nCupac OLS MDE calculation for length 28 took 0.26 seconds\n</pre> In\u00a0[16]: Copied! <pre>plot_mdes(mde_cupac_ols, y_value=mde_value, x_lim=30)\n</pre> plot_mdes(mde_cupac_ols, y_value=mde_value, x_lim=30) In\u00a0[17]: Copied! <pre>get_length_print(mdes_cupac, mde_value)\n</pre> get_length_print(mdes_cupac, mde_value) <pre>Minimum experiment length to detect MDE of 1.4: 14\n</pre> In\u00a0[18]: Copied! <pre>real_experiment_data = experiment_data.query(\"date &gt;= '2024-03-01' and date &lt; '2024-03-21'\")\nreal_pre_experiment_data = experiment_data.query(\"date &lt; '2024-03-01'\")\nreal_experiment_data\n\nfrom cluster_experiments import ClusteredSplitter, ConstantPerturbator\n\n\n# Add effect on the order value\nsplitter = ClusteredSplitter(\n    cluster_cols=[\"customer_id\"],\n)\nperturbator = ConstantPerturbator(\n    target_col=\"order_value\",\n)\n\nreal_experiment_data = splitter.assign_treatment_df(real_experiment_data)\nreal_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=1.5)\n\n# Add effect on the delivery time\nperturbator = ConstantPerturbator(\n    target_col=\"delivery_time\",\n)\nreal_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=.2)\nreal_experiment_data\n</pre> real_experiment_data = experiment_data.query(\"date &gt;= '2024-03-01' and date &lt; '2024-03-21'\") real_pre_experiment_data = experiment_data.query(\"date &lt; '2024-03-01'\") real_experiment_data  from cluster_experiments import ClusteredSplitter, ConstantPerturbator   # Add effect on the order value splitter = ClusteredSplitter(     cluster_cols=[\"customer_id\"], ) perturbator = ConstantPerturbator(     target_col=\"order_value\", )  real_experiment_data = splitter.assign_treatment_df(real_experiment_data) real_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=1.5)  # Add effect on the delivery time perturbator = ConstantPerturbator(     target_col=\"delivery_time\", ) real_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=.2) real_experiment_data Out[18]: customer_id customer_age historical_orders order_value delivery_time city date one treatment 0 4162 49 3 179.306483 10.075481 LA 2024-03-08 1 B 1 1076 58 5 196.404074 8.837389 LA 2024-03-08 1 B 2 5142 31 6 139.203140 7.468698 LA 2024-03-14 1 A 3 4825 54 2 184.653481 7.267328 LA 2024-03-16 1 A 4 2569 36 4 154.003198 7.125005 NYC 2024-03-14 1 A ... ... ... ... ... ... ... ... ... ... 22071 3869 38 3 171.082504 7.156785 LA 2024-03-13 1 B 22072 2598 41 4 147.810428 9.690305 NYC 2024-03-08 1 A 22073 2672 46 5 180.692472 7.126516 LA 2024-03-13 1 B 22074 3529 29 6 103.223538 9.117135 NYC 2024-03-15 1 B 22075 1834 38 3 142.912978 8.840475 NYC 2024-03-02 1 A <p>22076 rows \u00d7 9 columns</p> <p>We run the analysis with and without covariate adjustment. We use city as a dimension, showing results in the overall population and by city.</p> <p>We compare with OLS with clustered standard errors. We see that results are exactly the same.</p> In\u00a0[19]: Copied! <pre>from cluster_experiments import AnalysisPlan, HypothesisTest, Variant, RatioMetric, SimpleMetric\n\nplan = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"numerator_name\": \"order_value\", \"denominator_name\": \"one\"},\n        {\"alias\": \"delivery_time\", \"numerator_name\": \"delivery_time\", \"denominator_name\": \"one\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"dimensions\": [\n        {\"name\": \"city\", \"values\": [\"NYC\", \"LA\"]},\n    ],\n    \"analysis_type\": \"delta\",\n    \"analysis_config\": {\"cluster_cols\": [\"customer_id\"]},\n})\n</pre> from cluster_experiments import AnalysisPlan, HypothesisTest, Variant, RatioMetric, SimpleMetric  plan = AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"numerator_name\": \"order_value\", \"denominator_name\": \"one\"},         {\"alias\": \"delivery_time\", \"numerator_name\": \"delivery_time\", \"denominator_name\": \"one\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"dimensions\": [         {\"name\": \"city\", \"values\": [\"NYC\", \"LA\"]},     ],     \"analysis_type\": \"delta\",     \"analysis_config\": {\"cluster_cols\": [\"customer_id\"]}, })  <p>We see that CIs contain the true effect, and CIs are pretty big.</p> In\u00a0[20]: Copied! <pre># Run the analysis plan\ndf_plan = plan.analyze(real_experiment_data).to_dataframe()\ndf_plan\n</pre> # Run the analysis plan df_plan = plan.analyze(real_experiment_data).to_dataframe() df_plan Out[20]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 149.944691 150.758469 delta 0.813778 -1.102842 2.730398 0.405307 0.977885 __total_dimension total 0.05 1 AOV A B 150.312273 151.164539 delta 0.852266 -1.279811 2.984343 0.433353 1.087814 city NYC 0.05 2 AOV A B 149.584125 150.363999 delta 0.779873 -1.365121 2.924867 0.476093 1.094405 city LA 0.05 3 delivery_time A B 7.996075 8.209871 delta 0.213796 0.171630 0.255962 0.000000 0.021514 __total_dimension total 0.05 4 delivery_time A B 7.991050 8.202004 delta 0.210954 0.163212 0.258696 0.000000 0.024359 city NYC 0.05 5 delivery_time A B 8.001005 8.217514 delta 0.216509 0.168266 0.264753 0.000000 0.024615 city LA 0.05 In\u00a0[21]: Copied! <pre>AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"name\": \"order_value\"},\n        {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"analysis_type\": \"clustered_ols\",\n    \"analysis_config\": {\"cluster_cols\": [\"customer_id\"]},\n}).analyze(real_experiment_data).to_dataframe()\n</pre> AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"name\": \"order_value\"},         {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"analysis_type\": \"clustered_ols\",     \"analysis_config\": {\"cluster_cols\": [\"customer_id\"]}, }).analyze(real_experiment_data).to_dataframe()  Out[21]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 149.944691 150.758469 clustered_ols 0.813778 -1.103050 2.730606 4.053575e-01 0.977992 __total_dimension total 0.05 1 delivery_time A B 7.996075 8.209871 clustered_ols 0.213796 0.171626 0.255967 2.885569e-23 0.021516 __total_dimension total 0.05 <p>Now we use covariate adjustment with customer age. We see that the effect in order value is closer to the true effect, but the effect in delivery time is just as bad. This is because in our data generation process we used customer age as a covariate for order value but not for delivery time.</p> In\u00a0[22]: Copied! <pre>plan_covariates = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"numerator_name\": \"order_value\", \"denominator_name\": \"one\"},\n        {\"alias\": \"delivery_time\", \"numerator_name\": \"delivery_time\", \"denominator_name\": \"one\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"analysis_type\": \"delta\",\n    \"analysis_config\": {\"cluster_cols\": [\"customer_id\"], \"covariates\": [\"customer_age\"]},\n})\n</pre> plan_covariates = AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"numerator_name\": \"order_value\", \"denominator_name\": \"one\"},         {\"alias\": \"delivery_time\", \"numerator_name\": \"delivery_time\", \"denominator_name\": \"one\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"analysis_type\": \"delta\",     \"analysis_config\": {\"cluster_cols\": [\"customer_id\"], \"covariates\": [\"customer_age\"]}, })  In\u00a0[23]: Copied! <pre># Run the analysis plan -&gt; Need to aggregate by customer_id for cuped to work with variance reduction\nreal_experiment_data_aggregated = real_experiment_data.groupby(\"customer_id\").agg({\n    \"order_value\": \"sum\",\n    \"one\": \"sum\",\n    \"delivery_time\": \"sum\",\n    \"customer_age\": \"mean\",\n    \"historical_orders\": \"mean\",\n    \"date\": \"min\",\n    \"treatment\": \"first\",\n    \"city\": \"first\"\n}).reset_index()\n\npre_real_experiment_data_aggregated = real_pre_experiment_data.groupby(\"customer_id\").agg({\n    \"order_value\": \"sum\",\n    \"one\": \"sum\",\n    \"delivery_time\": \"sum\",\n    \"customer_age\": \"mean\",\n    \"historical_orders\": \"mean\",\n    \"date\": \"min\",\n    \"city\": \"first\"\n}).reset_index()\n\n\nplan_covariates.analyze(real_experiment_data_aggregated).to_dataframe()\n</pre> # Run the analysis plan -&gt; Need to aggregate by customer_id for cuped to work with variance reduction real_experiment_data_aggregated = real_experiment_data.groupby(\"customer_id\").agg({     \"order_value\": \"sum\",     \"one\": \"sum\",     \"delivery_time\": \"sum\",     \"customer_age\": \"mean\",     \"historical_orders\": \"mean\",     \"date\": \"min\",     \"treatment\": \"first\",     \"city\": \"first\" }).reset_index()  pre_real_experiment_data_aggregated = real_pre_experiment_data.groupby(\"customer_id\").agg({     \"order_value\": \"sum\",     \"one\": \"sum\",     \"delivery_time\": \"sum\",     \"customer_age\": \"mean\",     \"historical_orders\": \"mean\",     \"date\": \"min\",     \"city\": \"first\" }).reset_index()   plan_covariates.analyze(real_experiment_data_aggregated).to_dataframe() Out[23]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 149.944691 150.758469 delta 1.247213 0.297971 2.196455 0.010018 0.484316 __total_dimension total 0.05 1 delivery_time A B 7.996075 8.209871 delta 0.213711 0.171539 0.255884 0.000000 0.021517 __total_dimension total 0.05 <p>Comparing with clustered OLS, pretty much the same results.</p> In\u00a0[24]: Copied! <pre>AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"name\": \"order_value\"},\n        {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"analysis_type\": \"clustered_ols\",\n    \"analysis_config\": {\"cluster_cols\": [\"customer_id\"], \"covariates\": [\"customer_age\"]},\n}).analyze(real_experiment_data).to_dataframe()\n</pre> AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"name\": \"order_value\"},         {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"analysis_type\": \"clustered_ols\",     \"analysis_config\": {\"cluster_cols\": [\"customer_id\"], \"covariates\": [\"customer_age\"]}, }).analyze(real_experiment_data).to_dataframe()  Out[24]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 149.944691 150.758469 clustered_ols 1.247367 0.298029 2.196706 1.001642e-02 0.484365 __total_dimension total 0.05 1 delivery_time A B 7.996075 8.209871 clustered_ols 0.213768 0.171596 0.255940 2.933129e-23 0.021517 __total_dimension total 0.05 <p>Now we run analysis using cupac model. We see that the CIs are smaller, and results in OLS and delta are kind of similar.</p> In\u00a0[25]: Copied! <pre>plan_cupac = AnalysisPlan(\n    tests=[\n        HypothesisTest(\n            metric=RatioMetric(alias=\"AOV\", numerator_name=\"order_value\", denominator_name=\"one\"),\n            analysis_type=\"delta\",\n            analysis_config={\n                \"cluster_cols\": [\"customer_id\"],\n                \"covariates\": [\"estimate_order_value\"],\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],\n                \"target_col\": \"order_value\",\n                \"scale_col\": \"one\",\n            },\n        ),\n        HypothesisTest(\n            metric=RatioMetric(alias=\"delivery_time\", numerator_name=\"delivery_time\", denominator_name=\"one\"),\n            analysis_type=\"delta\",\n            analysis_config={\n                \"cluster_cols\": [\"customer_id\"],\n                \"covariates\": [\"estimate_delivery_time\", \"customer_age\"], # adding to show how you can add more covariates\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],\n                \"target_col\": \"delivery_time\",\n                \"scale_col\": \"one\",\n            },\n        ),\n    ],\n    variants=[\n        Variant(name=\"A\", is_control=True),\n        Variant(name=\"B\", is_control=False),\n    ],\n    variant_col=\"treatment\",\n)\n</pre> plan_cupac = AnalysisPlan(     tests=[         HypothesisTest(             metric=RatioMetric(alias=\"AOV\", numerator_name=\"order_value\", denominator_name=\"one\"),             analysis_type=\"delta\",             analysis_config={                 \"cluster_cols\": [\"customer_id\"],                 \"covariates\": [\"estimate_order_value\"],             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],                 \"target_col\": \"order_value\",                 \"scale_col\": \"one\",             },         ),         HypothesisTest(             metric=RatioMetric(alias=\"delivery_time\", numerator_name=\"delivery_time\", denominator_name=\"one\"),             analysis_type=\"delta\",             analysis_config={                 \"cluster_cols\": [\"customer_id\"],                 \"covariates\": [\"estimate_delivery_time\", \"customer_age\"], # adding to show how you can add more covariates             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],                 \"target_col\": \"delivery_time\",                 \"scale_col\": \"one\",             },         ),     ],     variants=[         Variant(name=\"A\", is_control=True),         Variant(name=\"B\", is_control=False),     ],     variant_col=\"treatment\", ) In\u00a0[26]: Copied! <pre>plan_cupac.analyze(\n    real_experiment_data_aggregated,\n    pre_real_experiment_data_aggregated\n).to_dataframe()\n</pre> plan_cupac.analyze(     real_experiment_data_aggregated,     pre_real_experiment_data_aggregated ).to_dataframe() Out[26]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 149.944691 150.758469 delta 1.242198 0.361807 2.122589 0.005685 0.449187 __total_dimension total 0.05 1 delivery_time A B 7.996075 8.209871 delta 0.221000 0.192007 0.249993 0.000000 0.014793 __total_dimension total 0.05 <p>Comparison with clustered OLS</p> In\u00a0[27]: Copied! <pre>plan_cupac_ols = AnalysisPlan(\n    tests=[\n        HypothesisTest(\n            metric=SimpleMetric(alias=\"AOV\", name=\"order_value\"),\n            analysis_type=\"clustered_ols\",\n            analysis_config={\n                \"cluster_cols\": [\"customer_id\"],\n                \"covariates\": [\"estimate_order_value\"],\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],\n                \"target_col\": \"order_value\",\n            },\n        ),\n        HypothesisTest(\n            metric=SimpleMetric(alias=\"delivery_time\", name=\"delivery_time\"),\n            analysis_type=\"clustered_ols\",\n            analysis_config={\n                \"cluster_cols\": [\"customer_id\"],\n                \"covariates\": [\"customer_age\", \"estimate_delivery_time\"], # adding to show how you can add more covariates\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],\n                \"target_col\": \"delivery_time\",\n            },\n        ),\n    ],\n    variants=[\n        Variant(name=\"A\", is_control=True),\n        Variant(name=\"B\", is_control=False),\n    ],\n    variant_col=\"treatment\",\n)\n</pre> plan_cupac_ols = AnalysisPlan(     tests=[         HypothesisTest(             metric=SimpleMetric(alias=\"AOV\", name=\"order_value\"),             analysis_type=\"clustered_ols\",             analysis_config={                 \"cluster_cols\": [\"customer_id\"],                 \"covariates\": [\"estimate_order_value\"],             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],                 \"target_col\": \"order_value\",             },         ),         HypothesisTest(             metric=SimpleMetric(alias=\"delivery_time\", name=\"delivery_time\"),             analysis_type=\"clustered_ols\",             analysis_config={                 \"cluster_cols\": [\"customer_id\"],                 \"covariates\": [\"customer_age\", \"estimate_delivery_time\"], # adding to show how you can add more covariates             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\", \"historical_orders\"],                 \"target_col\": \"delivery_time\",             },         ),     ],     variants=[         Variant(name=\"A\", is_control=True),         Variant(name=\"B\", is_control=False),     ],     variant_col=\"treatment\", ) In\u00a0[28]: Copied! <pre>plan_cupac_ols.analyze(\n    real_experiment_data,\n    real_pre_experiment_data\n).to_dataframe()\n</pre> plan_cupac_ols.analyze(     real_experiment_data,     real_pre_experiment_data ).to_dataframe() Out[28]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 149.944691 150.758469 clustered_ols 1.174836 0.315865 2.033807 7.347039e-03 0.438259 __total_dimension total 0.05 1 delivery_time A B 7.996075 8.209871 clustered_ols 0.200532 0.171478 0.229585 1.066636e-41 0.014823 __total_dimension total 0.05 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"e2e_mde_delta.html#end-to-end-delta-method","title":"End-to-end: Delta Method\u00b6","text":"<p>We'll show the different functionalities of delta method cluster_experiments, which are:</p> <ul> <li>MDE calculation in cluster-randomized experiments with delta method and clustered standard errors.</li> <li>MDE calculation with simple covariate adjustment using delta method.</li> <li>MDE calculation with cupac (adjustment via ML models) using delta method.</li> <li>Inference for all the cases above using delta method, and comparing with OLS with clustered standard errors.</li> </ul>"},{"location":"e2e_mde_delta.html#data-generation","title":"Data generation\u00b6","text":"<p>We create some pre-experimental data that we could use to run power analysis.</p> <p>We have a dataframe with orders and customers, each customer may have many orders, and the two target metrics are delivery time and order value.</p>"},{"location":"e2e_mde_delta.html#power-analysis","title":"Power analysis\u00b6","text":""},{"location":"e2e_mde_delta.html#customer-level-split","title":"Customer-level split\u00b6","text":"<p>Assume we randomize at customer level, in this case we need to use delta to run the power analysis. The following code shows the mde on order_value given some experiment length (1, 2, 3 and 4 weeks).</p>"},{"location":"e2e_mde_delta.html#analysis","title":"Analysis\u00b6","text":"<p>Now we run analysis assuming that the experiment run after 2024-03-01 for 3 weeks. We simulate some fake effects (1.5 in order value and 0.2 in delivery time). We use functionalities in cluster_experiments to simulate the experiment.</p>"},{"location":"e2e_mde_switchback.html","title":"End-to-end: Switchback Design","text":"In\u00a0[1]: Copied! <pre>import random\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\nfrom datetime import timedelta\nfrom cluster_experiments import NormalPowerAnalysis\n\nnp.random.seed(42)\nrandom.seed(42)\n\n# Constants\nN = 10000  # Number of orders\nNUM_CUSTOMERS = 1000  # Unique customers\n\ndef generate_customers(num_customers):\n    \"\"\"Generate unique customers with a mean order value based on age.\"\"\"\n    customer_ids = np.arange(1, num_customers + 1)\n    customer_ages = np.random.randint(20, 60, size=num_customers)\n    mean_order_values = 50 + 0.8 * customer_ages + np.random.normal(0, 10, size=num_customers)\n\n    return pd.DataFrame({\n        \"customer_id\": customer_ids,\n        \"customer_age\": customer_ages,\n        \"mean_order_value\": mean_order_values\n    })\n\ndef sample_orders(customers, num_orders):\n    \"\"\"Sample customers and generate order-level data.\"\"\"\n    sampled_customers = np.random.choice(customers[\"customer_id\"], size=num_orders)\n    return pd.DataFrame({\"customer_id\": sampled_customers}).merge(customers, on=\"customer_id\", how=\"left\")\n\ndef generate_orders(customers, num_orders):\n    \"\"\"Full order generation pipeline using .assign() for cleaner transformations.\"\"\"\n    date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\", freq='H')\n    \n    # Generate 100 cities\n    cities = [f\"City_{i:02d}\" for i in range(1, 101)]\n    \n    def calculate_delivery_time(df):\n        # Base delivery time per customer\n        base_time = 8 + np.sin(df[\"customer_id\"] / 10)\n        \n        # Time of day effect (slower during rush hours: 8-9am, 5-7pm)\n        hour = df[\"datetime\"].dt.hour\n        time_effect = np.where(\n            ((hour &gt;= 8) &amp; (hour &lt;= 9)) | ((hour &gt;= 17) &amp; (hour &lt;= 19)),\n            1.5,  # 1.5 hours slower during rush hours\n            0\n        )\n        \n        # Day of week effect (slower on weekends)\n        day_of_week = df[\"datetime\"].dt.dayofweek\n        weekend_effect = np.where(day_of_week &gt;= 5, 0.8, 0)  # 0.8 hours slower on weekends\n        \n        # City effect (some cities have systematically longer delivery times)\n        # Extract city number and create varying delivery times\n        city_numbers = df[\"city\"].str.extract(r'City_(\\d+)')[0].astype(int)\n        city_effect = 0.5 * np.sin(city_numbers / 20) + 0.3 * np.cos(city_numbers / 15)\n        \n        # Random noise\n        noise = np.random.normal(0, 0.5, size=len(df))\n        \n        return base_time + time_effect + 4 * weekend_effect + city_effect + noise\n\n    return (\n        sample_orders(customers, num_orders)\n        .assign(\n            order_value=lambda df: df[\"mean_order_value\"] + np.random.normal(0, 5, size=len(df)),\n            datetime=lambda df: pd.to_datetime(np.random.choice(date_range, size=len(df))),\n            city=lambda df: np.random.choice(cities, size=len(df)),\n        )\n        .assign(\n            delivery_time=calculate_delivery_time,\n            date=lambda df: df[\"datetime\"].dt.date  # Extract date for compatibility\n        )\n        .drop(columns=[\"mean_order_value\"])  # Remove intermediate column\n    )\n\ndef plot_mdes(mdes, x_lim=60, y_value=3):\n    sns.lineplot(\n        data=pd.DataFrame(mdes),\n        x=\"experiment_length\",\n        y=\"mde\",\n    )\n\n    sns.lineplot(\n        x=[0, x_lim],\n        y=[y_value, y_value],\n        color=\"red\",\n        linestyle=\"--\",\n    )\n\n\ndef get_length_print(mdes, mde_value):\n    if any(x[\"mde\"] &lt; mde_value for x in mdes):\n        length = min(x[\"experiment_length\"] for x in mdes if x[\"mde\"] &lt; mde_value)\n        print(f\"Minimum experiment length to detect MDE of {mde_value}: {length}\")\n        return\n    print(f\"No MDE below {mde_value} found in the provided data.\")\n\n# Run the pipeline\ncustomers = generate_customers(NUM_CUSTOMERS)\nexperiment_data = generate_orders(customers, N).assign(\n    is_weekend=lambda df: df[\"datetime\"].dt.dayofweek &gt;= 5,\n    hour_of_day=lambda df: df[\"datetime\"].dt.hour,\n)\n\nexperiment_data\n</pre> import random import datetime  import numpy as np import pandas as pd import seaborn as sns   from datetime import timedelta from cluster_experiments import NormalPowerAnalysis  np.random.seed(42) random.seed(42)  # Constants N = 10000  # Number of orders NUM_CUSTOMERS = 1000  # Unique customers  def generate_customers(num_customers):     \"\"\"Generate unique customers with a mean order value based on age.\"\"\"     customer_ids = np.arange(1, num_customers + 1)     customer_ages = np.random.randint(20, 60, size=num_customers)     mean_order_values = 50 + 0.8 * customer_ages + np.random.normal(0, 10, size=num_customers)      return pd.DataFrame({         \"customer_id\": customer_ids,         \"customer_age\": customer_ages,         \"mean_order_value\": mean_order_values     })  def sample_orders(customers, num_orders):     \"\"\"Sample customers and generate order-level data.\"\"\"     sampled_customers = np.random.choice(customers[\"customer_id\"], size=num_orders)     return pd.DataFrame({\"customer_id\": sampled_customers}).merge(customers, on=\"customer_id\", how=\"left\")  def generate_orders(customers, num_orders):     \"\"\"Full order generation pipeline using .assign() for cleaner transformations.\"\"\"     date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\", freq='H')          # Generate 100 cities     cities = [f\"City_{i:02d}\" for i in range(1, 101)]          def calculate_delivery_time(df):         # Base delivery time per customer         base_time = 8 + np.sin(df[\"customer_id\"] / 10)                  # Time of day effect (slower during rush hours: 8-9am, 5-7pm)         hour = df[\"datetime\"].dt.hour         time_effect = np.where(             ((hour &gt;= 8) &amp; (hour &lt;= 9)) | ((hour &gt;= 17) &amp; (hour &lt;= 19)),             1.5,  # 1.5 hours slower during rush hours             0         )                  # Day of week effect (slower on weekends)         day_of_week = df[\"datetime\"].dt.dayofweek         weekend_effect = np.where(day_of_week &gt;= 5, 0.8, 0)  # 0.8 hours slower on weekends                  # City effect (some cities have systematically longer delivery times)         # Extract city number and create varying delivery times         city_numbers = df[\"city\"].str.extract(r'City_(\\d+)')[0].astype(int)         city_effect = 0.5 * np.sin(city_numbers / 20) + 0.3 * np.cos(city_numbers / 15)                  # Random noise         noise = np.random.normal(0, 0.5, size=len(df))                  return base_time + time_effect + 4 * weekend_effect + city_effect + noise      return (         sample_orders(customers, num_orders)         .assign(             order_value=lambda df: df[\"mean_order_value\"] + np.random.normal(0, 5, size=len(df)),             datetime=lambda df: pd.to_datetime(np.random.choice(date_range, size=len(df))),             city=lambda df: np.random.choice(cities, size=len(df)),         )         .assign(             delivery_time=calculate_delivery_time,             date=lambda df: df[\"datetime\"].dt.date  # Extract date for compatibility         )         .drop(columns=[\"mean_order_value\"])  # Remove intermediate column     )  def plot_mdes(mdes, x_lim=60, y_value=3):     sns.lineplot(         data=pd.DataFrame(mdes),         x=\"experiment_length\",         y=\"mde\",     )      sns.lineplot(         x=[0, x_lim],         y=[y_value, y_value],         color=\"red\",         linestyle=\"--\",     )   def get_length_print(mdes, mde_value):     if any(x[\"mde\"] &lt; mde_value for x in mdes):         length = min(x[\"experiment_length\"] for x in mdes if x[\"mde\"] &lt; mde_value)         print(f\"Minimum experiment length to detect MDE of {mde_value}: {length}\")         return     print(f\"No MDE below {mde_value} found in the provided data.\")  # Run the pipeline customers = generate_customers(NUM_CUSTOMERS) experiment_data = generate_orders(customers, N).assign(     is_weekend=lambda df: df[\"datetime\"].dt.dayofweek &gt;= 5,     hour_of_day=lambda df: df[\"datetime\"].dt.hour, )  experiment_data  <pre>/var/folders/n0/x7n4w_fs4vz094fp0sjd9mk00000gp/T/ipykernel_25340/3103132035.py:38: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-03-31\", freq='H')\n</pre> Out[1]: customer_id customer_age order_value datetime city delivery_time date is_weekend hour_of_day 0 611 39 72.199186 2024-02-21 08:00:00 City_39 9.268687 2024-02-21 False 8 1 704 23 81.011831 2024-01-26 16:00:00 City_98 8.421502 2024-01-26 False 16 2 243 52 89.979294 2024-03-25 11:00:00 City_34 6.935599 2024-03-25 False 11 3 831 23 65.235192 2024-03-27 09:00:00 City_91 10.286479 2024-03-27 False 9 4 561 58 91.154383 2024-03-29 14:00:00 City_44 8.100706 2024-03-29 False 14 ... ... ... ... ... ... ... ... ... ... 9995 280 46 83.915813 2024-01-31 10:00:00 City_23 8.824491 2024-01-31 False 10 9996 538 27 51.848917 2024-01-05 14:00:00 City_44 6.947604 2024-01-05 False 14 9997 685 30 76.810120 2024-02-22 02:00:00 City_38 8.982564 2024-02-22 False 2 9998 156 58 77.046916 2024-03-16 10:00:00 City_49 11.140749 2024-03-16 True 10 9999 238 38 78.391366 2024-01-06 14:00:00 City_43 9.824124 2024-01-06 True 14 <p>10000 rows \u00d7 9 columns</p> <p></p> In\u00a0[2]: Copied! <pre># we want to detect an effect of 0.1 on delivery_time, so we set the mde_value to 0.1\nMDE_VALUE = 0.1\nEXPERIMENT_LENGTHS = (7, 14, 21, 28, 35, 42, 49, 56)\n\nmde_time_split = NormalPowerAnalysis.from_dict({\n    \"analysis\": \"clustered_ols\",\n    \"cluster_cols\": [\"datetime\"],\n    \"splitter\": \"switchback\",\n    \"switch_frequency\": \"24h\",\n    \"target_col\": \"delivery_time\",\n    \"time_col\": \"datetime\",\n})\n\nmdes = mde_time_split.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=EXPERIMENT_LENGTHS,\n    n_simulations=10\n)\n\nplot_mdes(mdes, y_value=MDE_VALUE)\n</pre> # we want to detect an effect of 0.1 on delivery_time, so we set the mde_value to 0.1 MDE_VALUE = 0.1 EXPERIMENT_LENGTHS = (7, 14, 21, 28, 35, 42, 49, 56)  mde_time_split = NormalPowerAnalysis.from_dict({     \"analysis\": \"clustered_ols\",     \"cluster_cols\": [\"datetime\"],     \"splitter\": \"switchback\",     \"switch_frequency\": \"24h\",     \"target_col\": \"delivery_time\",     \"time_col\": \"datetime\", })  mdes = mde_time_split.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=EXPERIMENT_LENGTHS,     n_simulations=10 )  plot_mdes(mdes, y_value=MDE_VALUE) In\u00a0[3]: Copied! <pre>get_length_print(mdes, MDE_VALUE)\n</pre> get_length_print(mdes, MDE_VALUE) <pre>No MDE below 0.1 found in the provided data.\n</pre> <p></p> In\u00a0[4]: Copied! <pre>mde_time_split = NormalPowerAnalysis.from_dict({\n    \"analysis\": \"clustered_ols\",\n    \"cluster_cols\": [\"datetime\"],\n    \"splitter\": \"switchback\",\n    \"switch_frequency\": \"4h\",\n    \"target_col\": \"delivery_time\",\n    \"time_col\": \"datetime\",\n    \"washover\": \"constant_washover\",\n    \"washover_time_delta\": timedelta(minutes=30),\n})\n\nmdes = mde_time_split.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=EXPERIMENT_LENGTHS,\n    n_simulations=10\n)\n\nplot_mdes(mdes, y_value=MDE_VALUE)\n</pre> mde_time_split = NormalPowerAnalysis.from_dict({     \"analysis\": \"clustered_ols\",     \"cluster_cols\": [\"datetime\"],     \"splitter\": \"switchback\",     \"switch_frequency\": \"4h\",     \"target_col\": \"delivery_time\",     \"time_col\": \"datetime\",     \"washover\": \"constant_washover\",     \"washover_time_delta\": timedelta(minutes=30), })  mdes = mde_time_split.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=EXPERIMENT_LENGTHS,     n_simulations=10 )  plot_mdes(mdes, y_value=MDE_VALUE) In\u00a0[5]: Copied! <pre>get_length_print(mdes, MDE_VALUE)\n</pre> get_length_print(mdes, MDE_VALUE) <pre>No MDE below 0.1 found in the provided data.\n</pre> <p></p> In\u00a0[6]: Copied! <pre>mde_time_split = NormalPowerAnalysis.from_dict({\n    \"analysis\": \"clustered_ols\",\n    \"cluster_cols\": [\"datetime\", \"city\"],\n    \"splitter\": \"switchback\",\n    \"switch_frequency\": \"4h\",\n    \"target_col\": \"delivery_time\",\n    \"time_col\": \"datetime\",\n    \"washover\": \"constant_washover\",\n    \"washover_time_delta\": timedelta(minutes=30),\n})\n\nmdes = mde_time_split.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=EXPERIMENT_LENGTHS,\n    n_simulations=10\n)\n\nplot_mdes(mdes, y_value=MDE_VALUE)\n</pre> mde_time_split = NormalPowerAnalysis.from_dict({     \"analysis\": \"clustered_ols\",     \"cluster_cols\": [\"datetime\", \"city\"],     \"splitter\": \"switchback\",     \"switch_frequency\": \"4h\",     \"target_col\": \"delivery_time\",     \"time_col\": \"datetime\",     \"washover\": \"constant_washover\",     \"washover_time_delta\": timedelta(minutes=30), })  mdes = mde_time_split.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=EXPERIMENT_LENGTHS,     n_simulations=10 )  plot_mdes(mdes, y_value=MDE_VALUE) In\u00a0[7]: Copied! <pre>get_length_print(mdes, MDE_VALUE)\n</pre> get_length_print(mdes, MDE_VALUE) <pre>No MDE below 0.1 found in the provided data.\n</pre> <p>is_weekend is a good predictor of delivery time that is not impacted by the treatment. We can use this covariate to adjust our analysis and decrease mde. We see that mde is smaller in this case, because we are using a covariate that is not impacted by the treatment.</p> In\u00a0[8]: Copied! <pre>mde_variance_reduction = NormalPowerAnalysis.from_dict({\n    \"analysis\": \"clustered_ols\",\n    \"cluster_cols\": [\"datetime\", \"city\"],\n    \"splitter\": \"switchback\",\n    \"switch_frequency\": \"4h\",\n    \"target_col\": \"delivery_time\",\n    \"time_col\": \"datetime\",\n    \"covariates\": [\"is_weekend\"],\n    \"washover\": \"constant_washover\",\n    \"washover_time_delta\": timedelta(minutes=30),\n})\n</pre> mde_variance_reduction = NormalPowerAnalysis.from_dict({     \"analysis\": \"clustered_ols\",     \"cluster_cols\": [\"datetime\", \"city\"],     \"splitter\": \"switchback\",     \"switch_frequency\": \"4h\",     \"target_col\": \"delivery_time\",     \"time_col\": \"datetime\",     \"covariates\": [\"is_weekend\"],     \"washover\": \"constant_washover\",     \"washover_time_delta\": timedelta(minutes=30), }) In\u00a0[9]: Copied! <pre>mdes = mde_variance_reduction.mde_time_line(\n    experiment_data,\n    powers=[0.8],\n    experiment_length=EXPERIMENT_LENGTHS,\n    n_simulations=10\n)\n</pre> mdes = mde_variance_reduction.mde_time_line(     experiment_data,     powers=[0.8],     experiment_length=EXPERIMENT_LENGTHS,     n_simulations=10 ) In\u00a0[10]: Copied! <pre>plot_mdes(mdes, y_value=MDE_VALUE)\n</pre> plot_mdes(mdes, y_value=MDE_VALUE) In\u00a0[11]: Copied! <pre>get_length_print(mdes, MDE_VALUE)\n</pre> get_length_print(mdes, MDE_VALUE) <pre>Minimum experiment length to detect MDE of 0.1: 42\n</pre> <p>This is the last example of power analysis.</p> <p>We assume data from Feb onwards is used to run the power analysis (it's the data we take as experimental, though it is actually pre-experimental data). Data from Jan is taken as pre-experimental data. We simulate that the experiment happened after Feb. We do this because we need pre-experimental data to train the Cupac Model, which we didn't do before.</p> <p>We use cupac model with customer id and datetime features (delivery-time has a non-linear relationship with customer id, that's why we don't add it a single covariate). We use the same data as before, but we train the model with pre-experimental data. We see that mde is smaller in this case, because we are using a better covariate that is not impacted by the treatment.</p> <p>In this case we cannot init by dict because we're using cupac, but happy to review a PR that includes this :) In this case we need to create splitter and analysis classes.</p> In\u00a0[12]: Copied! <pre>cutoff_date = datetime.date(2024, 2, 1)\n\npre_experiment_df = experiment_data[experiment_data[\"date\"] &lt; cutoff_date]\nexperiment_df = experiment_data[experiment_data[\"date\"] &gt;= cutoff_date]\n</pre> cutoff_date = datetime.date(2024, 2, 1)  pre_experiment_df = experiment_data[experiment_data[\"date\"] &lt; cutoff_date] experiment_df = experiment_data[experiment_data[\"date\"] &gt;= cutoff_date] In\u00a0[13]: Copied! <pre>from cluster_experiments import SwitchbackSplitter, ClusteredOLSAnalysis, ConstantWashover\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n\nsplitter = SwitchbackSplitter(\n    cluster_cols=[\"datetime\", \"city\"],\n    switch_frequency=\"4h\",\n    time_col=\"datetime\",\n    washover=ConstantWashover(\n        washover_time_delta=timedelta(minutes=30)\n    )\n)\nols = ClusteredOLSAnalysis(\n    target_col=\"delivery_time\",\n    covariates=[\"estimate_delivery_time\"],\n    cluster_cols=[\"datetime\", \"city\"],\n)\n\npwr = NormalPowerAnalysis(\n    splitter=splitter,\n    analysis=ols,\n    cupac_model=HistGradientBoostingRegressor(),\n    time_col=\"datetime\",\n    target_col=\"delivery_time\",\n    features_cupac_model=[\"customer_id\", \"is_weekend\", \"hour_of_day\"],\n)\n</pre> from cluster_experiments import SwitchbackSplitter, ClusteredOLSAnalysis, ConstantWashover from sklearn.ensemble import HistGradientBoostingRegressor   splitter = SwitchbackSplitter(     cluster_cols=[\"datetime\", \"city\"],     switch_frequency=\"4h\",     time_col=\"datetime\",     washover=ConstantWashover(         washover_time_delta=timedelta(minutes=30)     ) ) ols = ClusteredOLSAnalysis(     target_col=\"delivery_time\",     covariates=[\"estimate_delivery_time\"],     cluster_cols=[\"datetime\", \"city\"], )  pwr = NormalPowerAnalysis(     splitter=splitter,     analysis=ols,     cupac_model=HistGradientBoostingRegressor(),     time_col=\"datetime\",     target_col=\"delivery_time\",     features_cupac_model=[\"customer_id\", \"is_weekend\", \"hour_of_day\"], )  In\u00a0[14]: Copied! <pre>mde_cupac = pwr.mde_time_line(\n    experiment_df,\n    pre_experiment_df,\n    powers=[0.8],\n    experiment_length=EXPERIMENT_LENGTHS,\n    n_simulations=10,\n)\n\nplot_mdes(mde_cupac, y_value=MDE_VALUE)\n</pre> mde_cupac = pwr.mde_time_line(     experiment_df,     pre_experiment_df,     powers=[0.8],     experiment_length=EXPERIMENT_LENGTHS,     n_simulations=10, )  plot_mdes(mde_cupac, y_value=MDE_VALUE) In\u00a0[15]: Copied! <pre>get_length_print(mde_cupac, MDE_VALUE)\n</pre> get_length_print(mde_cupac, MDE_VALUE) <pre>Minimum experiment length to detect MDE of 0.1: 14\n</pre> In\u00a0[16]: Copied! <pre>cutoff_date = datetime.date(2024, 3, 1)\n\nreal_experiment_data = experiment_data[experiment_data[\"date\"] &gt;= cutoff_date]\nreal_pre_experiment_data = experiment_data[experiment_data[\"date\"] &lt; cutoff_date]\n\nfrom cluster_experiments import ConstantPerturbator\n\n\n# Add effect on the order value\nsplitter = SwitchbackSplitter(\n    cluster_cols=[\"datetime\", \"city\"],\n    switch_frequency=\"4h\",\n    time_col=\"datetime\",\n    washover=ConstantWashover(\n        washover_time_delta=timedelta(minutes=30)\n    )\n)\n\nperturbator = ConstantPerturbator(\n    target_col=\"delivery_time\",\n)\n\nreal_experiment_data = splitter.assign_treatment_df(real_experiment_data)\nreal_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=0.1)\n</pre> cutoff_date = datetime.date(2024, 3, 1)  real_experiment_data = experiment_data[experiment_data[\"date\"] &gt;= cutoff_date] real_pre_experiment_data = experiment_data[experiment_data[\"date\"] &lt; cutoff_date]  from cluster_experiments import ConstantPerturbator   # Add effect on the order value splitter = SwitchbackSplitter(     cluster_cols=[\"datetime\", \"city\"],     switch_frequency=\"4h\",     time_col=\"datetime\",     washover=ConstantWashover(         washover_time_delta=timedelta(minutes=30)     ) )  perturbator = ConstantPerturbator(     target_col=\"delivery_time\", )  real_experiment_data = splitter.assign_treatment_df(real_experiment_data) real_experiment_data = perturbator.perturbate(real_experiment_data, average_effect=0.1) <p>We run the analysis with and without covariate adjustment.</p> <p>We see that the effect is closer to the true effect when we use covariate adjustment.</p> In\u00a0[17]: Copied! <pre>from cluster_experiments import AnalysisPlan, HypothesisTest, Variant, SimpleMetric\n\nplan = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"name\": \"order_value\"},\n        {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"analysis_type\": \"clustered_ols\",\n    \"analysis_config\": {\"cluster_cols\": [\"city\", \"datetime\"]},\n})\n</pre> from cluster_experiments import AnalysisPlan, HypothesisTest, Variant, SimpleMetric  plan = AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"name\": \"order_value\"},         {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"analysis_type\": \"clustered_ols\",     \"analysis_config\": {\"cluster_cols\": [\"city\", \"datetime\"]}, })  <p>We see that CIs contain the true effect, and CIs are pretty big.</p> In\u00a0[18]: Copied! <pre># Run the analysis plan\nplan.analyze(real_experiment_data).to_dataframe()\n</pre> # Run the analysis plan plan.analyze(real_experiment_data).to_dataframe() Out[18]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 82.007654 82.172876 clustered_ols 0.165222 -0.880170 1.210615 0.756737 0.533373 __total_dimension total 0.05 1 delivery_time A B 9.299918 9.452242 clustered_ols 0.152324 0.013381 0.291268 0.031657 0.070891 __total_dimension total 0.05 <p>Now we use covariate adjustment with is weekend.</p> In\u00a0[19]: Copied! <pre>plan_covariates = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"alias\": \"AOV\", \"name\": \"order_value\"},\n        {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},\n    ],\n    \"variants\": [\n        {\"name\": \"A\", \"is_control\": True},\n        {\"name\": \"B\", \"is_control\": False},\n    ],\n    \"variant_col\": \"treatment\",\n    \"alpha\": 0.05,\n    \"analysis_type\": \"clustered_ols\",\n    \"analysis_config\": {\"cluster_cols\": [\"city\", \"datetime\"], \"covariates\": [\"is_weekend\"]},\n})\n</pre> plan_covariates = AnalysisPlan.from_metrics_dict({     \"metrics\": [         {\"alias\": \"AOV\", \"name\": \"order_value\"},         {\"alias\": \"delivery_time\", \"name\": \"delivery_time\"},     ],     \"variants\": [         {\"name\": \"A\", \"is_control\": True},         {\"name\": \"B\", \"is_control\": False},     ],     \"variant_col\": \"treatment\",     \"alpha\": 0.05,     \"analysis_type\": \"clustered_ols\",     \"analysis_config\": {\"cluster_cols\": [\"city\", \"datetime\"], \"covariates\": [\"is_weekend\"]}, }) In\u00a0[20]: Copied! <pre># Run the analysis plan\nplan_covariates.analyze(real_experiment_data).to_dataframe()\n</pre> # Run the analysis plan plan_covariates.analyze(real_experiment_data).to_dataframe() Out[20]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 82.007654 82.172876 clustered_ols 0.167288 -0.878599 1.213175 0.753907 0.533626 __total_dimension total 0.05 1 delivery_time A B 9.299918 9.452242 clustered_ols 0.095147 0.014737 0.175557 0.020385 0.041026 __total_dimension total 0.05 <p>Now we run analysis using cupac model. We see that the effect in both metrics is closer to the true effect, and the CIs are smaller.</p> In\u00a0[21]: Copied! <pre>plan_cupac = AnalysisPlan(\n    tests=[\n        HypothesisTest(\n            metric=SimpleMetric(alias=\"AOV\", name=\"order_value\"),\n            analysis_type=\"clustered_ols\",\n            analysis_config={\n                \"cluster_cols\": [\"city\", \"datetime\"],\n                \"covariates\": [\"estimate_order_value\"],\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"customer_age\"],\n                \"target_col\": \"order_value\",\n            },\n        ),\n        HypothesisTest(\n            metric=SimpleMetric(alias=\"delivery_time\", name=\"delivery_time\"),\n            analysis_type=\"clustered_ols\",\n            analysis_config={\n                \"cluster_cols\": [\"city\", \"datetime\"],\n                \"covariates\": [\"estimate_delivery_time\"],\n            },\n            cupac_config={\n                \"cupac_model\": HistGradientBoostingRegressor(),\n                \"features_cupac_model\": [\"customer_id\", \"is_weekend\", \"hour_of_day\"],\n                \"target_col\": \"delivery_time\",\n            },\n        ),\n    ],\n    variants=[\n        Variant(name=\"A\", is_control=True),\n        Variant(name=\"B\", is_control=False),\n    ],\n    variant_col=\"treatment\",\n)\n</pre> plan_cupac = AnalysisPlan(     tests=[         HypothesisTest(             metric=SimpleMetric(alias=\"AOV\", name=\"order_value\"),             analysis_type=\"clustered_ols\",             analysis_config={                 \"cluster_cols\": [\"city\", \"datetime\"],                 \"covariates\": [\"estimate_order_value\"],             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"customer_age\"],                 \"target_col\": \"order_value\",             },         ),         HypothesisTest(             metric=SimpleMetric(alias=\"delivery_time\", name=\"delivery_time\"),             analysis_type=\"clustered_ols\",             analysis_config={                 \"cluster_cols\": [\"city\", \"datetime\"],                 \"covariates\": [\"estimate_delivery_time\"],             },             cupac_config={                 \"cupac_model\": HistGradientBoostingRegressor(),                 \"features_cupac_model\": [\"customer_id\", \"is_weekend\", \"hour_of_day\"],                 \"target_col\": \"delivery_time\",             },         ),     ],     variants=[         Variant(name=\"A\", is_control=True),         Variant(name=\"B\", is_control=False),     ],     variant_col=\"treatment\", ) In\u00a0[22]: Copied! <pre>plan_cupac.analyze(\n    real_experiment_data,\n    real_pre_experiment_data,\n).to_dataframe()\n</pre> plan_cupac.analyze(     real_experiment_data,     real_pre_experiment_data, ).to_dataframe() Out[22]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV A B 82.007654 82.172876 clustered_ols 0.322643 -0.270459 0.915745 0.286331 0.302609 __total_dimension total 0.05 1 delivery_time A B 9.299918 9.452242 clustered_ols 0.107405 0.062609 0.152201 0.000003 0.022856 __total_dimension total 0.05 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"e2e_mde_switchback.html#end-to-end-switchback-design","title":"End-to-end: Switchback Design\u00b6","text":"<p>We'll show the different functionalities of cluster_experiments, which are:</p> <ul> <li>MDE calculation in different setups of switchback experiments</li> <li>MDE calculation with simple covariate adjustment</li> <li>MDE calculation with cupac (adjustment via ML models)</li> <li>Inference for all the cases above</li> </ul>"},{"location":"e2e_mde_switchback.html#data-generation","title":"Data generation\u00b6","text":"<p>We create some pre-experimental data that we could use to run power analysis.</p> <p>We have a dataframe with orders and customers, each customer may have many orders, and the two target metrics are delivery time and order value.</p>"},{"location":"e2e_mde_switchback.html#power-analysis","title":"Power analysis\u00b6","text":""},{"location":"e2e_mde_switchback.html#day-level-split","title":"Day-level split\u00b6","text":"<p>Assume we run an switchback test randomizing at day level</p>"},{"location":"e2e_mde_switchback.html#4h-level-split","title":"4h-level split\u00b6","text":"<p>Assume we randomize the treatment at 4h level, we can use the same data as above, but we need to change the switch frequency to 4h.</p>"},{"location":"e2e_mde_switchback.html#city-4h-level-split","title":"City + 4h-level split\u00b6","text":"<p>Assume we randomize the treatment at city + 4h level, we can use the same data as above, but we need to add city as a cluster column.</p>"},{"location":"e2e_mde_switchback.html#analysis","title":"Analysis\u00b6","text":"<p>Now we run analysis assuming that the experiment run after 2024-03-01 for 3 weeks. We simulate some fake effects (0.0 in order value and 0.1 in delivery time). We use functionalities in cluster_experiments to simulate the experiment.</p>"},{"location":"experiment_analysis.html","title":"Experiment Analysis","text":"<p>First of all we import the necessary libraries and generate some fake data about our experiment.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom cluster_experiments import (\n    AnalysisPlan,\n    SimpleMetric,\n    Dimension,\n    Variant,\n    HypothesisTest,\n    TargetAggregation,\n)\n</pre> import pandas as pd import numpy as np from cluster_experiments import (     AnalysisPlan,     SimpleMetric,     Dimension,     Variant,     HypothesisTest,     TargetAggregation, ) In\u00a0[3]: Copied! <pre>def generate_fake_data():\n\n    # Constants\n    NUM_ORDERS = 10_000\n    NUM_CUSTOMERS = 3_000\n    EXPERIMENT_GROUPS = ['control', 'treatment_1', 'treatment_2']\n    GROUP_SIZE = NUM_CUSTOMERS // len(EXPERIMENT_GROUPS)\n    \n    # Generate customer_ids\n    customer_ids = np.arange(1, NUM_CUSTOMERS + 1)\n    \n    # Shuffle and split customer_ids into experiment groups\n    np.random.shuffle(customer_ids)\n    experiment_group = np.repeat(EXPERIMENT_GROUPS, GROUP_SIZE)\n    experiment_group = np.concatenate((experiment_group, np.random.choice(EXPERIMENT_GROUPS, NUM_CUSTOMERS - len(experiment_group))))\n    \n    # Assign customers to groups\n    customer_group_mapping = dict(zip(customer_ids, experiment_group))\n    \n    # Generate orders\n    order_ids = np.arange(1, NUM_ORDERS + 1)\n    customers = np.random.choice(customer_ids, NUM_ORDERS)\n    order_values = np.abs(np.random.normal(loc=10, scale=2, size=NUM_ORDERS))  # Normally distributed around 10 and positive\n    order_delivery_times = np.abs(np.random.normal(loc=30, scale=5, size=NUM_ORDERS))  # Normally distributed around 30 minutes and positive\n    order_city_codes = np.random.randint(1, 3, NUM_ORDERS)  # Random city codes between 1 and 2\n    \n    # Create DataFrame\n    data = {\n        'order_id': order_ids,\n        'customer_id': customers,\n        'experiment_group': [customer_group_mapping[customer_id] for customer_id in customers],\n        'order_value': order_values,\n        'order_delivery_time_in_minutes': order_delivery_times,\n        'order_city_code': order_city_codes\n    }\n    \n    df = pd.DataFrame(data)\n    df.order_city_code = df.order_city_code.astype(str)\n    \n    pre_exp_df = df.assign(\n        order_value = lambda df: df['order_value'] + np.random.normal(loc=0, scale=1, size=NUM_ORDERS),\n        order_delivery_time_in_minutes = lambda df: df['order_delivery_time_in_minutes'] + np.random.normal(loc=0, scale=2, size=NUM_ORDERS)\n    ).sample(int(NUM_ORDERS/3))\n    \n    return df, pre_exp_df\n\ndf, pre_exp_df = generate_fake_data()\n\n# Show the first few rows of the DataFrame\ndisplay(df.head())\ndisplay(pre_exp_df.head())\n</pre> def generate_fake_data():      # Constants     NUM_ORDERS = 10_000     NUM_CUSTOMERS = 3_000     EXPERIMENT_GROUPS = ['control', 'treatment_1', 'treatment_2']     GROUP_SIZE = NUM_CUSTOMERS // len(EXPERIMENT_GROUPS)          # Generate customer_ids     customer_ids = np.arange(1, NUM_CUSTOMERS + 1)          # Shuffle and split customer_ids into experiment groups     np.random.shuffle(customer_ids)     experiment_group = np.repeat(EXPERIMENT_GROUPS, GROUP_SIZE)     experiment_group = np.concatenate((experiment_group, np.random.choice(EXPERIMENT_GROUPS, NUM_CUSTOMERS - len(experiment_group))))          # Assign customers to groups     customer_group_mapping = dict(zip(customer_ids, experiment_group))          # Generate orders     order_ids = np.arange(1, NUM_ORDERS + 1)     customers = np.random.choice(customer_ids, NUM_ORDERS)     order_values = np.abs(np.random.normal(loc=10, scale=2, size=NUM_ORDERS))  # Normally distributed around 10 and positive     order_delivery_times = np.abs(np.random.normal(loc=30, scale=5, size=NUM_ORDERS))  # Normally distributed around 30 minutes and positive     order_city_codes = np.random.randint(1, 3, NUM_ORDERS)  # Random city codes between 1 and 2          # Create DataFrame     data = {         'order_id': order_ids,         'customer_id': customers,         'experiment_group': [customer_group_mapping[customer_id] for customer_id in customers],         'order_value': order_values,         'order_delivery_time_in_minutes': order_delivery_times,         'order_city_code': order_city_codes     }          df = pd.DataFrame(data)     df.order_city_code = df.order_city_code.astype(str)          pre_exp_df = df.assign(         order_value = lambda df: df['order_value'] + np.random.normal(loc=0, scale=1, size=NUM_ORDERS),         order_delivery_time_in_minutes = lambda df: df['order_delivery_time_in_minutes'] + np.random.normal(loc=0, scale=2, size=NUM_ORDERS)     ).sample(int(NUM_ORDERS/3))          return df, pre_exp_df  df, pre_exp_df = generate_fake_data()  # Show the first few rows of the DataFrame display(df.head()) display(pre_exp_df.head()) order_id customer_id experiment_group order_value order_delivery_time_in_minutes order_city_code 0 1 2121 treatment_1 8.617823 28.018369 2 1 2 1358 control 14.091706 35.770020 2 2 3 1891 control 10.710713 31.483367 1 3 4 2173 control 12.860734 29.235751 1 4 5 995 treatment_1 11.782487 33.076537 1 order_id customer_id experiment_group order_value order_delivery_time_in_minutes order_city_code 6211 6212 2470 control 9.740159 28.986726 1 5971 5972 1586 treatment_2 10.682427 38.246326 1 6460 6461 1706 control 7.093542 19.930616 2 249 250 1828 treatment_2 5.679958 39.600876 2 5258 5259 1990 treatment_2 9.483023 24.158893 2 <p>Now that we have a sample experimental dataset and also a pre-experimental dataset that we can use to showcase how to include cupac-style variance reduction in the analysis flow, we can proceed to define the building blocks of the analysis plan: metrics, dimensions and variants first.</p> <p>Metrics:</p> <ul> <li>AOV (Average Order Value)</li> <li>AVG DT (Average Delivery Time)</li> </ul> <p>Dimensions:</p> <ul> <li>order_city_code</li> </ul> <p>Variants:</p> <ul> <li>control</li> <li>treatment_1</li> <li>treatment_2</li> </ul> In\u00a0[4]: Copied! <pre>dimension__city_code = Dimension(\n    name='order_city_code',\n    values=['1','2']\n)\n\nmetric__order_value = SimpleMetric(\n    alias='AOV',\n    name='order_value'\n)\n\nmetric__delivery_time = SimpleMetric(\n    alias='AVG DT',\n    name='order_delivery_time_in_minutes'\n)\n\nvariants = [\n    Variant('control', is_control=True),\n    Variant('treatment_1', is_control=False),\n    Variant('treatment_2', is_control=False)\n]\n</pre> dimension__city_code = Dimension(     name='order_city_code',     values=['1','2'] )  metric__order_value = SimpleMetric(     alias='AOV',     name='order_value' )  metric__delivery_time = SimpleMetric(     alias='AVG DT',     name='order_delivery_time_in_minutes' )  variants = [     Variant('control', is_control=True),     Variant('treatment_1', is_control=False),     Variant('treatment_2', is_control=False) ] <p>Now we can define the hypothesis tests that we want to run on the data. We will run two tests:</p> <ul> <li>A clustered OLS test for the order value:<ul> <li>no variance reduction</li> <li>slice results by the city code of the orders</li> </ul> </li> <li>A GEE test for the delivery time:<ul> <li>with variance reduction using cupac (target aggregation)</li> <li>no slicing</li> </ul> </li> </ul> <p>As you can see, each hypothesis test can be flexible enough to have its own logic.</p> In\u00a0[5]: Copied! <pre>test__order_value = HypothesisTest(\n    metric=metric__order_value,\n    analysis_type=\"clustered_ols\",\n    analysis_config={\"cluster_cols\":[\"customer_id\"]},\n    dimensions=[dimension__city_code]\n)\n\ncupac__model = TargetAggregation(agg_col=\"customer_id\", target_col=\"order_delivery_time_in_minutes\")\n\ntest__delivery_time = HypothesisTest(\n    metric=metric__delivery_time,\n    analysis_type=\"gee\",\n    analysis_config={\"cluster_cols\":[\"customer_id\"], \"covariates\":[\"estimate_order_delivery_time_in_minutes\"]},\n    cupac_config={\"cupac_model\":cupac__model,\n                  \"target_col\":\"order_delivery_time_in_minutes\"}\n)\n</pre> test__order_value = HypothesisTest(     metric=metric__order_value,     analysis_type=\"clustered_ols\",     analysis_config={\"cluster_cols\":[\"customer_id\"]},     dimensions=[dimension__city_code] )  cupac__model = TargetAggregation(agg_col=\"customer_id\", target_col=\"order_delivery_time_in_minutes\")  test__delivery_time = HypothesisTest(     metric=metric__delivery_time,     analysis_type=\"gee\",     analysis_config={\"cluster_cols\":[\"customer_id\"], \"covariates\":[\"estimate_order_delivery_time_in_minutes\"]},     cupac_config={\"cupac_model\":cupac__model,                   \"target_col\":\"order_delivery_time_in_minutes\"} ) <p>Finally, we can define the analysis plan where we pack and run all the tests on the data. The results will be displayed in a DataFrame.</p> <p>Note that all tests included in a single analysis plan must run on the same exact dataset (or datasets, in case the pre-experimental data is provided and used). Should there be the need to use different datasets, the user must create separate analysis plans for each dataset.</p> In\u00a0[6]: Copied! <pre>analysis_plan = AnalysisPlan(\n    tests=[test__order_value, test__delivery_time],\n    variants=variants,\n    variant_col='experiment_group',\n    alpha=0.01\n)\n\nresults = analysis_plan.analyze(exp_data=df, pre_exp_data=pre_exp_df)\n\nresults_df = results.to_dataframe()\n\ndisplay(results_df)\n</pre> analysis_plan = AnalysisPlan(     tests=[test__order_value, test__delivery_time],     variants=variants,     variant_col='experiment_group',     alpha=0.01 )  results = analysis_plan.analyze(exp_data=df, pre_exp_data=pre_exp_df)  results_df = results.to_dataframe()  display(results_df) metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV control treatment_1 9.987651 10.027835 clustered_ols 0.040185 -0.081815 0.162184 0.396195 0.047363 __total_dimension total 0.01 1 AOV control treatment_1 10.037250 9.999080 clustered_ols -0.038170 -0.211476 0.135135 0.570493 0.067281 order_city_code 1 0.01 2 AOV control treatment_1 9.936038 10.056783 clustered_ols 0.120746 -0.059459 0.300951 0.084361 0.069960 order_city_code 2 0.01 3 AOV control treatment_2 9.987651 10.048016 clustered_ols 0.060365 -0.061972 0.182701 0.203729 0.047494 __total_dimension total 0.01 4 AOV control treatment_2 10.037250 10.026263 clustered_ols -0.010987 -0.187316 0.165342 0.872492 0.068455 order_city_code 1 0.01 5 AOV control treatment_2 9.936038 10.070004 clustered_ols 0.133967 -0.039486 0.307419 0.046652 0.067339 order_city_code 2 0.01 6 AVG DT control treatment_1 29.964957 29.985370 gee 0.031963 -0.250092 0.314017 0.770368 0.109500 __total_dimension total 0.01 7 AVG DT control treatment_2 29.964957 30.005273 gee 0.129250 -0.149748 0.408248 0.232756 0.108314 __total_dimension total 0.01 In\u00a0[7]: Copied! <pre>simple_analysis_plan = AnalysisPlan.from_metrics(\n    metrics=[metric__delivery_time, metric__order_value],\n    variants=variants,\n    variant_col='experiment_group',\n    alpha=0.01,\n    dimensions=[dimension__city_code],\n    analysis_type=\"clustered_ols\",\n    analysis_config={\"cluster_cols\":[\"customer_id\"]},\n)\n\nsimple_results = simple_analysis_plan.analyze(exp_data=df, verbose=True)\n\nsimple_results_df = simple_results.to_dataframe()\n\ndisplay(simple_results_df)\n</pre> simple_analysis_plan = AnalysisPlan.from_metrics(     metrics=[metric__delivery_time, metric__order_value],     variants=variants,     variant_col='experiment_group',     alpha=0.01,     dimensions=[dimension__city_code],     analysis_type=\"clustered_ols\",     analysis_config={\"cluster_cols\":[\"customer_id\"]}, )  simple_results = simple_analysis_plan.analyze(exp_data=df, verbose=True)  simple_results_df = simple_results.to_dataframe()  display(simple_results_df) <pre>2024-11-06 20:03:19,308 - Metric: AVG DT, Treatment: treatment_1, Dimension: __total_dimension, Value: total\n2024-11-06 20:03:19,385 - Metric: AVG DT, Treatment: treatment_1, Dimension: order_city_code, Value: 1\n2024-11-06 20:03:19,413 - Metric: AVG DT, Treatment: treatment_1, Dimension: order_city_code, Value: 2\n2024-11-06 20:03:19,440 - Metric: AVG DT, Treatment: treatment_2, Dimension: __total_dimension, Value: total\n2024-11-06 20:03:19,482 - Metric: AVG DT, Treatment: treatment_2, Dimension: order_city_code, Value: 1\n2024-11-06 20:03:19,513 - Metric: AVG DT, Treatment: treatment_2, Dimension: order_city_code, Value: 2\n2024-11-06 20:03:19,556 - Metric: AOV, Treatment: treatment_1, Dimension: __total_dimension, Value: total\n2024-11-06 20:03:19,609 - Metric: AOV, Treatment: treatment_1, Dimension: order_city_code, Value: 1\n2024-11-06 20:03:19,639 - Metric: AOV, Treatment: treatment_1, Dimension: order_city_code, Value: 2\n2024-11-06 20:03:19,669 - Metric: AOV, Treatment: treatment_2, Dimension: __total_dimension, Value: total\n2024-11-06 20:03:19,705 - Metric: AOV, Treatment: treatment_2, Dimension: order_city_code, Value: 1\n2024-11-06 20:03:19,732 - Metric: AOV, Treatment: treatment_2, Dimension: order_city_code, Value: 2\n</pre> metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AVG DT control treatment_1 29.964957 29.985370 clustered_ols 0.020413 -0.300872 0.341698 0.870000 0.124731 __total_dimension total 0.01 1 AVG DT control treatment_1 29.862094 29.800854 clustered_ols -0.061240 -0.508945 0.386466 0.724585 0.173810 order_city_code 1 0.01 2 AVG DT control treatment_1 30.071997 30.171119 clustered_ols 0.099122 -0.347323 0.545568 0.567389 0.173321 order_city_code 2 0.01 3 AVG DT control treatment_2 29.964957 30.005273 clustered_ols 0.040316 -0.279913 0.360546 0.745716 0.124321 __total_dimension total 0.01 4 AVG DT control treatment_2 29.862094 30.069727 clustered_ols 0.207634 -0.232033 0.647301 0.223817 0.170689 order_city_code 1 0.01 5 AVG DT control treatment_2 30.071997 29.940118 clustered_ols -0.131878 -0.581788 0.318032 0.450230 0.174666 order_city_code 2 0.01 6 AOV control treatment_1 9.987651 10.027835 clustered_ols 0.040185 -0.081815 0.162184 0.396195 0.047363 __total_dimension total 0.01 7 AOV control treatment_1 10.037250 9.999080 clustered_ols -0.038170 -0.211476 0.135135 0.570493 0.067281 order_city_code 1 0.01 8 AOV control treatment_1 9.936038 10.056783 clustered_ols 0.120746 -0.059459 0.300951 0.084361 0.069960 order_city_code 2 0.01 9 AOV control treatment_2 9.987651 10.048016 clustered_ols 0.060365 -0.061972 0.182701 0.203729 0.047494 __total_dimension total 0.01 10 AOV control treatment_2 10.037250 10.026263 clustered_ols -0.010987 -0.187316 0.165342 0.872492 0.068455 order_city_code 1 0.01 11 AOV control treatment_2 9.936038 10.070004 clustered_ols 0.133967 -0.039486 0.307419 0.046652 0.067339 order_city_code 2 0.01 In\u00a0[9]: Copied! <pre>from cluster_experiments.experiment_analysis import ClusteredOLSAnalysis\n\n# assuming we define a meaningful custom ExperimentAnalysis class\nclass CustomExperimentAnalysis(ClusteredOLSAnalysis):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\ncustom_simple_analysis_plan = AnalysisPlan.from_metrics(\n    metrics=[metric__order_value],\n    variants=variants,\n    variant_col='experiment_group',\n    alpha=0.01,\n    dimensions=[dimension__city_code],\n    analysis_type=\"custom_clustered_ols\",\n    analysis_config={\"cluster_cols\":[\"customer_id\"]},\n    custom_analysis_type_mapper={\"custom_clustered_ols\": CustomExperimentAnalysis}\n)\n\ncustom_simple_results = custom_simple_analysis_plan.analyze(exp_data=df)\n\ncustom_simple_results_df = custom_simple_results.to_dataframe()\n\ndisplay(custom_simple_results_df)\n</pre> from cluster_experiments.experiment_analysis import ClusteredOLSAnalysis  # assuming we define a meaningful custom ExperimentAnalysis class class CustomExperimentAnalysis(ClusteredOLSAnalysis):     def __init__(self, **kwargs):         super().__init__(**kwargs)  custom_simple_analysis_plan = AnalysisPlan.from_metrics(     metrics=[metric__order_value],     variants=variants,     variant_col='experiment_group',     alpha=0.01,     dimensions=[dimension__city_code],     analysis_type=\"custom_clustered_ols\",     analysis_config={\"cluster_cols\":[\"customer_id\"]},     custom_analysis_type_mapper={\"custom_clustered_ols\": CustomExperimentAnalysis} )  custom_simple_results = custom_simple_analysis_plan.analyze(exp_data=df)  custom_simple_results_df = custom_simple_results.to_dataframe()  display(custom_simple_results_df) metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 AOV control treatment_1 9.987651 10.027835 custom_clustered_ols 0.040185 -0.081815 0.162184 0.396195 0.047363 __total_dimension total 0.01 1 AOV control treatment_1 10.037250 9.999080 custom_clustered_ols -0.038170 -0.211476 0.135135 0.570493 0.067281 order_city_code 1 0.01 2 AOV control treatment_1 9.936038 10.056783 custom_clustered_ols 0.120746 -0.059459 0.300951 0.084361 0.069960 order_city_code 2 0.01 3 AOV control treatment_2 9.987651 10.048016 custom_clustered_ols 0.060365 -0.061972 0.182701 0.203729 0.047494 __total_dimension total 0.01 4 AOV control treatment_2 10.037250 10.026263 custom_clustered_ols -0.010987 -0.187316 0.165342 0.872492 0.068455 order_city_code 1 0.01 5 AOV control treatment_2 9.936038 10.070004 custom_clustered_ols 0.133967 -0.039486 0.307419 0.046652 0.067339 order_city_code 2 0.01 <p>Now it's your turn! Have fun experimenting with the library and analyzing your data!</p>"},{"location":"experiment_analysis.html#experiment-analysis","title":"Experiment Analysis\u00b6","text":"<p>This notebook demonstrates how to analyze the results of an experiment and get a useful scorecard as a summary of the analysis.</p> <p>The experiment is an A/B/C test where we randomise users of a food delivery app to test a new ranking algorithm, and we compare the monetary value and the delivery time of the orders created.</p>"},{"location":"experiment_analysis.html#shortcut-creating-a-simple-analysis-plan-skipping-hypothesis-tests-definition","title":"Shortcut: creating a simple analysis plan skipping hypothesis tests definition\u00b6","text":"<p>In case the user does not need to define custom hypothesis tests, they can use the <code>AnalysisPlan.from_metrics</code> method to create a simple analysis plan where the user only needs to define the metrics, dimensions and variants. The method will automatically create the necessary hypothesis tests and run them on the data.</p> <p>This works for the cases where all the desired tests should run with the same analysis type and configuration, and with the same dimensions to slice upon. In case the user needs to run tests with differences in such components, they should use the standard way of defining the analysis plan as illustrated in the previous section.</p> <p>Below is an example of how to create a simple analysis plan using the same metrics, dimensions and variants as before. The results will be displayed in a DataFrame. Additionally, we also show how setting <code>verbose=True</code> will log the setup of all the comparisons that are performed when running the analysis plan.</p>"},{"location":"experiment_analysis.html#bonus-plugging-in-a-custom-analysis-method","title":"Bonus: Plugging in a custom analysis method\u00b6","text":"<p>In case the user needs to run a custom analysis method that is not covered by the standard analysis types provided by the library, they can define a custom analysis class and plug it into the analysis plan. Below is an example of how to do this.</p> <p>In this example, we define a custom analysis class that extends the <code>ClusteredOLSAnalysis</code> class provided by the library. The custom class will be used to run a clustered OLS analysis with a custom logic.</p> <p>The analysis plan will be created with the custom analysis type mapper that will map the custom analysis type to the custom analysis class.</p>"},{"location":"license.html","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2022 David Masip</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"multivariate.html","title":"Multiple Treatments","text":"<p>This notebook shows how to use the multivariate module. The idea is to use several treatments in the splitter and only one of them is used to run the hypothesis test.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom cluster_experiments import PowerAnalysis\n</pre> import numpy as np import pandas as pd from cluster_experiments import PowerAnalysis  In\u00a0[2]: Copied! <pre># Create fake data\nN = 1_000\ndf = pd.DataFrame(\n    {\n        \"target\": np.random.normal(0, 1, size=N),\n    }\n)\n</pre> # Create fake data N = 1_000 df = pd.DataFrame(     {         \"target\": np.random.normal(0, 1, size=N),     } ) In\u00a0[3]: Copied! <pre># Run power analysis using 3 variants\nconfig_abc = {\n    \"analysis\": \"ols_non_clustered\",\n    \"perturbator\": \"constant\",\n    \"splitter\": \"non_clustered\",\n    \"treatments\": [\"A\", \"B\", \"C\"],\n    \"control\": \"A\",\n    \"treatment\": \"B\",\n    \"n_simulations\": 50,\n}\n\npower_abc = PowerAnalysis.from_dict(config_abc)\npower_abc.power_analysis(df, average_effect=0.1)\n</pre> # Run power analysis using 3 variants config_abc = {     \"analysis\": \"ols_non_clustered\",     \"perturbator\": \"constant\",     \"splitter\": \"non_clustered\",     \"treatments\": [\"A\", \"B\", \"C\"],     \"control\": \"A\",     \"treatment\": \"B\",     \"n_simulations\": 50, }  power_abc = PowerAnalysis.from_dict(config_abc) power_abc.power_analysis(df, average_effect=0.1) Out[3]: <pre>0.18</pre> In\u00a0[4]: Copied! <pre># Run power analysis using 2 variants\nconfig_ab = {\n    \"analysis\": \"ols_non_clustered\",\n    \"perturbator\": \"constant\",\n    \"splitter\": \"non_clustered\",\n    \"treatments\": [\"A\", \"B\"],\n    \"control\": \"A\",\n    \"treatment\": \"B\",\n    \"n_simulations\": 50,\n}\npower_ab = PowerAnalysis.from_dict(config_ab)\npower_ab.power_analysis(df, average_effect=0.1)\n</pre> # Run power analysis using 2 variants config_ab = {     \"analysis\": \"ols_non_clustered\",     \"perturbator\": \"constant\",     \"splitter\": \"non_clustered\",     \"treatments\": [\"A\", \"B\"],     \"control\": \"A\",     \"treatment\": \"B\",     \"n_simulations\": 50, } power_ab = PowerAnalysis.from_dict(config_ab) power_ab.power_analysis(df, average_effect=0.1) Out[4]: <pre>0.28</pre> <p>The power of the AB test is higher than the ABC test, which makes sense.</p>"},{"location":"normal_power.html","title":"Normal Power Comparison","text":"<p>This notebook shows how NormalPowerAnalysis and PowerAnalysis calculators give similar powers for a switchback experiment, and the normal one is way faster.</p> In\u00a0[1]: Copied! <pre>from datetime import date\n\nimport numpy as np\nfrom cluster_experiments import PowerAnalysis, ConstantPerturbator, BalancedClusteredSplitter, ExperimentAnalysis, ClusteredOLSAnalysis, NormalPowerAnalysis\nimport pandas as pd\nfrom time import time\n\n\n\n# Create fake data\nN = 10_000\nclusters = [f\"Cluster {i}\" for i in range(10)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 15)]\ndf = pd.DataFrame(\n    {\n        \"cluster\": np.random.choice(clusters, size=N),\n        \"date\": np.random.choice(dates, size=N),\n    }\n).assign(\n    # Target is a linear combination of cluster and day of week, plus some noise\n    cluster_id=lambda df: df[\"cluster\"].astype(\"category\").cat.codes,\n    day_of_week=lambda df: pd.to_datetime(df[\"date\"]).dt.dayofweek,\n    target=lambda df: df[\"cluster_id\"] + df[\"day_of_week\"] + np.random.normal(size=N),\n)\n</pre> from datetime import date  import numpy as np from cluster_experiments import PowerAnalysis, ConstantPerturbator, BalancedClusteredSplitter, ExperimentAnalysis, ClusteredOLSAnalysis, NormalPowerAnalysis import pandas as pd from time import time    # Create fake data N = 10_000 clusters = [f\"Cluster {i}\" for i in range(10)] dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 15)] df = pd.DataFrame(     {         \"cluster\": np.random.choice(clusters, size=N),         \"date\": np.random.choice(dates, size=N),     } ).assign(     # Target is a linear combination of cluster and day of week, plus some noise     cluster_id=lambda df: df[\"cluster\"].astype(\"category\").cat.codes,     day_of_week=lambda df: pd.to_datetime(df[\"date\"]).dt.dayofweek,     target=lambda df: df[\"cluster_id\"] + df[\"day_of_week\"] + np.random.normal(size=N), )  In\u00a0[2]: Copied! <pre>df.head()\n</pre> df.head() Out[2]: cluster date cluster_id day_of_week target 0 Cluster 3 2022-01-12 3 2 5.206403 1 Cluster 3 2022-01-14 3 4 7.004311 2 Cluster 9 2022-01-06 9 3 11.554722 3 Cluster 1 2022-01-10 1 0 2.697168 4 Cluster 8 2022-01-14 8 4 10.311295 <p>Some clusters have a higher average outcome than others</p> In\u00a0[3]: Copied! <pre>cluster_cols = [\"cluster\", \"date\"]\n\nsplitter = BalancedClusteredSplitter(\n    cluster_cols=cluster_cols,\n)\n\nperturbator = ConstantPerturbator()\n\nanalysis = ClusteredOLSAnalysis(\n    cluster_cols=cluster_cols,\n)\n\nalpha = 0.05\nn_simulations = 100\nn_simulations_normal = 10\n\n# Simulated power analysis, we use clustered splitter and ols clustered analysis\npw_simulated = PowerAnalysis(\n    splitter=splitter,\n    perturbator=perturbator,\n    alpha=alpha,\n    n_simulations=n_simulations,\n    analysis=analysis,\n)\n\n# Normal power analysis, uses Central limit theorem to estimate power, and needs less simulations\npw_normal = NormalPowerAnalysis(\n    splitter=splitter,\n    alpha=alpha,\n    n_simulations=n_simulations_normal,\n    analysis=analysis,\n)\n</pre> cluster_cols = [\"cluster\", \"date\"]  splitter = BalancedClusteredSplitter(     cluster_cols=cluster_cols, )  perturbator = ConstantPerturbator()  analysis = ClusteredOLSAnalysis(     cluster_cols=cluster_cols, )  alpha = 0.05 n_simulations = 100 n_simulations_normal = 10  # Simulated power analysis, we use clustered splitter and ols clustered analysis pw_simulated = PowerAnalysis(     splitter=splitter,     perturbator=perturbator,     alpha=alpha,     n_simulations=n_simulations,     analysis=analysis, )  # Normal power analysis, uses Central limit theorem to estimate power, and needs less simulations pw_normal = NormalPowerAnalysis(     splitter=splitter,     alpha=alpha,     n_simulations=n_simulations_normal,     analysis=analysis, )  In\u00a0[4]: Copied! <pre># power line for simulated\neffects = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75]\ntime_pw = time()\npw_simulated_line = pw_simulated.power_line(df, average_effects=effects)\nprint(f\"Simulated power line took {time() - time_pw} seconds\")\n</pre> # power line for simulated effects = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75] time_pw = time() pw_simulated_line = pw_simulated.power_line(df, average_effects=effects) print(f\"Simulated power line took {time() - time_pw} seconds\") <pre>Simulated power line took 35.480019092559814 seconds\n</pre> In\u00a0[5]: Copied! <pre># power line for normal\ntime_pw = time()\npw_normal_line = pw_normal.power_line(df, average_effects=effects)\nprint(f\"Time for normal power line: {time() - time_pw} seconds\")\n</pre> # power line for normal time_pw = time() pw_normal_line = pw_normal.power_line(df, average_effects=effects) print(f\"Time for normal power line: {time() - time_pw} seconds\") <pre>Time for normal power line: 0.5191936492919922 seconds\n</pre> In\u00a0[6]: Copied! <pre># power line for normal, single simulation\ntime_pw = time()\npw_normal.n_simulations = 1\npw_normal_line_single = pw_normal.power_line(df, average_effects=effects)\nprint(f\"Time for normal power line single simulation: {time() - time_pw} seconds\")\n</pre> # power line for normal, single simulation time_pw = time() pw_normal.n_simulations = 1 pw_normal_line_single = pw_normal.power_line(df, average_effects=effects) print(f\"Time for normal power line single simulation: {time() - time_pw} seconds\") <pre>Time for normal power line single simulation: 0.06762981414794922 seconds\n</pre> In\u00a0[7]: Copied! <pre>pd.DataFrame(\n    {\n        \"Average effect\": effects,\n        \"Simulated power\": pw_simulated_line.values(),\n        \"Normal power\": pw_normal_line.values(),\n        \"Normal power single simulation\": pw_normal_line_single.values(),\n    }\n).plot(\n    x=\"Average effect\",\n    y=[\"Simulated power\", \"Normal power\", \"Normal power single simulation\"],\n    title=\"Power analysis\",\n    xlabel=\"Average effect\",\n    ylabel=\"Power\",\n    marker=\"o\",\n)\n</pre> pd.DataFrame(     {         \"Average effect\": effects,         \"Simulated power\": pw_simulated_line.values(),         \"Normal power\": pw_normal_line.values(),         \"Normal power single simulation\": pw_normal_line_single.values(),     } ).plot(     x=\"Average effect\",     y=[\"Simulated power\", \"Normal power\", \"Normal power single simulation\"],     title=\"Power analysis\",     xlabel=\"Average effect\",     ylabel=\"Power\",     marker=\"o\", ) Out[7]: <pre>&lt;AxesSubplot:title={'center':'Power analysis'}, xlabel='Average effect', ylabel='Power'&gt;</pre> In\u00a0[8]: Copied! <pre># pw_normal can also be used to find MDE\nmde = pw_normal.mde(df, power=0.7)\nprint(f\"Minimum detectable effect: {mde:.2f}\")\n\n# and we can retrieve the power for a given effect size\npower = pw_normal.power_analysis(df, average_effect=mde)\nprint(f\"Power for MDE: {power:.2f}, should be 0.7\")\n</pre> # pw_normal can also be used to find MDE mde = pw_normal.mde(df, power=0.7) print(f\"Minimum detectable effect: {mde:.2f}\")  # and we can retrieve the power for a given effect size power = pw_normal.power_analysis(df, average_effect=mde) print(f\"Power for MDE: {power:.2f}, should be 0.7\") <pre>Minimum detectable effect: 1.44\nPower for MDE: 0.70, should be 0.7\n</pre>"},{"location":"normal_power_lines.html","title":"Power Analysis Guide","text":"<p>Uses power and mde lines functionalities to showcase how to plot MDE and power as function of the sample size, given, respectively, the power and the MDE.</p> In\u00a0[1]: Copied! <pre>from datetime import date\n\nimport numpy as np\nfrom cluster_experiments import NormalPowerAnalysis\nimport pandas as pd\nimport plotnine as p9\n\n# Create fake data\nN = 10_000\nclusters = [f\"Cluster {i}\" for i in range(10)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 15)]\ndf = pd.DataFrame(\n    {\n        \"cluster\": np.random.choice(clusters, size=N),\n        \"date\": np.random.choice(dates, size=N),\n    }\n).assign(\n    # Target is a linear combination of cluster and day of week, plus some noise\n    cluster_id=lambda df: df[\"cluster\"].astype(\"category\").cat.codes,\n    day_of_week=lambda df: pd.to_datetime(df[\"date\"]).dt.dayofweek,\n    target=lambda df: df[\"cluster_id\"] + df[\"day_of_week\"] + np.random.normal(size=N),\n    date=lambda df: pd.to_datetime(df[\"date\"]),\n)\n</pre> from datetime import date  import numpy as np from cluster_experiments import NormalPowerAnalysis import pandas as pd import plotnine as p9  # Create fake data N = 10_000 clusters = [f\"Cluster {i}\" for i in range(10)] dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 15)] df = pd.DataFrame(     {         \"cluster\": np.random.choice(clusters, size=N),         \"date\": np.random.choice(dates, size=N),     } ).assign(     # Target is a linear combination of cluster and day of week, plus some noise     cluster_id=lambda df: df[\"cluster\"].astype(\"category\").cat.codes,     day_of_week=lambda df: pd.to_datetime(df[\"date\"]).dt.dayofweek,     target=lambda df: df[\"cluster_id\"] + df[\"day_of_week\"] + np.random.normal(size=N),     date=lambda df: pd.to_datetime(df[\"date\"]), )  In\u00a0[2]: Copied! <pre># Set-up power analysis for switchback experiment\npw_normal = NormalPowerAnalysis.from_dict(\n    {\n        \"splitter\": \"clustered\",\n        \"analysis\": \"clustered_ols\",\n        \"cluster_cols\": [\"cluster\", \"date\"],\n        \"n_simulations\": 5,\n        \"hypothesis\": \"two-sided\",\n        \"seed\": 20240922,\n        \"time_col\": \"date\",\n        \"relative_effect\": True\n    }\n)\n</pre> # Set-up power analysis for switchback experiment pw_normal = NormalPowerAnalysis.from_dict(     {         \"splitter\": \"clustered\",         \"analysis\": \"clustered_ols\",         \"cluster_cols\": [\"cluster\", \"date\"],         \"n_simulations\": 5,         \"hypothesis\": \"two-sided\",         \"seed\": 20240922,         \"time_col\": \"date\",         \"relative_effect\": True     } ) In\u00a0[3]: Copied! <pre>%%time\n# compute power line for different lengths different average effects\npower_line = pw_normal.power_time_line(\n    df, experiment_length=range(1, 14), average_effects=np.arange(0, 0.5, 0.1)\n)\n</pre> %%time # compute power line for different lengths different average effects power_line = pw_normal.power_time_line(     df, experiment_length=range(1, 14), average_effects=np.arange(0, 0.5, 0.1) ) <pre>CPU times: user 870 ms, sys: 31.6 ms, total: 902 ms\nWall time: 902 ms\n</pre> In\u00a0[4]: Copied! <pre># plot line\np9.ggplot(\n    pd.DataFrame(power_line),\n    p9.aes(x=\"experiment_length\", y=\"power\", color=\"effect\", group=\"effect\"),\n) + p9.geom_line() + p9.geom_point() + p9.theme_minimal() + p9.labs(\n    x=\"Experiment Length\", y=\"Power\"\n) + p9.ggtitle(\"Power lines\")\n</pre> # plot line p9.ggplot(     pd.DataFrame(power_line),     p9.aes(x=\"experiment_length\", y=\"power\", color=\"effect\", group=\"effect\"), ) + p9.geom_line() + p9.geom_point() + p9.theme_minimal() + p9.labs(     x=\"Experiment Length\", y=\"Power\" ) + p9.ggtitle(\"Power lines\") In\u00a0[5]: Copied! <pre>%%time\n# compute mde line for different lengths and different powers\nmde_line = pw_normal.mde_time_line(\n    df, experiment_length=range(1, 14), powers=[0.7, 0.8, 0.9]\n)\n</pre> %%time # compute mde line for different lengths and different powers mde_line = pw_normal.mde_time_line(     df, experiment_length=range(1, 14), powers=[0.7, 0.8, 0.9] ) <pre>CPU times: user 889 ms, sys: 29.7 ms, total: 918 ms\nWall time: 928 ms\n</pre> In\u00a0[6]: Copied! <pre># plot line\np9.ggplot(\n    pd.DataFrame(mde_line),\n    p9.aes(x=\"experiment_length\", y=\"mde\", color=\"power\", group=\"power\"),\n) + p9.geom_line() + p9.geom_point() + p9.theme_minimal() + p9.labs(\n    x=\"Experiment Length\", y=\"Minimum Detectable Effect\"\n) + p9.ggtitle(\"MDE lines\")\n</pre> # plot line p9.ggplot(     pd.DataFrame(mde_line),     p9.aes(x=\"experiment_length\", y=\"mde\", color=\"power\", group=\"power\"), ) + p9.geom_line() + p9.geom_point() + p9.theme_minimal() + p9.labs(     x=\"Experiment Length\", y=\"Minimum Detectable Effect\" ) + p9.ggtitle(\"MDE lines\") <p>Now, suppose we want to analyze how the target metric evolves over time (for instance, how the average user profit changes on a daily basis). To do this, we can compute the Minimum Detectable Effect over a rolling time window.</p> In\u00a0[7]: Copied! <pre>%%time\n# compute mde line for different lengths and different powers on a rolling basis\nrolling_mde_line = pw_normal.mde_rolling_time_line(\n    df=df,\n    powers=[0.7, 0.8, 0.9],\n    experiment_length=range(1, 15),\n    agg_func='mean',\n)\n</pre> %%time # compute mde line for different lengths and different powers on a rolling basis rolling_mde_line = pw_normal.mde_rolling_time_line(     df=df,     powers=[0.7, 0.8, 0.9],     experiment_length=range(1, 15),     agg_func='mean', ) <pre>CPU times: user 434 ms, sys: 10.5 ms, total: 445 ms\nWall time: 444 ms\n</pre> In\u00a0[8]: Copied! <pre># plot line\np9.ggplot(\n    pd.DataFrame(rolling_mde_line),\n    p9.aes(x=\"experiment_length\", y=\"mde\", color=\"power\", group=\"power\"),\n) + p9.geom_line() + p9.geom_point() + p9.theme_minimal() + p9.labs(\n    x=\"Experiment Length\", y=\"Relative Minimum Detectable Effect\"\n) + p9.ggtitle(\"Rolling relative MDE lines\")\n</pre> # plot line p9.ggplot(     pd.DataFrame(rolling_mde_line),     p9.aes(x=\"experiment_length\", y=\"mde\", color=\"power\", group=\"power\"), ) + p9.geom_line() + p9.geom_point() + p9.theme_minimal() + p9.labs(     x=\"Experiment Length\", y=\"Relative Minimum Detectable Effect\" ) + p9.ggtitle(\"Rolling relative MDE lines\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"paired_ttest.html","title":"Paired T-Test","text":"<p>This notebook shows how the PairedTTestClusteredAnalysis class is performing the paired t test. It's important to get a grasp on the difference between cluster and strata columns.</p> In\u00a0[1]: Copied! <pre>from cluster_experiments.experiment_analysis import PairedTTestClusteredAnalysis\n\nimport pandas as pd\n</pre> from cluster_experiments.experiment_analysis import PairedTTestClusteredAnalysis  import pandas as pd  In\u00a0[2]: Copied! <pre># Let's generate some fake switchback data (the clusters here would be city and date\ndf = pd.DataFrame(\n        {\n            \"country_code\": [\"ES\"] * 4 + [\"IT\"] * 4 + [\"PL\"] * 4 + [\"RO\"] * 4,\n            \"date\": [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"2022-01-04\"] * 4,\n            \"treatment\": [\"A\", 'B'] * 8,\n            \"target\": [0.01] * 15 + [0.1],\n        }\n    )\n</pre> # Let's generate some fake switchback data (the clusters here would be city and date df = pd.DataFrame(         {             \"country_code\": [\"ES\"] * 4 + [\"IT\"] * 4 + [\"PL\"] * 4 + [\"RO\"] * 4,             \"date\": [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"2022-01-04\"] * 4,             \"treatment\": [\"A\", 'B'] * 8,             \"target\": [0.01] * 15 + [0.1],         }     ) <p>Let's see what the PairedTTestClusteredAnalysis class is doing under the hood. As I am passing already the treatment column, there's no need for splitter nor perturbator</p> In\u00a0[3]: Copied! <pre>analysis = PairedTTestClusteredAnalysis(\n    cluster_cols=[\"country_code\", \"date\"], strata_cols = ['country_code']\n)\n\nanalysis._preprocessing(df, verbose=True)\n</pre> analysis = PairedTTestClusteredAnalysis(     cluster_cols=[\"country_code\", \"date\"], strata_cols = ['country_code'] )  analysis._preprocessing(df, verbose=True) <pre>performing paired t test in this data \n treatment        A      B\ncountry_code             \nES            0.01  0.010\nIT            0.01  0.010\nPL            0.01  0.010\nRO            0.01  0.055 \n\n</pre> Out[3]: treatment A B country_code ES 0.01 0.010 IT 0.01 0.010 PL 0.01 0.010 RO 0.01 0.055 <p>Keep in mind that strata_cols needs to be a subset of cluster_cols and it will be used as the index for pivoting.</p> In\u00a0[4]: Copied! <pre>analysis.analysis_pvalue(df, verbose=True)\n</pre> analysis.analysis_pvalue(df, verbose=True)  <pre>paired t test results: \n TtestResult(statistic=-1.0, pvalue=0.39100221895577053, df=3) \n\n</pre> Out[4]: <pre>0.39100221895577053</pre> In\u00a0[4]: Copied! <pre>\n</pre>"},{"location":"plot_calendars.html","title":"Calendar Visualization","text":"<p>This notebook shows the visualizations of different calendars that can be generated with switchback splitters</p> In\u00a0[1]: Copied! <pre>from cluster_experiments import PowerAnalysis\nimport pandas as pd\nimport numpy as np\nfrom plotnine import ggplot, geom_tile, element_text, element_blank, theme, labs, facet_wrap, aes, theme_set, theme_minimal\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\ntheme_set(theme_minimal())\n</pre> from cluster_experiments import PowerAnalysis import pandas as pd import numpy as np from plotnine import ggplot, geom_tile, element_text, element_blank, theme, labs, facet_wrap, aes, theme_set, theme_minimal # Remove warnings import warnings warnings.filterwarnings('ignore')  np.random.seed(42)  theme_set(theme_minimal()) In\u00a0[2]: Copied! <pre>def plot_calendar(splitter, df, calendar_name):\n    treatments = splitter.assign_treatment_df(df)\n    \n    treatment_df = (\n        treatments\n        .loc[:, [\"city\", \"treatment\", \"time\"]]\n        .drop_duplicates()\n    )\n    # Convert time column to datetime format and extract week and day\n    treatment_df['time'] = pd.to_datetime(treatment_df['time'])\n    treatment_df['week'] = treatment_df['time'].dt.isocalendar().week.astype(str)\n    treatment_df['day'] = treatment_df['time'].dt.dayofweek.astype(str)\n    # day of week with format number + name\n    treatment_df['day_of_week'] = treatment_df['day'] + ' - ' + treatment_df['time'].dt.day_name()\n\n    return (ggplot(treatment_df, aes(x='day_of_week', y='week', fill='treatment'))\n            + geom_tile()\n            + facet_wrap('city', nrow=2)\n            + labs(title=f'Switchback calendar in different cities, using {calendar_name}', x='Day', y='Week')\n            + theme(axis_text_x=element_text(angle=90, vjust=1.0),\n                    axis_text_y=element_text(size=8),\n                    strip_background=element_blank(),\n                    strip_text=element_text(size=10)))\n\n\n        \n</pre> def plot_calendar(splitter, df, calendar_name):     treatments = splitter.assign_treatment_df(df)          treatment_df = (         treatments         .loc[:, [\"city\", \"treatment\", \"time\"]]         .drop_duplicates()     )     # Convert time column to datetime format and extract week and day     treatment_df['time'] = pd.to_datetime(treatment_df['time'])     treatment_df['week'] = treatment_df['time'].dt.isocalendar().week.astype(str)     treatment_df['day'] = treatment_df['time'].dt.dayofweek.astype(str)     # day of week with format number + name     treatment_df['day_of_week'] = treatment_df['day'] + ' - ' + treatment_df['time'].dt.day_name()      return (ggplot(treatment_df, aes(x='day_of_week', y='week', fill='treatment'))             + geom_tile()             + facet_wrap('city', nrow=2)             + labs(title=f'Switchback calendar in different cities, using {calendar_name}', x='Day', y='Week')             + theme(axis_text_x=element_text(angle=90, vjust=1.0),                     axis_text_y=element_text(size=8),                     strip_background=element_blank(),                     strip_text=element_text(size=10)))            In\u00a0[3]: Copied! <pre># Define data with random dates\ndf_raw = pd.DataFrame(\n    {   \n        \"time\": pd.date_range(\"2021-03-01\", \"2021-03-15\", freq=\"1min\")[\n            np.random.randint(14 * 24 * 60, size=14 * 24 * 60)\n        ],\n        \"y\": np.random.randn(14 * 24 * 60),\n    }\n).assign(\n    day_of_week=lambda df: df.time.dt.dayofweek,\n    hour_of_day=lambda df: df.time.dt.hour,\n    week=lambda df: df.time.dt.isocalendar().week,\n)\ndf = pd.concat([df_raw.assign(city=city) for city in (\"TGN\", \"NYC\", \"LON\", \"REU\")])\n</pre> # Define data with random dates df_raw = pd.DataFrame(     {            \"time\": pd.date_range(\"2021-03-01\", \"2021-03-15\", freq=\"1min\")[             np.random.randint(14 * 24 * 60, size=14 * 24 * 60)         ],         \"y\": np.random.randn(14 * 24 * 60),     } ).assign(     day_of_week=lambda df: df.time.dt.dayofweek,     hour_of_day=lambda df: df.time.dt.hour,     week=lambda df: df.time.dt.isocalendar().week, ) df = pd.concat([df_raw.assign(city=city) for city in (\"TGN\", \"NYC\", \"LON\", \"REU\")]) In\u00a0[4]: Copied! <pre># We have data of 2-weeks in 4 cities\ndf.head()\n</pre> # We have data of 2-weeks in 4 cities df.head() Out[4]: time y day_of_week hour_of_day week city 0 2021-03-11 23:15:00 0.827658 3 23 10 TGN 1 2021-03-01 14:20:00 0.843550 0 14 9 TGN 2 2021-03-04 17:50:00 0.306790 3 17 9 TGN 3 2021-03-09 07:24:00 -2.133119 1 7 10 TGN 4 2021-03-08 20:04:00 -1.026710 0 20 10 TGN In\u00a0[5]: Copied! <pre># Completely random calendar\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"1d\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"random calendar\")\n</pre> # Completely random calendar config = {     \"time_col\": \"time\",     \"switch_frequency\": \"1d\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback\",     \"cluster_cols\": [\"time\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"random calendar\") Out[5]: <pre>&lt;ggplot: (310936046)&gt;</pre> In\u00a0[6]: Copied! <pre># Balanced calendar\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"1d\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_balance\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"balanced Calendar\")\n</pre> # Balanced calendar config = {     \"time_col\": \"time\",     \"switch_frequency\": \"1d\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_balance\",     \"cluster_cols\": [\"time\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"balanced Calendar\") Out[6]: <pre>&lt;ggplot: (311115590)&gt;</pre> In\u00a0[7]: Copied! <pre># Stratified - city level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"1d\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (city level)\")\n</pre> # Stratified - city level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"1d\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (city level)\") Out[7]: <pre>&lt;ggplot: (311223354)&gt;</pre> In\u00a0[8]: Copied! <pre># Stratified - city + week level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"1d\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"week\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (week + city level)\")\n</pre> # Stratified - city + week level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"1d\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"week\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (week + city level)\") Out[8]: <pre>&lt;ggplot: (311217083)&gt;</pre> In\u00a0[9]: Copied! <pre># Stratified - city + day of week level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"1d\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"day_of_week\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (day of week + city level)\")\n</pre> # Stratified - city + day of week level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"1d\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"day_of_week\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (day of week + city level)\") Out[9]: <pre>&lt;ggplot: (311416839)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"plot_calendars_hours.html","title":"4-Hour Switches","text":"<p>This notebook shows the visualizations of different calendars that can be generated with switchback splitters</p> In\u00a0[1]: Copied! <pre>from cluster_experiments import PowerAnalysis\nimport pandas as pd\nimport numpy as np\nimport plotnine as pn\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\npn.theme_set(pn.theme_minimal())\n</pre> from cluster_experiments import PowerAnalysis import pandas as pd import numpy as np import plotnine as pn # Remove warnings import warnings warnings.filterwarnings('ignore')  np.random.seed(42)  pn.theme_set(pn.theme_minimal()) In\u00a0[2]: Copied! <pre>def plot_calendar(splitter, df, calendar_name):\n    treatments = splitter.assign_treatment_df(df)\n    \n    treatment_df = (\n        treatments\n        .loc[:, [\"city\", \"treatment\", \"time\"]]\n        .drop_duplicates()\n    )\n    # Convert time column to datetime format and extract week and day\n    treatment_df['week_number'] = treatment_df['time'].dt.isocalendar().week.astype(int).astype(str)\n    treatment_df['week_number_num'] = treatment_df['time'].dt.isocalendar().week\n    # add time with the week set to the minimum week number\n    treatment_df['time_in_week'] = treatment_df['time'] - pd.to_timedelta(treatment_df['week_number_num'], unit='W') + pd.to_timedelta(treatment_df['week_number_num'].min(), unit='W')\n\n    # Custom date formatter function\n    def custom_date_formatter(dates):\n        return [date.strftime('%a %H:%M') for date in dates]\n\n    # Create the plot\n    plot = (\n        pn.ggplot(treatment_df, pn.aes(x='time_in_week', y='week_number', fill='treatment'))\n        + pn.geom_tile()\n        + pn.theme_minimal()\n        + pn.labs(\n            title=f\"Treatment assignment using {calendar_name}\",\n            x=\"Time since start of week\",\n            y=\"Week number\",\n            fill=\"Treatment\"\n        )\n        + pn.facet_wrap('~city')\n        + pn.scale_x_datetime(labels=custom_date_formatter)\n        # vertical labels\n        + pn.theme(axis_text_x=pn.element_text(angle=90, vjust=1.0))\n    )\n\n    # Display the plot\n    return plot\n</pre> def plot_calendar(splitter, df, calendar_name):     treatments = splitter.assign_treatment_df(df)          treatment_df = (         treatments         .loc[:, [\"city\", \"treatment\", \"time\"]]         .drop_duplicates()     )     # Convert time column to datetime format and extract week and day     treatment_df['week_number'] = treatment_df['time'].dt.isocalendar().week.astype(int).astype(str)     treatment_df['week_number_num'] = treatment_df['time'].dt.isocalendar().week     # add time with the week set to the minimum week number     treatment_df['time_in_week'] = treatment_df['time'] - pd.to_timedelta(treatment_df['week_number_num'], unit='W') + pd.to_timedelta(treatment_df['week_number_num'].min(), unit='W')      # Custom date formatter function     def custom_date_formatter(dates):         return [date.strftime('%a %H:%M') for date in dates]      # Create the plot     plot = (         pn.ggplot(treatment_df, pn.aes(x='time_in_week', y='week_number', fill='treatment'))         + pn.geom_tile()         + pn.theme_minimal()         + pn.labs(             title=f\"Treatment assignment using {calendar_name}\",             x=\"Time since start of week\",             y=\"Week number\",             fill=\"Treatment\"         )         + pn.facet_wrap('~city')         + pn.scale_x_datetime(labels=custom_date_formatter)         # vertical labels         + pn.theme(axis_text_x=pn.element_text(angle=90, vjust=1.0))     )      # Display the plot     return plot In\u00a0[3]: Copied! <pre># Define data with random dates\ndf_raw = pd.DataFrame(\n    {   \n        \"time\": pd.date_range(\"2021-03-01\", \"2021-03-15\", freq=\"1s\")[\n            np.random.randint(14 * 24 * 60 * 60, size=14 * 24 * 60 * 60)\n        ],\n        \"y\": np.random.randn(14 * 24 * 60 * 60),\n    }\n).assign(\n    day_of_week=lambda df: df.time.dt.dayofweek,\n    hour_of_day=lambda df: df.time.dt.hour,\n    hour_4h=lambda df: df.time.dt.hour // 4,\n    week=lambda df: df.time.dt.isocalendar().week,\n)\ndf = pd.concat([df_raw.assign(city=city) for city in (\"TGN\", \"NYC\", \"LON\", \"REU\")])\n</pre> # Define data with random dates df_raw = pd.DataFrame(     {            \"time\": pd.date_range(\"2021-03-01\", \"2021-03-15\", freq=\"1s\")[             np.random.randint(14 * 24 * 60 * 60, size=14 * 24 * 60 * 60)         ],         \"y\": np.random.randn(14 * 24 * 60 * 60),     } ).assign(     day_of_week=lambda df: df.time.dt.dayofweek,     hour_of_day=lambda df: df.time.dt.hour,     hour_4h=lambda df: df.time.dt.hour // 4,     week=lambda df: df.time.dt.isocalendar().week, ) df = pd.concat([df_raw.assign(city=city) for city in (\"TGN\", \"NYC\", \"LON\", \"REU\")]) In\u00a0[4]: Copied! <pre># We have data of 2-weeks in 4 cities\ndf.head()\n</pre> # We have data of 2-weeks in 4 cities df.head() Out[4]: time y day_of_week hour_of_day hour_4h week city 0 2021-03-02 09:52:38 0.328784 1 9 2 9 TGN 1 2021-03-08 18:25:55 0.489899 0 18 4 10 TGN 2 2021-03-02 12:38:52 -0.559446 1 12 3 9 TGN 3 2021-03-03 23:59:38 1.644260 2 23 5 9 TGN 4 2021-03-02 06:37:48 -0.418535 1 6 1 9 TGN In\u00a0[5]: Copied! <pre># Completely random calendar\nfrom datetime import timedelta\n\n\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"4h\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"target_col\": \"y\",\n    \"washover\": \"constant_washover\",\n    \"washover_time_delta\": timedelta(hours=2),\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"random calendar\")\n</pre> # Completely random calendar from datetime import timedelta   config = {     \"time_col\": \"time\",     \"switch_frequency\": \"4h\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback\",     \"cluster_cols\": [\"time\", \"city\"],     \"target_col\": \"y\",     \"washover\": \"constant_washover\",     \"washover_time_delta\": timedelta(hours=2), }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"random calendar\") Out[5]: <pre>&lt;ggplot: (304288799)&gt;</pre> In\u00a0[6]: Copied! <pre># Balanced calendar\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"4h\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_balance\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"balanced Calendar\")\n</pre> # Balanced calendar config = {     \"time_col\": \"time\",     \"switch_frequency\": \"4h\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_balance\",     \"cluster_cols\": [\"time\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"balanced Calendar\") Out[6]: <pre>&lt;ggplot: (304498911)&gt;</pre> In\u00a0[7]: Copied! <pre># Stratified - city level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"4h\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (city level)\")\n</pre> # Stratified - city level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"4h\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (city level)\") Out[7]: <pre>&lt;ggplot: (304646891)&gt;</pre> In\u00a0[8]: Copied! <pre># Stratified - city + week level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"4h\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"week\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (week + city level)\")\n</pre> # Stratified - city + week level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"4h\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"week\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (week + city level)\") Out[8]: <pre>&lt;ggplot: (304292286)&gt;</pre> In\u00a0[9]: Copied! <pre># Stratified - city + day of week level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"4h\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"day_of_week\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (day of week + city level)\")\n</pre> # Stratified - city + day of week level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"4h\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"day_of_week\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (day of week + city level)\") Out[9]: <pre>&lt;ggplot: (304915270)&gt;</pre> In\u00a0[10]: Copied! <pre># Stratified - city + day of week + hour of day level\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"4h\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"day_of_week\", \"hour_4h\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n\nplot_calendar(power.splitter, df, \"stratified calendar (day of week + hour of day + city level)\")\n</pre> # Stratified - city + day of week + hour of day level config = {     \"time_col\": \"time\",     \"switch_frequency\": \"4h\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"day_of_week\", \"hour_4h\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config)  plot_calendar(power.splitter, df, \"stratified calendar (day of week + hour of day + city level)\") Out[10]: <pre>&lt;ggplot: (305061328)&gt;</pre>"},{"location":"quickstart.html","title":"Quickstart","text":"<p>Get started with <code>cluster-experiments</code> in minutes! This guide will walk you through installation and your first experiment analysis.</p>"},{"location":"quickstart.html#installation","title":"Installation","text":"<p>Install via pip:</p> <pre><code>pip install cluster-experiments\n</code></pre> <p>Requirements</p> <ul> <li>Python 3.8 or higher</li> <li>Main dependencies: <code>pandas</code>, <code>numpy</code>, <code>scipy</code>, <code>statsmodels</code></li> </ul>"},{"location":"quickstart.html#1-your-first-analysis","title":"1. Your First Analysis","text":"<p>Let's analyze a simple A/B test with multiple metrics. This is the most common use case. See Simple A/B Test for a complete walkthrough.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom cluster_experiments import AnalysisPlan, Variant\n\n# 1. Set seed for reproducibility\nnp.random.seed(42)\n\n# 2. Create simulated data\nN = 1_000\ndf = pd.DataFrame({\n    \"variant\": np.random.choice([\"control\", \"treatment\"], N),\n    \"orders\": np.random.poisson(10, N),\n    \"visits\": np.random.poisson(100, N),\n})\n# Add some treatment effect to orders\ndf.loc[df[\"variant\"] == \"treatment\", \"orders\"] += np.random.poisson(1, df[df[\"variant\"] == \"treatment\"].shape[0])\n\ndf[\"converted\"] = (df[\"orders\"] &gt; 0).astype(int)\ndf[\"cost\"] = np.random.normal(50, 10, N) # New metric: cost\ndf[\"clicks\"] = np.random.poisson(200, N) # New metric: clicks\n\n# 3. Define your analysis plan\nplan = AnalysisPlan.from_metrics_dict({\n    \"metrics\": [\n        {\"name\": \"orders\", \"alias\": \"revenue\", \"metric_type\": \"simple\"},\n        {\"name\": \"converted\", \"alias\": \"conversion\", \"metric_type\": \"ratio\", \"numerator\": \"converted\", \"denominator\": \"visits\"},\n        {\"name\": \"cost\", \"alias\": \"avg_cost\", \"metric_type\": \"simple\"},\n        {\"name\": \"clicks\", \"alias\": \"ctr\", \"metric_type\": \"ratio\", \"numerator\": \"clicks\", \"denominator\": \"visits\"}\n    ],\n    \"variants\": [\n        {\"name\": \"control\", \"is_control\": True},\n        {\"name\": \"treatment\", \"is_control\": False}\n    ],\n    \"variant_col\": \"variant\",\n    \"analysis_type\": \"ols\"\n})\n\n# 4. Run analysis on your dataframe\nresults = plan.analyze(df)\nprint(results.to_dataframe().head())\n</code></pre> <p>Output: <pre><code>  metric_alias control_variant_name treatment_variant_name  control_variant_mean  treatment_variant_mean analysis_type           ate  ate_ci_lower  ate_ci_upper   p_value     std_error     dimension_name dimension_value  alpha\n0      revenue              control              treatment              9.973469               10.994118           ols  1.020648e+00  6.140829e-01  1.427214e+00  8.640027e-07  2.074351e-01  __total_dimension           total   0.05\n1   conversion              control              treatment              1.000000                1.000000           ols -4.163336e-16 -5.971983e-16 -2.354689e-16  6.432406e-06  9.227960e-17  __total_dimension           total   0.05\n2     avg_cost              control              treatment             49.463206               49.547386           ols  8.417999e-02 -1.222365e+00  1.390725e+00  8.995107e-01  6.666166e-01  __total_dimension           total   0.05\n3          ctr              control              treatment            199.795918              199.692157           ols -1.037615e-01 -1.767938e+00  1.560415e+00  9.027376e-01  8.490855e-01  __total_dimension           total   0.05\n</code></pre></p>"},{"location":"quickstart.html#11-understanding-your-results","title":"1.1. Understanding Your Results","text":"<p>The results dataframe includes:</p> Column Description <code>metric</code> Name of the metric being analyzed <code>control_mean</code> Average value in control group <code>treatment_mean</code> Average value in treatment group <code>ate</code> Average Treatment Effect (absolute difference) <code>ate_ci_lower/upper</code> 95% confidence interval for ATE <code>p_value</code> Statistical significance (&lt; 0.05 = significant) <p>Interpreting Results</p> <ul> <li>p_value &lt; 0.05: Result is statistically significant (95% confidence)</li> <li>Confidence interval: If it doesn't include 0, effect is significant (95% confidence)</li> </ul>"},{"location":"quickstart.html#12-analysis-extensions-ratio-metrics","title":"1.2. Analysis Extensions: Ratio Metrics","text":"<p><code>cluster-experiments</code> has built-in support for ratio metrics (e.g., conversion rate, average order value), as seen in the first example:</p> <pre><code># Ratio metric: conversions / visits\n{\n    'alias': 'conversion_rate',\n    'metric_type': 'ratio',\n    'numerator_name': 'converted',      # Numerator column\n    'denominator_name': 'visits'         # Denominator column\n}\n</code></pre> <p>The library automatically handles the statistical complexities of ratio metrics using the Delta Method.</p>"},{"location":"quickstart.html#13-analysis-extensions-multi-dimensional-analysis","title":"1.3. Analysis Extensions: Multi-dimensional Analysis","text":"<p>Slice your results by dimensions (e.g., city, device type):</p> <pre><code>from cluster_experiments import Dimension\n\n# Example with complete configuration\nanalysis_plan = AnalysisPlan.from_metrics_dict({\n    'metrics': [\n        {'name': 'orders', 'alias': 'revenue', 'metric_type': 'simple'}\n    ],\n    'variants': [\n        {'name': 'control', 'is_control': True},\n        {'name': 'treatment', 'is_control': False}\n    ],\n    'variant_col': 'variant',\n    'dimensions': [\n        {'name': 'city', 'values': ['NYC', 'LA', 'Chicago']},\n        {'name': 'device', 'values': ['mobile', 'desktop']},\n    ],\n    'analysis_type': 'ols',\n})\n</code></pre> <p>Results will include treatment effects for each dimension slice.</p>"},{"location":"quickstart.html#2-power-analysis","title":"2. Power Analysis","text":"<p>Before running an experiment, it's crucial to know how long it needs to run to detect a significant effect. See the Power Analysis Guide for more complex designs (switchback, cluster randomization) and simulation methods.</p>"},{"location":"quickstart.html#21-mde","title":"2.1. MDE","text":"<p>Calculate the Minimum Detectable Effect (MDE) for a given sample size, $\\alpha$ and $\\beta$ parameters.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom cluster_experiments import NormalPowerAnalysis\n\n# Create sample historical data\nnp.random.seed(42)\nN = 500\nhistorical_data = pd.DataFrame({\n    'user_id': range(N),\n    'metric': np.random.normal(100, 20, N),\n    'date': pd.to_datetime('2025-10-01') + pd.to_timedelta(np.random.randint(0, 30, N), unit='d')\n})\n\npower_analysis = NormalPowerAnalysis.from_dict({\n    'analysis': 'ols',\n    'splitter': 'non_clustered',\n    'target_col': 'metric',\n    'time_col': 'date'\n})\n\nmde = power_analysis.mde(historical_data, power=0.8)\nprint(f\"Minimum Detectable Effect: {mde:.2f}\")\n</code></pre> <p>Output: <pre><code>Minimum Detectable Effect: 4.94\n</code></pre></p>"},{"location":"quickstart.html#22-calculate-power","title":"2.2. Calculate Power","text":"<p>Calculate the statistical power for a specific effect size you expect to see.</p> <pre><code>power = power_analysis.power_analysis(historical_data, average_effect=3.5)\nprint(f\"Power: {power:.1%}\")\n</code></pre> <p>Output: <pre><code>Power: 51.1%\n</code></pre></p>"},{"location":"quickstart.html#23-power-curve","title":"2.3. Power Curve","text":"<p>Generate a power curve to see how power changes with effect size.</p> <pre><code># Calculate power for multiple effect sizes\neffect_sizes = [2.0, 4.0, 6.0, 8.0, 10.0]\npower_curve = power_analysis.power_line(\n    historical_data,\n    average_effects=effect_sizes\n)\n\n# View power curve as a DataFrame\nimport pandas as pd\npower_df = pd.DataFrame([\n    {\"effect_size\": k, \"power\": round(v, 2)}\n    for k, v in power_curve.items()\n])\nprint(power_df.to_string(index=False))\n</code></pre> <p>Output: <pre><code> effect_size  power\n         2.0   0.18\n         4.0   0.54\n         6.0   0.87\n         8.0   0.98\n        10.0   1.00\n</code></pre></p> <p></p>"},{"location":"quickstart.html#3-quick-reference","title":"3. Quick Reference","text":""},{"location":"quickstart.html#31-analysis-types","title":"3.1. Analysis Types","text":"<p>Choose the appropriate analysis method:</p> Analysis Type When to Use <code>ols</code> Standard A/B test, individual randomization <code>clustered_ols</code> Cluster randomization (stores, cities, etc.) <code>gee</code> Repeated measures, correlated observations <code>mlm</code> Multi-level/hierarchical data <code>synthetic_control</code> Observational studies, no randomization"},{"location":"quickstart.html#32-dictionary-vs-class-based-api","title":"3.2. Dictionary vs Class-Based API","text":"<p><code>cluster-experiments</code> offers two ways to define analysis plans, catering to different needs:</p>"},{"location":"quickstart.html#321-dictionary-configuration","title":"3.2.1. Dictionary Configuration","text":"<p>Best for storing configurations in YAML/JSON files and automated pipelines.</p> <pre><code>config = {\n    \"metrics\": [\n        {\"name\": \"orders\", \"alias\": \"revenue\", \"metric_type\": \"simple\"},\n        {\"name\": \"converted\", \"alias\": \"conversion\", \"metric_type\": \"ratio\", \"numerator\": \"converted\", \"denominator\": \"visits\"}\n    ],\n    \"variants\": [\n        {\"name\": \"control\", \"is_control\": True},\n        {\"name\": \"treatment\", \"is_control\": False}\n    ],\n    \"variant_col\": \"variant\",\n    \"analysis_type\": \"ols\"\n}\n\nplan = AnalysisPlan.from_metrics_dict(config)\n</code></pre>"},{"location":"quickstart.html#322-class-based-api","title":"3.2.2 Class-Based API","text":"<p>Best for exploration and custom extensions.</p> <pre><code>from cluster_experiments import HypothesisTest, SimpleMetric, Variant\n\n# Explicitly define objects\nrevenue_metric = SimpleMetric(name=\"orders\", alias=\"revenue\")\ncontrol = Variant(\"control\", is_control=True)\ntreatment = Variant(\"treatment\", is_control=False)\n\nplan = AnalysisPlan(\n    tests=[HypothesisTest(metric=revenue_metric, analysis_type=\"ols\")],\n    variants=[control, treatment],\n    variant_col='variant'\n)\n</code></pre>"},{"location":"quickstart.html#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first analysis, explore:</p> <ul> <li>\ud83d\udcd6 API Reference - Detailed documentation for all classes</li> <li>Example Gallery - Real-world use cases and patterns</li> <li>Power Analysis Guide - Design experiments with confidence</li> <li>\ud83e\udd1d Contributing - Help improve the library</li> </ul>"},{"location":"quickstart.html#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcdd Documentation</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"relative.html","title":"Relative Lift Analysis","text":"<p>Dataframe creation</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nfrom copy import deepcopy\n\nfrom cluster_experiments import AnalysisPlan, NormalPowerAnalysis\n\ndef get_user_df(n_users=10_000):\n    df = pd.DataFrame(\n        {\n            \"customer_id\": np.arange(n_users),\n            \"orders_pre\": np.random.poisson(10, n_users),\n            \"_treatment\": np.random.rand(n_users) &gt; 0.5,\n            \"X1\": np.random.poisson(1, n_users),\n            \"X2\": np.random.poisson(2, n_users),\n        }\n    )\n    df = df.assign(**{\"_treatment\": df[\"_treatment\"].astype(int), \"orders\": lambda x: x[\"orders_pre\"] + 2 * x[\"X1\"] + x[\"X2\"] + 0.1 * x[\"_treatment\"]})\n    df = df.assign(\n        **{\n            \"center_X1\": lambda x: x[\"X1\"] - x[\"X1\"].mean(),\n            \"center_X2\": lambda x: x[\"X2\"] - x[\"X2\"].mean(),\n        }\n    )\n    df[\"_treatment\"] = df[\"_treatment\"].map({0: \"A\", 1: \"B\"})\n    return df\n\n\nuser_df = get_user_df()\n</pre> import numpy as np import pandas as pd  from copy import deepcopy  from cluster_experiments import AnalysisPlan, NormalPowerAnalysis  def get_user_df(n_users=10_000):     df = pd.DataFrame(         {             \"customer_id\": np.arange(n_users),             \"orders_pre\": np.random.poisson(10, n_users),             \"_treatment\": np.random.rand(n_users) &gt; 0.5,             \"X1\": np.random.poisson(1, n_users),             \"X2\": np.random.poisson(2, n_users),         }     )     df = df.assign(**{\"_treatment\": df[\"_treatment\"].astype(int), \"orders\": lambda x: x[\"orders_pre\"] + 2 * x[\"X1\"] + x[\"X2\"] + 0.1 * x[\"_treatment\"]})     df = df.assign(         **{             \"center_X1\": lambda x: x[\"X1\"] - x[\"X1\"].mean(),             \"center_X2\": lambda x: x[\"X2\"] - x[\"X2\"].mean(),         }     )     df[\"_treatment\"] = df[\"_treatment\"].map({0: \"A\", 1: \"B\"})     return df   user_df = get_user_df() In\u00a0[\u00a0]: Copied! <pre>config_relative = {\n    \"analysis\": \"ols\",\n    \"perturbator\": \"constant\",\n    \"splitter\": \"non_clustered\",\n    \"relative_effect\": True,\n    \"target_col\": \"orders\",\n    \"covariates\": [\"X1\"]\n}\n\nconfig_vanilla = deepcopy(config_relative)\nconfig_vanilla[\"relative_effect\"] = False\n\npw_relative = NormalPowerAnalysis.from_dict(config_relative)\npw_vanilla = NormalPowerAnalysis.from_dict(config_vanilla)\n</pre> config_relative = {     \"analysis\": \"ols\",     \"perturbator\": \"constant\",     \"splitter\": \"non_clustered\",     \"relative_effect\": True,     \"target_col\": \"orders\",     \"covariates\": [\"X1\"] }  config_vanilla = deepcopy(config_relative) config_vanilla[\"relative_effect\"] = False  pw_relative = NormalPowerAnalysis.from_dict(config_relative) pw_vanilla = NormalPowerAnalysis.from_dict(config_vanilla)   <p>Calculate relative MDE</p> In\u00a0[3]: Copied! <pre>pw_relative.mde(\n    user_df,\n    n_simulations=1\n)\n</pre> pw_relative.mde(     user_df,     n_simulations=1 ) Out[3]: <pre>0.013935217149223833</pre> <p>Calculate absolute MDE, shows different output</p> In\u00a0[\u00a0]: Copied! <pre>pw_vanilla.mde(\n    user_df,\n    n_simulations=1\n)\n</pre> pw_vanilla.mde(     user_df,     n_simulations=1 ) Out[\u00a0]: <pre>0.19537228277979887</pre> <p>When dividing by baseline to get relative MDE, it is slightly lower, because this would ignore the variance in the baseline.</p> In\u00a0[\u00a0]: Copied! <pre>float(\n    pw_vanilla.mde(\n        user_df,\n        n_simulations=1\n    ) / user_df[\"orders\"].mean()\n)\n</pre> float(     pw_vanilla.mde(         user_df,         n_simulations=1     ) / user_df[\"orders\"].mean() ) Out[\u00a0]: <pre>0.013868501242931779</pre> In\u00a0[\u00a0]: Copied! <pre>relative_plan_config = {\n        \"metrics\": [\n            {\"alias\": \"Orders\", \"name\": \"orders\"},\n        ],\n        \"variants\": [\n            {\"name\": \"A\", \"is_control\": True},\n            {\"name\": \"B\", \"is_control\": False},\n        ],\n        \"analysis_type\": \"ols\",\n        \"variant_col\": \"_treatment\",\n        \"analysis_config\": {\"relative_effect\": True, \"covariates\": [\"X1\"]},\n    }\nvanilla_plan_config = deepcopy(relative_plan_config)\nvanilla_plan_config[\"analysis_config\"] = {\"covariates\": [\"X1\"]}\n\nrelative_plan = AnalysisPlan.from_metrics_dict(relative_plan_config)\nvanilla_plan = AnalysisPlan.from_metrics_dict(vanilla_plan_config)\n</pre> relative_plan_config = {         \"metrics\": [             {\"alias\": \"Orders\", \"name\": \"orders\"},         ],         \"variants\": [             {\"name\": \"A\", \"is_control\": True},             {\"name\": \"B\", \"is_control\": False},         ],         \"analysis_type\": \"ols\",         \"variant_col\": \"_treatment\",         \"analysis_config\": {\"relative_effect\": True, \"covariates\": [\"X1\"]},     } vanilla_plan_config = deepcopy(relative_plan_config) vanilla_plan_config[\"analysis_config\"] = {\"covariates\": [\"X1\"]}  relative_plan = AnalysisPlan.from_metrics_dict(relative_plan_config) vanilla_plan = AnalysisPlan.from_metrics_dict(vanilla_plan_config) <p>Now we run the analysis for both plans</p> In\u00a0[\u00a0]: Copied! <pre>results_rel = relative_plan.analyze(user_df)\nresults_vanilla = vanilla_plan.analyze(user_df)\n</pre> results_rel = relative_plan.analyze(user_df) results_vanilla = vanilla_plan.analyze(user_df)  <p>Results are obviously different, as one is absolute and the other relative.</p> In\u00a0[8]: Copied! <pre>results_rel.to_dataframe()[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\", \"p_value\"]]\n</pre> results_rel.to_dataframe()[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\", \"p_value\"]] Out[8]: ate ate_ci_lower ate_ci_upper std_error p_value 0 0.00147 -0.008271 0.011211 0.00497 0.767389 In\u00a0[\u00a0]: Copied! <pre>results_vanilla.to_dataframe()[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\", \"p_value\"]]\n</pre> results_vanilla.to_dataframe()[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\", \"p_value\"]] Out[\u00a0]: ate ate_ci_lower ate_ci_upper std_error p_value 0 0.020644 -0.116049 0.157337 0.069743 0.767224 <p>When dividing by baseline to get relative effect and confidence intervals, the variance in the baseline is ignored, leading to slightly narrower intervals.</p> In\u00a0[\u00a0]: Copied! <pre>results_df = results_vanilla.to_dataframe()\nresults_df[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\"]] /= results_df[\"control_variant_mean\"].squeeze()\nresults_df[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\", \"p_value\"]]\n</pre> results_df = results_vanilla.to_dataframe() results_df[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\"]] /= results_df[\"control_variant_mean\"].squeeze() results_df[[\"ate\", \"ate_ci_lower\", \"ate_ci_upper\", \"std_error\", \"p_value\"]] Out[\u00a0]: ate ate_ci_lower ate_ci_upper std_error p_value 0 0.00147 -0.008264 0.011204 0.004966 0.767224 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"relative.html#relative-lift-analysis","title":"Relative Lift Analysis\u00b6","text":"<p>This notebook exemplifies how the library supports absolute and relative effects in MDE analysis and experiment analysis in OLS.</p> <p>The steps include:</p> <ol> <li>Create dataframe at customer level</li> <li>Run MDE analysis with absolute and relative effects, comparing the two of them</li> <li>Run experiment analysis with absolute and relative effects, comparing the two of them</li> </ol>"},{"location":"relative.html#mde-analysis-with-absolute-and-relative-effects","title":"MDE analysis with absolute and relative effects\u00b6","text":"<p>First we create NormalPowerAnalysis objects for absolute and relative effects</p>"},{"location":"relative.html#experiment-analysis-with-absolute-and-relative-effects","title":"Experiment analysis with absolute and relative effects\u00b6","text":"<p>First we create AnalysisPlan objects for absolute and relative effects</p>"},{"location":"switchback.html","title":"Stratified Switchback","text":"<p>This notebook shows how to use the switchback module. In particular, it shows how to create a PowerAnalysis object with a switchback splitter, using a time column and 30 min splits.</p> <p>It uses the splitter of the PowerAnalysis object to simulate the treatment assignment, and shows how the stratification of the clusters works.</p> <p>In the end, it also shows how to run the power analysis.</p> In\u00a0[1]: Copied! <pre>from cluster_experiments import PowerAnalysis\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\n</pre> from cluster_experiments import PowerAnalysis import pandas as pd import numpy as np  np.random.seed(42) In\u00a0[2]: Copied! <pre># Define bihourly switchback splitter\nconfig = {\n    \"time_col\": \"time\",\n    \"switch_frequency\": \"30min\",\n    \"perturbator\": \"constant\",\n    \"analysis\": \"ols_clustered\",\n    \"splitter\": \"switchback_stratified\",\n    \"cluster_cols\": [\"time\", \"city\"],\n    \"strata_cols\": [\"day_of_week\", \"hour_of_day\", \"city\"],\n    \"target_col\": \"y\",\n}\n\npower = PowerAnalysis.from_dict(config)\n</pre> # Define bihourly switchback splitter config = {     \"time_col\": \"time\",     \"switch_frequency\": \"30min\",     \"perturbator\": \"constant\",     \"analysis\": \"ols_clustered\",     \"splitter\": \"switchback_stratified\",     \"cluster_cols\": [\"time\", \"city\"],     \"strata_cols\": [\"day_of_week\", \"hour_of_day\", \"city\"],     \"target_col\": \"y\", }  power = PowerAnalysis.from_dict(config) In\u00a0[3]: Copied! <pre># Define data with random dates\ndf_raw = pd.DataFrame(\n    {   \n        # Generate 10k random timestamps from 2021-01-01 to 2021-01-10\n        \"time\": pd.date_range(\"2021-01-01\", \"2021-01-08\", freq=\"1min\")[\n            np.random.randint(7 * 24 * 60, size=7 * 24 * 60)\n        ],\n        \"y\": np.random.randn(7 * 24 * 60),\n    }\n).assign(\n    day_of_week=lambda df: df.time.dt.dayofweek,\n    hour_of_day=lambda df: df.time.dt.hour\n)\ndf = pd.concat([df_raw.assign(city=city) for city in (\"TGN\", \"NYC\", \"LON\", \"REU\")])\n</pre> # Define data with random dates df_raw = pd.DataFrame(     {            # Generate 10k random timestamps from 2021-01-01 to 2021-01-10         \"time\": pd.date_range(\"2021-01-01\", \"2021-01-08\", freq=\"1min\")[             np.random.randint(7 * 24 * 60, size=7 * 24 * 60)         ],         \"y\": np.random.randn(7 * 24 * 60),     } ).assign(     day_of_week=lambda df: df.time.dt.dayofweek,     hour_of_day=lambda df: df.time.dt.hour ) df = pd.concat([df_raw.assign(city=city) for city in (\"TGN\", \"NYC\", \"LON\", \"REU\")]) In\u00a0[4]: Copied! <pre>df.head(10)\n</pre> df.head(10) Out[4]: time y day_of_week hour_of_day city 0 2021-01-06 01:10:00 -0.216104 2 1 TGN 1 2021-01-01 14:20:00 -1.016524 4 14 TGN 2 2021-01-04 17:50:00 -2.326362 0 17 TGN 3 2021-01-04 14:31:00 -0.358456 0 14 TGN 4 2021-01-04 23:34:00 -0.490571 0 23 TGN 5 2021-01-05 08:25:00 -0.149901 1 8 TGN 6 2021-01-01 07:46:00 -0.628898 4 7 TGN 7 2021-01-04 01:46:00 1.829330 0 1 TGN 8 2021-01-04 20:58:00 0.517337 0 20 TGN 9 2021-01-06 18:42:00 -0.499613 2 18 TGN In\u00a0[5]: Copied! <pre>treatments = power.splitter.assign_treatment_df(df)\n</pre> treatments = power.splitter.assign_treatment_df(df) In\u00a0[6]: Copied! <pre># For every city, we have a balanced AB split\n(\n    treatments\n    .loc[:, [\"city\", \"treatment\", \"time\"]]\n    .drop_duplicates()\n    .groupby([\"city\", \"treatment\"])\n    .size()\n    .head(10)\n)\n</pre> # For every city, we have a balanced AB split (     treatments     .loc[:, [\"city\", \"treatment\", \"time\"]]     .drop_duplicates()     .groupby([\"city\", \"treatment\"])     .size()     .head(10) ) Out[6]: <pre>city  treatment\nLON   A            168\n      B            168\nNYC   A            168\n      B            168\nREU   A            168\n      B            168\nTGN   A            168\n      B            168\ndtype: int64</pre> In\u00a0[7]: Copied! <pre># For every hour of day, we have a balanced AB split\n(\n    treatments\n    .loc[:, [\"city\", \"treatment\", \"time\", \"hour_of_day\"]]\n    .drop_duplicates()\n    .groupby([\"hour_of_day\", \"treatment\"])\n    .size()\n    .head(10)\n)\n</pre> # For every hour of day, we have a balanced AB split (     treatments     .loc[:, [\"city\", \"treatment\", \"time\", \"hour_of_day\"]]     .drop_duplicates()     .groupby([\"hour_of_day\", \"treatment\"])     .size()     .head(10) ) Out[7]: <pre>hour_of_day  treatment\n0            A            28\n             B            28\n1            A            28\n             B            28\n2            A            28\n             B            28\n3            A            28\n             B            28\n4            A            28\n             B            28\ndtype: int64</pre> In\u00a0[8]: Copied! <pre># For every day of week, we have a balanced AB split\n(\n    treatments\n    .loc[:, [\"city\", \"treatment\", \"time\", \"day_of_week\"]]\n    .drop_duplicates()\n    .groupby([\"day_of_week\", \"treatment\"])\n    .size()\n    .head(10)\n)\n</pre> # For every day of week, we have a balanced AB split (     treatments     .loc[:, [\"city\", \"treatment\", \"time\", \"day_of_week\"]]     .drop_duplicates()     .groupby([\"day_of_week\", \"treatment\"])     .size()     .head(10) ) Out[8]: <pre>day_of_week  treatment\n0            A            96\n             B            96\n1            A            96\n             B            96\n2            A            96\n             B            96\n3            A            96\n             B            96\n4            A            96\n             B            96\ndtype: int64</pre> In\u00a0[9]: Copied! <pre># In the first 30 minutes of the day, LON, NYC, REU, and TGN have a constant treatment\ntreatments.query(\"time &lt; '2021-01-01 00:30:00'\").groupby([\"city\", \"treatment\"]).size()\n</pre> # In the first 30 minutes of the day, LON, NYC, REU, and TGN have a constant treatment treatments.query(\"time &lt; '2021-01-01 00:30:00'\").groupby([\"city\", \"treatment\"]).size() Out[9]: <pre>city  treatment\nLON   B            36\nNYC   B            36\nREU   A            36\nTGN   B            36\ndtype: int64</pre> In\u00a0[10]: Copied! <pre># We can run power analysis\npower.power_analysis(df, average_effect=0.01)\n</pre> # We can run power analysis power.power_analysis(df, average_effect=0.01) Out[10]: <pre>0.17</pre>"},{"location":"synthetic_control.html","title":"Synthetic Control","text":"<p>This NB serves as a tutorial on how to run power analysis using synthetic control analysis. It considers that looking at some time frame and some clusters, one of the clusters will receive a treatment and the rest remain in control. This treatment cluster will receive some effect from the Perturbator. Then, we run a power analysis to understand how many times we can capture this perturbation.</p> <p>In the end we compare the results with Clustered OLS method.</p> <p>We will generate some random data with normal distribution of mean 10 and standard deviation of 10.</p> In\u00a0[9]: Copied! <pre>from itertools import product\nimport numpy as np\nimport pandas as pd\nfrom cluster_experiments.experiment_analysis import SyntheticControlAnalysis, ClusteredOLSAnalysis\nfrom cluster_experiments.perturbator import ConstantPerturbator\nfrom cluster_experiments.power_analysis import PowerAnalysisWithPreExperimentData, PowerAnalysis\nfrom cluster_experiments.random_splitter import FixedSizeClusteredSplitter\nimport plotnine as p9\n\n\ndef generate_data(N, start_date, end_date):\n    dates = pd.date_range(start_date, end_date, freq=\"d\")\n\n    users = [f\"User {i}\" for i in range(N)]\n\n    combinations = list(product(users, dates))\n\n    target_values = np.random.normal(100, 10, size=len(combinations))\n\n    df = pd.DataFrame(combinations, columns=[\"user\", \"date\"])\n    df[\"target\"] = target_values\n\n\n    return df\n</pre> from itertools import product import numpy as np import pandas as pd from cluster_experiments.experiment_analysis import SyntheticControlAnalysis, ClusteredOLSAnalysis from cluster_experiments.perturbator import ConstantPerturbator from cluster_experiments.power_analysis import PowerAnalysisWithPreExperimentData, PowerAnalysis from cluster_experiments.random_splitter import FixedSizeClusteredSplitter import plotnine as p9   def generate_data(N, start_date, end_date):     dates = pd.date_range(start_date, end_date, freq=\"d\")      users = [f\"User {i}\" for i in range(N)]      combinations = list(product(users, dates))      target_values = np.random.normal(100, 10, size=len(combinations))      df = pd.DataFrame(combinations, columns=[\"user\", \"date\"])     df[\"target\"] = target_values       return df   In\u00a0[10]: Copied! <pre>df = generate_data(20, \"2022-01-01\", \"2022-01-30\")\nintervention_date = \"2022-01-15\"\neffects = [0, 1, 5, 10, 20]\n</pre> df = generate_data(20, \"2022-01-01\", \"2022-01-30\") intervention_date = \"2022-01-15\" effects = [0, 1, 5, 10, 20] <p>The graph below shows the evolution of target for each user. When running the synthetic control analysis:</p> <ul> <li>Select randomly one user to be in treatment group.</li> <li>Use pre experiment data (in this case case from 1 to 15th of Jan) to find the best combination of weights</li> <li>Apply weights to donors and generate synthetic control user</li> </ul> In\u00a0[11]: Copied! <pre>(p9.ggplot(df, p9.aes(x=\"date\", y=\"target\", color = 'user')) \n + p9.geom_line() \n + p9.labs(title=\"Data\", x=\"Date\", y=\"Target Value\") \n + p9.theme(axis_text_x=p9.element_text(angle=60), figure_size=(12, 6)))\n</pre> (p9.ggplot(df, p9.aes(x=\"date\", y=\"target\", color = 'user'))   + p9.geom_line()   + p9.labs(title=\"Data\", x=\"Date\", y=\"Target Value\")   + p9.theme(axis_text_x=p9.element_text(angle=60), figure_size=(12, 6)))  In\u00a0[12]: Copied! <pre>df\n</pre> df Out[12]: user date target 0 User 0 2022-01-01 99.884697 1 User 0 2022-01-02 107.471770 2 User 0 2022-01-03 86.269132 3 User 0 2022-01-04 116.092658 4 User 0 2022-01-05 100.131065 ... ... ... ... 595 User 19 2022-01-26 101.861088 596 User 19 2022-01-27 89.713442 597 User 19 2022-01-28 116.505479 598 User 19 2022-01-29 111.858909 599 User 19 2022-01-30 107.270857 <p>600 rows \u00d7 3 columns</p> In\u00a0[13]: Copied! <pre>sw = FixedSizeClusteredSplitter(n_treatment_clusters=1, cluster_cols=[\"user\"])\n\nperturbator = ConstantPerturbator(\n    average_effect=10,\n)\n\nanalysis = SyntheticControlAnalysis(\n    cluster_cols=[\"user\"], time_col=\"date\", intervention_date=intervention_date\n)\n\npw = PowerAnalysisWithPreExperimentData(\n     splitter=sw, analysis=analysis, n_simulations=200, perturbator=perturbator\n)\n</pre> sw = FixedSizeClusteredSplitter(n_treatment_clusters=1, cluster_cols=[\"user\"])  perturbator = ConstantPerturbator(     average_effect=10, )  analysis = SyntheticControlAnalysis(     cluster_cols=[\"user\"], time_col=\"date\", intervention_date=intervention_date )  pw = PowerAnalysisWithPreExperimentData(      splitter=sw, analysis=analysis, n_simulations=200, perturbator=perturbator )  In\u00a0[14]: Copied! <pre>point_estimates = list(pw.simulate_point_estimate(df))\n</pre> point_estimates = list(pw.simulate_point_estimate(df)) In\u00a0[15]: Copied! <pre>mean_pe = pd.Series(point_estimates).mean()\n\n(p9.ggplot(pd.DataFrame(point_estimates, columns=[\"effect\"]), p9.aes(x=\"effect\"))\n  + p9.geom_histogram(bins=18)\n  + p9.scale_x_continuous(limits=(0, 15))\n+ p9.labs(title = \"Point Estimate Distribution\")\n+ p9.geom_vline(xintercept=10, color=\"red\")\n+ p9.geom_text(x=10-2, y=20, label=\"True Effect\", color=\"red\", size = 8)\n+ p9.geom_vline(xintercept=mean_pe, color=\"blue\")\n+ p9.geom_text(x=mean_pe+2, y=20, label=\"Mean of point estimates\", color=\"blue\", size = 8)\n)\n</pre> mean_pe = pd.Series(point_estimates).mean()  (p9.ggplot(pd.DataFrame(point_estimates, columns=[\"effect\"]), p9.aes(x=\"effect\"))   + p9.geom_histogram(bins=18)   + p9.scale_x_continuous(limits=(0, 15)) + p9.labs(title = \"Point Estimate Distribution\") + p9.geom_vline(xintercept=10, color=\"red\") + p9.geom_text(x=10-2, y=20, label=\"True Effect\", color=\"red\", size = 8) + p9.geom_vline(xintercept=mean_pe, color=\"blue\") + p9.geom_text(x=mean_pe+2, y=20, label=\"Mean of point estimates\", color=\"blue\", size = 8) )    <pre>/home/asaas/PycharmProjects/cluster-experiments/.venv/lib/python3.10/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_histogram : Removed 2 rows containing missing values.\n</pre> <p>This power line takes a long time to run because we calculate p values using a permutation test.</p> In\u00a0[16]: Copied! <pre>p_synth = pw.power_line(df, average_effects=effects)\n</pre> p_synth = pw.power_line(df, average_effects=effects) In\u00a0[17]: Copied! <pre>analysis = ClusteredOLSAnalysis(\n    cluster_cols=[\"user\"] )\n\npw = PowerAnalysis(\n    perturbator=perturbator, splitter=sw, analysis=analysis, n_simulations=200\n)\n\ndf = df.query(f\"date &gt; '{intervention_date}'\")\n\np_ols = pw.power_line(df, average_effects=effects, n_jobs=5)\n</pre> analysis = ClusteredOLSAnalysis(     cluster_cols=[\"user\"] )  pw = PowerAnalysis(     perturbator=perturbator, splitter=sw, analysis=analysis, n_simulations=200 )  df = df.query(f\"date &gt; '{intervention_date}'\")  p_ols = pw.power_line(df, average_effects=effects, n_jobs=5) In\u00a0[18]: Copied! <pre>df_results = pd.DataFrame({'effect' :list(p_synth.keys()), 'power' : list(p_synth.values())})\n</pre> df_results = pd.DataFrame({'effect' :list(p_synth.keys()), 'power' : list(p_synth.values())})  In\u00a0[19]: Copied! <pre>synth_df = pd.DataFrame({'effect' : p_synth.keys(), \n              'method': 'Synthetic',\n              'power': p_synth.values()})\n\nols_df = pd.DataFrame({'effect' : p_ols.keys(), \n              'method': 'OLS',\n              'power': p_ols.values()})\n\ndf_results = pd.concat([synth_df, ols_df])\n</pre> synth_df = pd.DataFrame({'effect' : p_synth.keys(),                'method': 'Synthetic',               'power': p_synth.values()})  ols_df = pd.DataFrame({'effect' : p_ols.keys(),                'method': 'OLS',               'power': p_ols.values()})  df_results = pd.concat([synth_df, ols_df]) In\u00a0[20]: Copied! <pre>(\np9.ggplot(df_results, p9.aes(x='effect', y='power', color='method')) \n+ p9.geom_line() \n+ p9.geom_point()\n+ p9.labs(\n    title=\"Comparison between Synthetic Control and OLS\",\n    x=\"Average effect size\", y=\"Power\"\n))\n</pre> ( p9.ggplot(df_results, p9.aes(x='effect', y='power', color='method'))  + p9.geom_line()  + p9.geom_point() + p9.labs(     title=\"Comparison between Synthetic Control and OLS\",     x=\"Average effect size\", y=\"Power\" )) <p>At first sight, we might think that OLS has more power than synthetic control. But we can clearly see that the OLS doesn't behave as expected when using only 1 treatment cluster. This gets clear when we see the power at 0 effect, in which should be equal to alpha (5% in this case). Therefore, is more appropriate to use synthetic control.</p>"},{"location":"synthetic_control.html#synthetic-control","title":"Synthetic Control\u00b6","text":""},{"location":"synthetic_control.html#generate-data","title":"Generate data\u00b6","text":""},{"location":"synthetic_control.html#point-estimates","title":"Point Estimates\u00b6","text":""},{"location":"synthetic_control.html#power-line-and-comparison-with-ols","title":"Power line and Comparison with OLS\u00b6","text":""},{"location":"washover_example.html","title":"Washover Example","text":"<p>This notebook shows how to use the washover module. In particular, it shows how to apply a ConstantWashover object with a 30min time delta.</p> In\u00a0[1]: Copied! <pre>## Import Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nfrom cluster_experiments import ConstantWashover\n</pre> ## Import Libraries  import numpy as np import pandas as pd from datetime import datetime, timedelta  from cluster_experiments import ConstantWashover In\u00a0[2]: Copied! <pre>## Generate a dummy dataset of 4 time periods of 2 hours with 10 orders each\n\nnp.random.seed(42)\n\nnum_rows = 5\n\ndef random_timestamp(start_time, end_time):\n    time_delta = end_time - start_time\n    random_seconds = np.random.randint(0, time_delta.total_seconds())\n    return start_time + timedelta(seconds=random_seconds)\n\ndef generate_data(start_time, end_time, treatment):\n    data = {\n        'order_id': np.random.randint(10**9, 10**10, size=num_rows),\n        'city_code': 'VAL',\n        'activation_time_local': [random_timestamp(start_time, end_time) for _ in range(num_rows)],\n        'bin_start_time_local': start_time,\n        'treatment': treatment\n    }\n    return pd.DataFrame(data)\n\nstart_times = [datetime(2024, 1, 22, 9, 0), datetime(2024, 1, 22, 11, 0),\n               datetime(2024, 1, 22, 13, 0), datetime(2024, 1, 22, 15, 0)]\n\ntreatments = ['control', 'variation', 'variation', 'control']\n\ndataframes = [generate_data(start, start + timedelta(hours=2), treatment) for start, treatment in zip(start_times, treatments)]\n\ndf = pd.concat(dataframes).sort_values(by='activation_time_local').reset_index(drop=True)\n\ndf.head(20)\n</pre> ## Generate a dummy dataset of 4 time periods of 2 hours with 10 orders each  np.random.seed(42)  num_rows = 5  def random_timestamp(start_time, end_time):     time_delta = end_time - start_time     random_seconds = np.random.randint(0, time_delta.total_seconds())     return start_time + timedelta(seconds=random_seconds)  def generate_data(start_time, end_time, treatment):     data = {         'order_id': np.random.randint(10**9, 10**10, size=num_rows),         'city_code': 'VAL',         'activation_time_local': [random_timestamp(start_time, end_time) for _ in range(num_rows)],         'bin_start_time_local': start_time,         'treatment': treatment     }     return pd.DataFrame(data)  start_times = [datetime(2024, 1, 22, 9, 0), datetime(2024, 1, 22, 11, 0),                datetime(2024, 1, 22, 13, 0), datetime(2024, 1, 22, 15, 0)]  treatments = ['control', 'variation', 'variation', 'control']  dataframes = [generate_data(start, start + timedelta(hours=2), treatment) for start, treatment in zip(start_times, treatments)]  df = pd.concat(dataframes).sort_values(by='activation_time_local').reset_index(drop=True)  df.head(20) Out[2]: order_id city_code activation_time_local bin_start_time_local treatment 0 8395928407 VAL 2024-01-22 09:19:44 2024-01-22 09:00:00 control 1 5298312065 VAL 2024-01-22 10:15:55 2024-01-22 09:00:00 control 2 3563451924 VAL 2024-01-22 10:24:11 2024-01-22 09:00:00 control 3 1787846414 VAL 2024-01-22 10:28:31 2024-01-22 09:00:00 control 4 5537253172 VAL 2024-01-22 10:47:00 2024-01-22 09:00:00 control 5 2855189739 VAL 2024-01-22 11:21:07 2024-01-22 11:00:00 variation 6 8667272366 VAL 2024-01-22 11:25:28 2024-01-22 11:00:00 variation 7 7548779029 VAL 2024-01-22 11:31:39 2024-01-22 11:00:00 variation 8 6152559666 VAL 2024-01-22 11:53:22 2024-01-22 11:00:00 variation 9 2250819632 VAL 2024-01-22 12:56:15 2024-01-22 11:00:00 variation 10 8767007473 VAL 2024-01-22 13:22:43 2024-01-22 13:00:00 variation 11 3609385266 VAL 2024-01-22 13:34:01 2024-01-22 13:00:00 variation 12 9370399619 VAL 2024-01-22 13:43:32 2024-01-22 13:00:00 variation 13 1279394470 VAL 2024-01-22 13:47:04 2024-01-22 13:00:00 variation 14 5147358011 VAL 2024-01-22 14:57:21 2024-01-22 13:00:00 variation 15 7643070057 VAL 2024-01-22 15:02:41 2024-01-22 15:00:00 control 16 5164334270 VAL 2024-01-22 16:11:37 2024-01-22 15:00:00 control 17 7528642437 VAL 2024-01-22 16:34:35 2024-01-22 15:00:00 control 18 2111451555 VAL 2024-01-22 16:49:05 2024-01-22 15:00:00 control 19 8140478823 VAL 2024-01-22 16:54:33 2024-01-22 15:00:00 control In\u00a0[3]: Copied! <pre>## Define washover with 30 min duration\nwashover = ConstantWashover(washover_time_delta=timedelta(minutes=30))\n\n## Apply washover to the dataframe, the orders with activation time within the first 30 minutes after every change in the treatment column, clustering by city and 2h time bin, will be dropped\ndf_analysis_washover = washover.washover(\n    df=df,\n    truncated_time_col='bin_start_time_local',\n    treatment_col='treatment',\n    cluster_cols=['city_code','bin_start_time_local'],\n    original_time_col='activation_time_local',\n)\n</pre> ## Define washover with 30 min duration washover = ConstantWashover(washover_time_delta=timedelta(minutes=30))  ## Apply washover to the dataframe, the orders with activation time within the first 30 minutes after every change in the treatment column, clustering by city and 2h time bin, will be dropped df_analysis_washover = washover.washover(     df=df,     truncated_time_col='bin_start_time_local',     treatment_col='treatment',     cluster_cols=['city_code','bin_start_time_local'],     original_time_col='activation_time_local', ) In\u00a0[4]: Copied! <pre>## Show the rows that have been dropped\nanti_joined_df = df.merge(df_analysis_washover['order_id'], how='left', indicator=True, on='order_id')\nanti_joined_df = anti_joined_df[anti_joined_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n\nanti_joined_df.head(10)\n</pre> ## Show the rows that have been dropped anti_joined_df = df.merge(df_analysis_washover['order_id'], how='left', indicator=True, on='order_id') anti_joined_df = anti_joined_df[anti_joined_df['_merge'] == 'left_only'].drop(columns=['_merge'])  anti_joined_df.head(10) Out[4]: order_id city_code activation_time_local bin_start_time_local treatment 0 8395928407 VAL 2024-01-22 09:19:44 2024-01-22 09:00:00 control 5 2855189739 VAL 2024-01-22 11:21:07 2024-01-22 11:00:00 variation 6 8667272366 VAL 2024-01-22 11:25:28 2024-01-22 11:00:00 variation 15 7643070057 VAL 2024-01-22 15:02:41 2024-01-22 15:00:00 control In\u00a0[5]: Copied! <pre>## Check DF shapes\nprint('df:', df.shape)\nprint('df_analysis_washover:', df_analysis_washover.shape)\n</pre> ## Check DF shapes print('df:', df.shape) print('df_analysis_washover:', df_analysis_washover.shape) <pre>df: (20, 5)\ndf_analysis_washover: (16, 5)\n</pre>"},{"location":"api/analysis_plan.html","title":"<code>from cluster_experiments.inference.analysis_plan import *</code>","text":""},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan","title":"<code>AnalysisPlan</code>","text":"<p>A class used to represent an Analysis Plan with a list of hypothesis tests and a list of variants. All the hypothesis tests in the same analysis plan will be analysed with the same dataframe, which will need to be passed in the analyze() method.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan--attributes","title":"Attributes","text":"<p>tests : List[HypothesisTest]     A list of HypothesisTest instances variants : List[Variant]     A list of Variant instances variant_col : str     name of the column with the experiment groups alpha : float     significance level used to construct confidence intervals</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>class AnalysisPlan:\n    \"\"\"\n    A class used to represent an Analysis Plan with a list of hypothesis tests and a list of variants.\n    All the hypothesis tests in the same analysis plan will be analysed with the same dataframe, which will need to be passed in the analyze() method.\n\n    Attributes\n    ----------\n    tests : List[HypothesisTest]\n        A list of HypothesisTest instances\n    variants : List[Variant]\n        A list of Variant instances\n    variant_col : str\n        name of the column with the experiment groups\n    alpha : float\n        significance level used to construct confidence intervals\n    \"\"\"\n\n    def __init__(\n        self,\n        tests: List[HypothesisTest],\n        variants: List[Variant],\n        variant_col: str = \"treatment\",\n        alpha: float = 0.05,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        tests : List[HypothesisTest]\n            A list of HypothesisTest instances\n        variants : List[Variant]\n            A list of Variant instances\n        variant_col : str\n            The name of the column containing the variant names.\n        alpha : float\n            significance level used to construct confidence intervals\n        \"\"\"\n\n        self.tests = tests\n        self.variants = variants\n        self.variant_col = variant_col\n        self.alpha = alpha\n\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"\n        Validates the inputs for the AnalysisPlan class.\n\n        Raises\n        ------\n        TypeError\n            If tests is not a list of HypothesisTest instances or if variants is not a list of Variant instances.\n        ValueError\n            If tests or variants are empty lists.\n        \"\"\"\n        if not isinstance(self.tests, list) or not all(\n            isinstance(test, HypothesisTest) for test in self.tests\n        ):\n            raise TypeError(\"Tests must be a list of HypothesisTest instances\")\n        if not isinstance(self.variants, list) or not all(\n            isinstance(variant, Variant) for variant in self.variants\n        ):\n            raise TypeError(\"Variants must be a list of Variant instances\")\n        if not isinstance(self.variant_col, str):\n            raise TypeError(\"Variant_col must be a string\")\n        if not self.tests:\n            raise ValueError(\"Tests list cannot be empty\")\n        if not self.variants:\n            raise ValueError(\"Variants list cannot be empty\")\n\n    def analyze(\n        self,\n        exp_data: pd.DataFrame,\n        pre_exp_data: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n    ) -&gt; AnalysisPlanResults:\n        \"\"\"\n        Method to run the experiment analysis.\n        \"\"\"\n\n        # Validate input data at the beginning\n        self._validate_data(exp_data, pre_exp_data)\n\n        analysis_results = AnalysisPlanResults()\n\n        for test in self.tests:\n            exp_data = test.add_covariates(exp_data, pre_exp_data)\n\n            for treatment_variant in self.treatment_variants:\n                for dimension in test.dimensions:\n                    for dimension_value in dimension.iterate_dimension_values():\n\n                        if verbose:\n                            logger.info(\n                                f\"Metric: {test.metric.alias}, \"\n                                f\"Treatment: {treatment_variant.name}, \"\n                                f\"Dimension: {dimension.name}, \"\n                                f\"Value: {dimension_value}\"\n                            )\n\n                        test_results = test.get_test_results(\n                            exp_data=exp_data,\n                            control_variant=self.control_variant,\n                            treatment_variant=treatment_variant,\n                            variant_col=self.variant_col,\n                            dimension=dimension,\n                            dimension_value=dimension_value,\n                            alpha=self.alpha,\n                        )\n\n                        analysis_results = analysis_results + test_results\n\n        return analysis_results\n\n    def _validate_data(\n        self, exp_data: pd.DataFrame, pre_exp_data: Optional[pd.DataFrame] = None\n    ):\n        \"\"\"\n        Validates the input dataframes for the analyze method.\n\n        Parameters\n        ----------\n        exp_data : pd.DataFrame\n            The experimental data\n        pre_exp_data : Optional[pd.DataFrame]\n            The pre-experimental data (optional)\n\n        Raises\n        ------\n        ValueError\n            If exp_data is not a DataFrame or is empty\n            If pre_exp_data is provided and is not a DataFrame or is empty\n        \"\"\"\n        if not isinstance(exp_data, pd.DataFrame):\n            raise ValueError(\"exp_data must be a pandas DataFrame\")\n        if exp_data.empty:\n            raise ValueError(\"exp_data cannot be empty\")\n        if pre_exp_data is not None:\n            if not isinstance(pre_exp_data, pd.DataFrame):\n                raise ValueError(\"pre_exp_data must be a pandas DataFrame if provided\")\n            if pre_exp_data.empty:\n                raise ValueError(\"pre_exp_data cannot be empty if provided\")\n\n    @property\n    def control_variant(self) -&gt; Variant:\n        \"\"\"\n        Returns the control variant from the list of variants. Raises an error if no control variant is found.\n\n        Returns\n        -------\n        Variant\n            The control variant\n\n        Raises\n        ------\n        ValueError\n            If no control variant is found\n        \"\"\"\n        for variant in self.variants:\n            if variant.is_control:\n                return variant\n        raise ValueError(\"No control variant found\")\n\n    @property\n    def treatment_variants(self) -&gt; List[Variant]:\n        \"\"\"\n        Returns the treatment variants from the list of variants. Raises an error if no treatment variants are found.\n\n        Returns\n        -------\n        List[Variant]\n            A list of treatment variants\n\n        Raises\n        ------\n        ValueError\n            If no treatment variants are found\n        \"\"\"\n        treatments = [variant for variant in self.variants if not variant.is_control]\n        if not treatments:\n            raise ValueError(\"No treatment variants found\")\n        return treatments\n\n    @classmethod\n    def from_metrics(\n        cls,\n        metrics: List[Metric],\n        variants: List[Variant],\n        variant_col: str = \"treatment\",\n        alpha: float = 0.05,\n        dimensions: Optional[List[Dimension]] = None,\n        analysis_type: str = \"default\",\n        analysis_config: Optional[Dict[str, Any]] = None,\n        custom_analysis_type_mapper: Optional[Dict[str, ExperimentAnalysis]] = None,\n    ) -&gt; \"AnalysisPlan\":\n        \"\"\"\n        Creates a simplified AnalysisPlan instance from a list of metrics. It will create HypothesisTest objects under the hood.\n        This shortcut does not support cupac, and uses the same dimensions, analysis type and analysis config for all metrics.\n\n        Parameters\n        ----------\n        metrics : List[Metric]\n            A list of Metric instances\n        variants : List[Variant]\n            A list of Variant instances\n        variant_col : str\n            The name of the column containing the variant names.\n        alpha : float\n            Significance level used to construct confidence intervals\n        dimensions : Optional[List[Dimension]]\n            A list of Dimension instances (optional)\n        analysis_type : str\n            The type of analysis to be conducted (default: \"default\")\n        analysis_config : Optional[Dict[str, Any]]\n            A dictionary containing analysis configuration options (optional)\n        custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]\n            An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n\n        Returns\n        -------\n        AnalysisPlan\n            An instance of AnalysisPlan\n        \"\"\"\n        tests = [\n            HypothesisTest(\n                metric=metric,\n                dimensions=dimensions or [],\n                analysis_type=analysis_type,\n                analysis_config=analysis_config or {},\n                custom_analysis_type_mapper=custom_analysis_type_mapper or {},\n            )\n            for metric in metrics\n        ]\n\n        return cls(\n            tests=tests,\n            variants=variants,\n            variant_col=variant_col,\n            alpha=alpha,\n        )\n\n    @classmethod\n    def from_metrics_config(cls, config: AnalysisPlanMetricsConfig) -&gt; \"AnalysisPlan\":\n        \"\"\"\n        Creates an AnalysisPlan instance from a metrics-based configuration object.\n\n        Parameters\n        ----------\n        config : AnalysisPlanMetricsConfig\n            An instance of AnalysisPlanMetricsConfig\n\n        Returns\n        -------\n        AnalysisPlan\n            An instance of AnalysisPlan\n        \"\"\"\n        metrics = [\n            Metric.from_metrics_config(metric_config)\n            for metric_config in config.metrics\n        ]\n        variants = [\n            Variant.from_metrics_config(variant_config)\n            for variant_config in config.variants\n        ]\n        dimensions = [\n            Dimension.from_metrics_config(dimension_config)\n            for dimension_config in config.dimensions\n        ]\n        return cls.from_metrics(\n            metrics=metrics,\n            variants=variants,\n            variant_col=config.variant_col,\n            alpha=config.alpha,\n            dimensions=dimensions,\n            analysis_type=config.analysis_type,\n            analysis_config=config.analysis_config,\n            custom_analysis_type_mapper=config.custom_analysis_type_mapper,\n        )\n\n    @classmethod\n    def from_config(cls, config: AnalysisPlanConfig) -&gt; \"AnalysisPlan\":\n        \"\"\"\n        Creates an AnalysisPlan instance from a configuration object.\n\n        Parameters\n        ----------\n        config : AnalysisPlanConfig\n            An instance of AnalysisPlanConfig\n\n        Returns\n        -------\n        AnalysisPlan\n            An instance of AnalysisPlan\n        \"\"\"\n        tests = [HypothesisTest.from_config(test) for test in config.tests]\n        variants = [\n            Variant.from_metrics_config(variant_config)\n            for variant_config in config.variants\n        ]\n        return cls(\n            tests=tests,\n            variants=variants,\n            variant_col=config.variant_col,\n            alpha=config.alpha,\n        )\n\n    @classmethod\n    def from_metrics_dict(cls, d: Dict[str, Any]) -&gt; \"AnalysisPlan\":\n        \"\"\"\n        Creates an AnalysisPlan instance from a metrics-based dictionary.\n\n        Parameters\n        ----------\n        d : Dict[str, Any]\n            A dictionary containing the analysis plan configuration, parametrised via metrics\n\n        Returns\n        -------\n        AnalysisPlan\n            An instance of AnalysisPlan\n        \"\"\"\n        config = AnalysisPlanMetricsConfig(**d)\n        return cls.from_metrics_config(config)\n\n    @classmethod\n    def from_dict(cls, d: Dict[str, Any]) -&gt; \"AnalysisPlan\":\n        \"\"\"\n        Creates an AnalysisPlan instance from a dictionary.\n\n        Parameters\n        ----------\n        d : Dict[str, Any]\n            A dictionary containing the analysis plan configuration\n\n        Returns\n        -------\n        AnalysisPlan\n            An instance of AnalysisPlan\n        \"\"\"\n        config = AnalysisPlanConfig(**d)\n        return cls.from_config(config)\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.control_variant","title":"<code>control_variant</code>  <code>property</code>","text":"<p>Returns the control variant from the list of variants. Raises an error if no control variant is found.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.control_variant--returns","title":"Returns","text":"<p>Variant     The control variant</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.control_variant--raises","title":"Raises","text":"<p>ValueError     If no control variant is found</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.treatment_variants","title":"<code>treatment_variants</code>  <code>property</code>","text":"<p>Returns the treatment variants from the list of variants. Raises an error if no treatment variants are found.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.treatment_variants--returns","title":"Returns","text":"<p>List[Variant]     A list of treatment variants</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.treatment_variants--raises","title":"Raises","text":"<p>ValueError     If no treatment variants are found</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.__init__","title":"<code>__init__(tests, variants, variant_col='treatment', alpha=0.05)</code>","text":""},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.__init__--parameters","title":"Parameters","text":"<p>tests : List[HypothesisTest]     A list of HypothesisTest instances variants : List[Variant]     A list of Variant instances variant_col : str     The name of the column containing the variant names. alpha : float     significance level used to construct confidence intervals</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>def __init__(\n    self,\n    tests: List[HypothesisTest],\n    variants: List[Variant],\n    variant_col: str = \"treatment\",\n    alpha: float = 0.05,\n):\n    \"\"\"\n    Parameters\n    ----------\n    tests : List[HypothesisTest]\n        A list of HypothesisTest instances\n    variants : List[Variant]\n        A list of Variant instances\n    variant_col : str\n        The name of the column containing the variant names.\n    alpha : float\n        significance level used to construct confidence intervals\n    \"\"\"\n\n    self.tests = tests\n    self.variants = variants\n    self.variant_col = variant_col\n    self.alpha = alpha\n\n    self._validate_inputs()\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan._validate_data","title":"<code>_validate_data(exp_data, pre_exp_data=None)</code>","text":"<p>Validates the input dataframes for the analyze method.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan._validate_data--parameters","title":"Parameters","text":"<p>exp_data : pd.DataFrame     The experimental data pre_exp_data : Optional[pd.DataFrame]     The pre-experimental data (optional)</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan._validate_data--raises","title":"Raises","text":"<p>ValueError     If exp_data is not a DataFrame or is empty     If pre_exp_data is provided and is not a DataFrame or is empty</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>def _validate_data(\n    self, exp_data: pd.DataFrame, pre_exp_data: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    Validates the input dataframes for the analyze method.\n\n    Parameters\n    ----------\n    exp_data : pd.DataFrame\n        The experimental data\n    pre_exp_data : Optional[pd.DataFrame]\n        The pre-experimental data (optional)\n\n    Raises\n    ------\n    ValueError\n        If exp_data is not a DataFrame or is empty\n        If pre_exp_data is provided and is not a DataFrame or is empty\n    \"\"\"\n    if not isinstance(exp_data, pd.DataFrame):\n        raise ValueError(\"exp_data must be a pandas DataFrame\")\n    if exp_data.empty:\n        raise ValueError(\"exp_data cannot be empty\")\n    if pre_exp_data is not None:\n        if not isinstance(pre_exp_data, pd.DataFrame):\n            raise ValueError(\"pre_exp_data must be a pandas DataFrame if provided\")\n        if pre_exp_data.empty:\n            raise ValueError(\"pre_exp_data cannot be empty if provided\")\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan._validate_inputs","title":"<code>_validate_inputs()</code>","text":"<p>Validates the inputs for the AnalysisPlan class.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan._validate_inputs--raises","title":"Raises","text":"<p>TypeError     If tests is not a list of HypothesisTest instances or if variants is not a list of Variant instances. ValueError     If tests or variants are empty lists.</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>def _validate_inputs(self):\n    \"\"\"\n    Validates the inputs for the AnalysisPlan class.\n\n    Raises\n    ------\n    TypeError\n        If tests is not a list of HypothesisTest instances or if variants is not a list of Variant instances.\n    ValueError\n        If tests or variants are empty lists.\n    \"\"\"\n    if not isinstance(self.tests, list) or not all(\n        isinstance(test, HypothesisTest) for test in self.tests\n    ):\n        raise TypeError(\"Tests must be a list of HypothesisTest instances\")\n    if not isinstance(self.variants, list) or not all(\n        isinstance(variant, Variant) for variant in self.variants\n    ):\n        raise TypeError(\"Variants must be a list of Variant instances\")\n    if not isinstance(self.variant_col, str):\n        raise TypeError(\"Variant_col must be a string\")\n    if not self.tests:\n        raise ValueError(\"Tests list cannot be empty\")\n    if not self.variants:\n        raise ValueError(\"Variants list cannot be empty\")\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.analyze","title":"<code>analyze(exp_data, pre_exp_data=None, verbose=False)</code>","text":"<p>Method to run the experiment analysis.</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>def analyze(\n    self,\n    exp_data: pd.DataFrame,\n    pre_exp_data: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n) -&gt; AnalysisPlanResults:\n    \"\"\"\n    Method to run the experiment analysis.\n    \"\"\"\n\n    # Validate input data at the beginning\n    self._validate_data(exp_data, pre_exp_data)\n\n    analysis_results = AnalysisPlanResults()\n\n    for test in self.tests:\n        exp_data = test.add_covariates(exp_data, pre_exp_data)\n\n        for treatment_variant in self.treatment_variants:\n            for dimension in test.dimensions:\n                for dimension_value in dimension.iterate_dimension_values():\n\n                    if verbose:\n                        logger.info(\n                            f\"Metric: {test.metric.alias}, \"\n                            f\"Treatment: {treatment_variant.name}, \"\n                            f\"Dimension: {dimension.name}, \"\n                            f\"Value: {dimension_value}\"\n                        )\n\n                    test_results = test.get_test_results(\n                        exp_data=exp_data,\n                        control_variant=self.control_variant,\n                        treatment_variant=treatment_variant,\n                        variant_col=self.variant_col,\n                        dimension=dimension,\n                        dimension_value=dimension_value,\n                        alpha=self.alpha,\n                    )\n\n                    analysis_results = analysis_results + test_results\n\n    return analysis_results\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates an AnalysisPlan instance from a configuration object.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_config--parameters","title":"Parameters","text":"<p>config : AnalysisPlanConfig     An instance of AnalysisPlanConfig</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_config--returns","title":"Returns","text":"<p>AnalysisPlan     An instance of AnalysisPlan</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>@classmethod\ndef from_config(cls, config: AnalysisPlanConfig) -&gt; \"AnalysisPlan\":\n    \"\"\"\n    Creates an AnalysisPlan instance from a configuration object.\n\n    Parameters\n    ----------\n    config : AnalysisPlanConfig\n        An instance of AnalysisPlanConfig\n\n    Returns\n    -------\n    AnalysisPlan\n        An instance of AnalysisPlan\n    \"\"\"\n    tests = [HypothesisTest.from_config(test) for test in config.tests]\n    variants = [\n        Variant.from_metrics_config(variant_config)\n        for variant_config in config.variants\n    ]\n    return cls(\n        tests=tests,\n        variants=variants,\n        variant_col=config.variant_col,\n        alpha=config.alpha,\n    )\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Creates an AnalysisPlan instance from a dictionary.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_dict--parameters","title":"Parameters","text":"<p>d : Dict[str, Any]     A dictionary containing the analysis plan configuration</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_dict--returns","title":"Returns","text":"<p>AnalysisPlan     An instance of AnalysisPlan</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Dict[str, Any]) -&gt; \"AnalysisPlan\":\n    \"\"\"\n    Creates an AnalysisPlan instance from a dictionary.\n\n    Parameters\n    ----------\n    d : Dict[str, Any]\n        A dictionary containing the analysis plan configuration\n\n    Returns\n    -------\n    AnalysisPlan\n        An instance of AnalysisPlan\n    \"\"\"\n    config = AnalysisPlanConfig(**d)\n    return cls.from_config(config)\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics","title":"<code>from_metrics(metrics, variants, variant_col='treatment', alpha=0.05, dimensions=None, analysis_type='default', analysis_config=None, custom_analysis_type_mapper=None)</code>  <code>classmethod</code>","text":"<p>Creates a simplified AnalysisPlan instance from a list of metrics. It will create HypothesisTest objects under the hood. This shortcut does not support cupac, and uses the same dimensions, analysis type and analysis config for all metrics.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics--parameters","title":"Parameters","text":"<p>metrics : List[Metric]     A list of Metric instances variants : List[Variant]     A list of Variant instances variant_col : str     The name of the column containing the variant names. alpha : float     Significance level used to construct confidence intervals dimensions : Optional[List[Dimension]]     A list of Dimension instances (optional) analysis_type : str     The type of analysis to be conducted (default: \"default\") analysis_config : Optional[Dict[str, Any]]     A dictionary containing analysis configuration options (optional) custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]     An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics--returns","title":"Returns","text":"<p>AnalysisPlan     An instance of AnalysisPlan</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>@classmethod\ndef from_metrics(\n    cls,\n    metrics: List[Metric],\n    variants: List[Variant],\n    variant_col: str = \"treatment\",\n    alpha: float = 0.05,\n    dimensions: Optional[List[Dimension]] = None,\n    analysis_type: str = \"default\",\n    analysis_config: Optional[Dict[str, Any]] = None,\n    custom_analysis_type_mapper: Optional[Dict[str, ExperimentAnalysis]] = None,\n) -&gt; \"AnalysisPlan\":\n    \"\"\"\n    Creates a simplified AnalysisPlan instance from a list of metrics. It will create HypothesisTest objects under the hood.\n    This shortcut does not support cupac, and uses the same dimensions, analysis type and analysis config for all metrics.\n\n    Parameters\n    ----------\n    metrics : List[Metric]\n        A list of Metric instances\n    variants : List[Variant]\n        A list of Variant instances\n    variant_col : str\n        The name of the column containing the variant names.\n    alpha : float\n        Significance level used to construct confidence intervals\n    dimensions : Optional[List[Dimension]]\n        A list of Dimension instances (optional)\n    analysis_type : str\n        The type of analysis to be conducted (default: \"default\")\n    analysis_config : Optional[Dict[str, Any]]\n        A dictionary containing analysis configuration options (optional)\n    custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]\n        An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n\n    Returns\n    -------\n    AnalysisPlan\n        An instance of AnalysisPlan\n    \"\"\"\n    tests = [\n        HypothesisTest(\n            metric=metric,\n            dimensions=dimensions or [],\n            analysis_type=analysis_type,\n            analysis_config=analysis_config or {},\n            custom_analysis_type_mapper=custom_analysis_type_mapper or {},\n        )\n        for metric in metrics\n    ]\n\n    return cls(\n        tests=tests,\n        variants=variants,\n        variant_col=variant_col,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics_config","title":"<code>from_metrics_config(config)</code>  <code>classmethod</code>","text":"<p>Creates an AnalysisPlan instance from a metrics-based configuration object.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics_config--parameters","title":"Parameters","text":"<p>config : AnalysisPlanMetricsConfig     An instance of AnalysisPlanMetricsConfig</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics_config--returns","title":"Returns","text":"<p>AnalysisPlan     An instance of AnalysisPlan</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>@classmethod\ndef from_metrics_config(cls, config: AnalysisPlanMetricsConfig) -&gt; \"AnalysisPlan\":\n    \"\"\"\n    Creates an AnalysisPlan instance from a metrics-based configuration object.\n\n    Parameters\n    ----------\n    config : AnalysisPlanMetricsConfig\n        An instance of AnalysisPlanMetricsConfig\n\n    Returns\n    -------\n    AnalysisPlan\n        An instance of AnalysisPlan\n    \"\"\"\n    metrics = [\n        Metric.from_metrics_config(metric_config)\n        for metric_config in config.metrics\n    ]\n    variants = [\n        Variant.from_metrics_config(variant_config)\n        for variant_config in config.variants\n    ]\n    dimensions = [\n        Dimension.from_metrics_config(dimension_config)\n        for dimension_config in config.dimensions\n    ]\n    return cls.from_metrics(\n        metrics=metrics,\n        variants=variants,\n        variant_col=config.variant_col,\n        alpha=config.alpha,\n        dimensions=dimensions,\n        analysis_type=config.analysis_type,\n        analysis_config=config.analysis_config,\n        custom_analysis_type_mapper=config.custom_analysis_type_mapper,\n    )\n</code></pre>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics_dict","title":"<code>from_metrics_dict(d)</code>  <code>classmethod</code>","text":"<p>Creates an AnalysisPlan instance from a metrics-based dictionary.</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics_dict--parameters","title":"Parameters","text":"<p>d : Dict[str, Any]     A dictionary containing the analysis plan configuration, parametrised via metrics</p>"},{"location":"api/analysis_plan.html#cluster_experiments.inference.analysis_plan.AnalysisPlan.from_metrics_dict--returns","title":"Returns","text":"<p>AnalysisPlan     An instance of AnalysisPlan</p> Source code in <code>cluster_experiments/inference/analysis_plan.py</code> <pre><code>@classmethod\ndef from_metrics_dict(cls, d: Dict[str, Any]) -&gt; \"AnalysisPlan\":\n    \"\"\"\n    Creates an AnalysisPlan instance from a metrics-based dictionary.\n\n    Parameters\n    ----------\n    d : Dict[str, Any]\n        A dictionary containing the analysis plan configuration, parametrised via metrics\n\n    Returns\n    -------\n    AnalysisPlan\n        An instance of AnalysisPlan\n    \"\"\"\n    config = AnalysisPlanMetricsConfig(**d)\n    return cls.from_metrics_config(config)\n</code></pre>"},{"location":"api/analysis_results.html","title":"<code>from cluster_experiments.inference.analysis_results import *</code>","text":""},{"location":"api/analysis_results.html#cluster_experiments.inference.analysis_results.AnalysisPlanResults","title":"<code>AnalysisPlanResults</code>  <code>dataclass</code>","text":"<p>A dataclass used to represent the results of the experiment analysis.</p>"},{"location":"api/analysis_results.html#cluster_experiments.inference.analysis_results.AnalysisPlanResults--attributes","title":"Attributes","text":"<p>metric_alias : List[str]     The alias of the metric used in the test control_variant_name : List[str]     The name of the control variant treatment_variant_name : List[str]     The name of the treatment variant control_variant_mean : List[float]     The mean value of the control variant treatment_variant_mean : List[float]     The mean value of the treatment variant analysis_type : List[str]     The type of analysis performed ate : List[float]     The average treatment effect ate_ci_lower : List[float]     The lower bound of the confidence interval for the ATE ate_ci_upper : List[float]     The upper bound of the confidence interval for the ATE p_value : List[float]     The p-value of the test std_error : List[float]     The standard error of the test dimension_name : List[str]     The name of the dimension dimension_value : List[str]     The value of the dimension alpha: List[float]     The significance level of the test</p> Source code in <code>cluster_experiments/inference/analysis_results.py</code> <pre><code>@dataclass\nclass AnalysisPlanResults:\n    \"\"\"\n    A dataclass used to represent the results of the experiment analysis.\n\n    Attributes\n    ----------\n    metric_alias : List[str]\n        The alias of the metric used in the test\n    control_variant_name : List[str]\n        The name of the control variant\n    treatment_variant_name : List[str]\n        The name of the treatment variant\n    control_variant_mean : List[float]\n        The mean value of the control variant\n    treatment_variant_mean : List[float]\n        The mean value of the treatment variant\n    analysis_type : List[str]\n        The type of analysis performed\n    ate : List[float]\n        The average treatment effect\n    ate_ci_lower : List[float]\n        The lower bound of the confidence interval for the ATE\n    ate_ci_upper : List[float]\n        The upper bound of the confidence interval for the ATE\n    p_value : List[float]\n        The p-value of the test\n    std_error : List[float]\n        The standard error of the test\n    dimension_name : List[str]\n        The name of the dimension\n    dimension_value : List[str]\n        The value of the dimension\n    alpha: List[float]\n        The significance level of the test\n    \"\"\"\n\n    metric_alias: List[str] = field(default_factory=lambda: [])\n    control_variant_name: List[str] = field(default_factory=lambda: [])\n    treatment_variant_name: List[str] = field(default_factory=lambda: [])\n    control_variant_mean: List[float] = field(default_factory=lambda: [])\n    treatment_variant_mean: List[float] = field(default_factory=lambda: [])\n    analysis_type: List[str] = field(default_factory=lambda: [])\n    ate: List[float] = field(default_factory=lambda: [])\n    ate_ci_lower: List[float] = field(default_factory=lambda: [])\n    ate_ci_upper: List[float] = field(default_factory=lambda: [])\n    p_value: List[float] = field(default_factory=lambda: [])\n    std_error: List[float] = field(default_factory=lambda: [])\n    dimension_name: List[str] = field(default_factory=lambda: [])\n    dimension_value: List[str] = field(default_factory=lambda: [])\n    alpha: List[float] = field(default_factory=lambda: [])\n\n    def __add__(self, other):\n        if not isinstance(other, AnalysisPlanResults):\n            return NotImplemented\n\n        return AnalysisPlanResults(\n            metric_alias=self.metric_alias + other.metric_alias,\n            control_variant_name=self.control_variant_name + other.control_variant_name,\n            treatment_variant_name=self.treatment_variant_name\n            + other.treatment_variant_name,\n            control_variant_mean=self.control_variant_mean + other.control_variant_mean,\n            treatment_variant_mean=self.treatment_variant_mean\n            + other.treatment_variant_mean,\n            analysis_type=self.analysis_type + other.analysis_type,\n            ate=self.ate + other.ate,\n            ate_ci_lower=self.ate_ci_lower + other.ate_ci_lower,\n            ate_ci_upper=self.ate_ci_upper + other.ate_ci_upper,\n            p_value=self.p_value + other.p_value,\n            std_error=self.std_error + other.std_error,\n            dimension_name=self.dimension_name + other.dimension_name,\n            dimension_value=self.dimension_value + other.dimension_value,\n            alpha=self.alpha + other.alpha,\n        )\n\n    def to_dataframe(self):\n        return pd.DataFrame(asdict(self))\n</code></pre>"},{"location":"api/cupac_model.html","title":"<code>from cluster_experiments.cupac import *</code>","text":""},{"location":"api/cupac_model.html#cluster_experiments.cupac.CupacHandler","title":"<code>CupacHandler</code>","text":"<p>CupacHandler class. It handles operations related to the cupac model.</p> <p>Its main goal is to call the add_covariates method, where it will add the ouptut from the cupac model, and this should be used as covariates in the regression method for the hypothesis test.</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>class CupacHandler:\n    \"\"\"\n    CupacHandler class. It handles operations related to the cupac model.\n\n    Its main goal is to call the add_covariates method, where it will add the ouptut from the cupac model,\n    and this should be used as covariates in the regression method for the hypothesis test.\n    \"\"\"\n\n    def __init__(\n        self,\n        cupac_model: Optional[BaseEstimator] = None,\n        target_col: str = \"target\",\n        scale_col: Optional[str] = None,\n        features_cupac_model: Optional[List[str]] = None,\n        cache_fit: bool = True,\n    ):\n        self.cupac_model: BaseEstimator = cupac_model or EmptyRegressor()\n        self.target_col = target_col\n        # TODO: implement CUPAC with both target_col and scale_col,\n        # right now it only supports target_col for delta method\n        self.scale_col = scale_col\n        self.cupac_outcome_name = f\"estimate_{target_col}\"\n        self.features_cupac_model: List[str] = features_cupac_model or []\n        self.is_cupac = not isinstance(self.cupac_model, EmptyRegressor)\n        self.cache_fit = cache_fit\n\n        self.check_cupac_config()\n\n    def get_pre_experiment_y(self, pre_experiment_df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Returns the pre-experiment target variable, scaled if scale_col is provided.\"\"\"\n        if self.scale_col is not None:\n            return (\n                pre_experiment_df[self.target_col] / pre_experiment_df[self.scale_col]\n            )\n        return pre_experiment_df[self.target_col]\n\n    def _prep_data_cupac(\n        self, df: pd.DataFrame, pre_experiment_df: pd.DataFrame\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n        \"\"\"Prepares data for training and prediction\"\"\"\n        df = df.copy()\n        pre_experiment_df = pre_experiment_df.copy()\n        df_predict = df.drop(columns=[self.target_col])\n        # Split data into X and y\n        pre_experiment_x = pre_experiment_df.drop(columns=[self.target_col])\n        pre_experiment_y = self.get_pre_experiment_y(pre_experiment_df)\n\n        # Keep only cupac features\n        if self.features_cupac_model:\n            pre_experiment_x = pre_experiment_x[self.features_cupac_model]\n            df_predict = df_predict[self.features_cupac_model]\n\n        return df_predict, pre_experiment_x, pre_experiment_y\n\n    def add_covariates(\n        self, df: pd.DataFrame, pre_experiment_df: Optional[pd.DataFrame] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Train model to predict outcome variable (based on pre-experiment data)\n        and  add the prediction to the experiment dataframe. Only do this if\n        we use cupac\n        Args:\n            pre_experiment_df: Dataframe with pre-experiment data.\n            df: Dataframe with outcome and treatment variables.\n        \"\"\"\n        self.check_cupac_inputs(pre_experiment_df)\n\n        # Early return if no need to add covariates\n        if not self.need_covariates(pre_experiment_df):\n            return df\n\n        df = df.copy()\n        pre_experiment_df = pre_experiment_df.copy()\n        df_predict, pre_experiment_x, pre_experiment_y = self._prep_data_cupac(\n            df=df, pre_experiment_df=pre_experiment_df\n        )\n\n        # Fit model if it has not been fitted before\n        self._fit_cupac_model(pre_experiment_x, pre_experiment_y)\n\n        # Predict\n        estimated_target = self._predict_cupac_model(df_predict)\n\n        # Add cupac outcome name to df\n        df[self.cupac_outcome_name] = estimated_target\n        return df\n\n    def _fit_cupac_model(\n        self, pre_experiment_x: pd.DataFrame, pre_experiment_y: pd.Series\n    ):\n        \"\"\"Fits the cupac model.\n        Caches the fitted model in the object, so we only fit it once.\n        We can disable this by setting cache_fit to False.\n        \"\"\"\n        if not self.cache_fit:\n            self.cupac_model.fit(pre_experiment_x, pre_experiment_y)\n            return\n\n        try:\n            check_is_fitted(self.cupac_model)\n        except NotFittedError:\n            self.cupac_model.fit(pre_experiment_x, pre_experiment_y)\n\n    def _predict_cupac_model(self, df_predict: pd.DataFrame) -&gt; ArrayLike:\n        \"\"\"Predicts the cupac model\"\"\"\n        if hasattr(self.cupac_model, \"predict_proba\"):\n            return self.cupac_model.predict_proba(df_predict)[:, 1]\n        if hasattr(self.cupac_model, \"predict\"):\n            return self.cupac_model.predict(df_predict)\n        raise ValueError(\"cupac_model should have predict or predict_proba method.\")\n\n    def need_covariates(self, pre_experiment_df: Optional[pd.DataFrame] = None) -&gt; bool:\n        return pre_experiment_df is not None and self.is_cupac\n\n    def check_cupac_inputs(self, pre_experiment_df: Optional[pd.DataFrame] = None):\n        if self.is_cupac and pre_experiment_df is None:\n            raise ValueError(\"If cupac is used, pre_experiment_df should be provided.\")\n\n        if not self.is_cupac and pre_experiment_df is not None:\n            raise ValueError(\n                \"If cupac is not used, pre_experiment_df should not be provided - remove pre_experiment_df argument or set cupac_model to not None.\"\n            )\n\n    def check_cupac_config(self):\n        if self.is_cupac and self.target_col in self.features_cupac_model:\n            raise ValueError(\n                \"If cupac is used, target_col should not be in features_cupac_model.\"\n            )\n        if self.is_cupac and self.scale_col in self.features_cupac_model:\n            raise ValueError(\n                \"If cupac is used, scale_col should not be in features_cupac_model.\"\n            )\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.CupacHandler._fit_cupac_model","title":"<code>_fit_cupac_model(pre_experiment_x, pre_experiment_y)</code>","text":"<p>Fits the cupac model. Caches the fitted model in the object, so we only fit it once. We can disable this by setting cache_fit to False.</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def _fit_cupac_model(\n    self, pre_experiment_x: pd.DataFrame, pre_experiment_y: pd.Series\n):\n    \"\"\"Fits the cupac model.\n    Caches the fitted model in the object, so we only fit it once.\n    We can disable this by setting cache_fit to False.\n    \"\"\"\n    if not self.cache_fit:\n        self.cupac_model.fit(pre_experiment_x, pre_experiment_y)\n        return\n\n    try:\n        check_is_fitted(self.cupac_model)\n    except NotFittedError:\n        self.cupac_model.fit(pre_experiment_x, pre_experiment_y)\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.CupacHandler._predict_cupac_model","title":"<code>_predict_cupac_model(df_predict)</code>","text":"<p>Predicts the cupac model</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def _predict_cupac_model(self, df_predict: pd.DataFrame) -&gt; ArrayLike:\n    \"\"\"Predicts the cupac model\"\"\"\n    if hasattr(self.cupac_model, \"predict_proba\"):\n        return self.cupac_model.predict_proba(df_predict)[:, 1]\n    if hasattr(self.cupac_model, \"predict\"):\n        return self.cupac_model.predict(df_predict)\n    raise ValueError(\"cupac_model should have predict or predict_proba method.\")\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.CupacHandler._prep_data_cupac","title":"<code>_prep_data_cupac(df, pre_experiment_df)</code>","text":"<p>Prepares data for training and prediction</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def _prep_data_cupac(\n    self, df: pd.DataFrame, pre_experiment_df: pd.DataFrame\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n    \"\"\"Prepares data for training and prediction\"\"\"\n    df = df.copy()\n    pre_experiment_df = pre_experiment_df.copy()\n    df_predict = df.drop(columns=[self.target_col])\n    # Split data into X and y\n    pre_experiment_x = pre_experiment_df.drop(columns=[self.target_col])\n    pre_experiment_y = self.get_pre_experiment_y(pre_experiment_df)\n\n    # Keep only cupac features\n    if self.features_cupac_model:\n        pre_experiment_x = pre_experiment_x[self.features_cupac_model]\n        df_predict = df_predict[self.features_cupac_model]\n\n    return df_predict, pre_experiment_x, pre_experiment_y\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.CupacHandler.add_covariates","title":"<code>add_covariates(df, pre_experiment_df=None)</code>","text":"<p>Train model to predict outcome variable (based on pre-experiment data) and  add the prediction to the experiment dataframe. Only do this if we use cupac Args:     pre_experiment_df: Dataframe with pre-experiment data.     df: Dataframe with outcome and treatment variables.</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def add_covariates(\n    self, df: pd.DataFrame, pre_experiment_df: Optional[pd.DataFrame] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Train model to predict outcome variable (based on pre-experiment data)\n    and  add the prediction to the experiment dataframe. Only do this if\n    we use cupac\n    Args:\n        pre_experiment_df: Dataframe with pre-experiment data.\n        df: Dataframe with outcome and treatment variables.\n    \"\"\"\n    self.check_cupac_inputs(pre_experiment_df)\n\n    # Early return if no need to add covariates\n    if not self.need_covariates(pre_experiment_df):\n        return df\n\n    df = df.copy()\n    pre_experiment_df = pre_experiment_df.copy()\n    df_predict, pre_experiment_x, pre_experiment_y = self._prep_data_cupac(\n        df=df, pre_experiment_df=pre_experiment_df\n    )\n\n    # Fit model if it has not been fitted before\n    self._fit_cupac_model(pre_experiment_x, pre_experiment_y)\n\n    # Predict\n    estimated_target = self._predict_cupac_model(df_predict)\n\n    # Add cupac outcome name to df\n    df[self.cupac_outcome_name] = estimated_target\n    return df\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.CupacHandler.get_pre_experiment_y","title":"<code>get_pre_experiment_y(pre_experiment_df)</code>","text":"<p>Returns the pre-experiment target variable, scaled if scale_col is provided.</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def get_pre_experiment_y(self, pre_experiment_df: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Returns the pre-experiment target variable, scaled if scale_col is provided.\"\"\"\n    if self.scale_col is not None:\n        return (\n            pre_experiment_df[self.target_col] / pre_experiment_df[self.scale_col]\n        )\n    return pre_experiment_df[self.target_col]\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.EmptyRegressor","title":"<code>EmptyRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Empty regressor class. It does not do anything, used to glue the code of other estimators and PowerAnalysis</p> <p>Each Regressor should have: - fit method: Uses pre experiment data to fit some kind of model to be used as a covariate and reduce variance. - predict method: Uses the fitted model to add the covariate on the experiment data.</p> <p>It can add aggregates of the target in older data as a covariate, or a model (cupac) to predict the target.</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>class EmptyRegressor(BaseEstimator):\n    \"\"\"\n    Empty regressor class. It does not do anything, used to glue the code of other estimators and PowerAnalysis\n\n    Each Regressor should have:\n    - fit method: Uses pre experiment data to fit some kind of model to be used as a covariate and reduce variance.\n    - predict method: Uses the fitted model to add the covariate on the experiment data.\n\n    It can add aggregates of the target in older data as a covariate, or a model (cupac) to predict the target.\n    \"\"\"\n\n    @classmethod\n    def from_config(cls, config):\n        return cls()\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation","title":"<code>TargetAggregation</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Adds average of target using pre-experiment data</p> <p>Parameters:</p> Name Type Description Default <code>agg_col</code> <code>str</code> <p>Column to group by to aggregate target</p> required <code>target_col</code> <code>str</code> <p>Column to aggregate</p> <code>'target'</code> <code>smoothing_factor</code> <code>int</code> <p>Smoothing factor for the smoothed mean</p> <code>20</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.cupac import TargetAggregation\n\ndf = pd.DataFrame({\"agg_col\": [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"], \"target_col\": [1, 2, 3, 4, 5, 6]})\nnew_df = pd.DataFrame({\"agg_col\": [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"]})\ntarget_agg = TargetAggregation(\"agg_col\", \"target_col\")\ntarget_agg.fit(df.drop(columns=\"target_col\"), df[\"target_col\"])\ndf_with_target_agg = target_agg.predict(new_df)\nprint(df_with_target_agg)\n</code></pre></p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>class TargetAggregation(BaseEstimator):\n    \"\"\"\n    Adds average of target using pre-experiment data\n\n    Args:\n        agg_col: Column to group by to aggregate target\n        target_col: Column to aggregate\n        smoothing_factor: Smoothing factor for the smoothed mean\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.cupac import TargetAggregation\n\n    df = pd.DataFrame({\"agg_col\": [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"], \"target_col\": [1, 2, 3, 4, 5, 6]})\n    new_df = pd.DataFrame({\"agg_col\": [\"a\", \"a\", \"b\", \"b\", \"c\", \"c\"]})\n    target_agg = TargetAggregation(\"agg_col\", \"target_col\")\n    target_agg.fit(df.drop(columns=\"target_col\"), df[\"target_col\"])\n    df_with_target_agg = target_agg.predict(new_df)\n    print(df_with_target_agg)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        agg_col: str,\n        target_col: str = \"target\",\n        smoothing_factor: int = 20,\n    ):\n        self.agg_col = agg_col\n        self.target_col = target_col\n        self.smoothing_factor = smoothing_factor\n        self.is_empty = False\n        self.mean_target_col = f\"{self.target_col}_mean\"\n        self.smooth_mean_target_col = f\"{self.target_col}_smooth_mean\"\n        self.pre_experiment_agg_df = pd.DataFrame()\n\n    def _get_pre_experiment_mean(self, pre_experiment_df: pd.DataFrame) -&gt; float:\n        return pre_experiment_df[self.target_col].mean()\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; \"TargetAggregation\":\n        \"\"\"Fits \"target encoder\" model to pre-experiment data\"\"\"\n        pre_experiment_df = X.copy()\n        pre_experiment_df[self.target_col] = y\n\n        self.pre_experiment_mean = self._get_pre_experiment_mean(pre_experiment_df)\n        self.pre_experiment_agg_df = (\n            pre_experiment_df.assign(count=1)\n            .groupby(self.agg_col, as_index=False)\n            .agg({self.target_col: \"sum\", \"count\": \"sum\"})\n            .assign(\n                **{\n                    self.mean_target_col: lambda x: x[self.target_col] / x[\"count\"],\n                    self.smooth_mean_target_col: lambda x: (\n                        x[self.target_col]\n                        + self.smoothing_factor * self.pre_experiment_mean\n                    )\n                    / (x[\"count\"] + self.smoothing_factor),\n                }\n            )\n            .drop(columns=[\"count\", self.target_col])\n        )\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; ArrayLike:\n        \"\"\"Adds average target of pre-experiment data to experiment data\"\"\"\n        return (\n            X.merge(self.pre_experiment_agg_df, how=\"left\", on=self.agg_col)[\n                self.smooth_mean_target_col\n            ]\n            .fillna(self.pre_experiment_mean)\n            .values\n        )\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates TargetAggregation from PowerConfig\"\"\"\n        return cls(\n            agg_col=config.agg_col,\n            target_col=config.target_col,\n            smoothing_factor=config.smoothing_factor,\n        )\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation.fit","title":"<code>fit(X, y)</code>","text":"<p>Fits \"target encoder\" model to pre-experiment data</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; \"TargetAggregation\":\n    \"\"\"Fits \"target encoder\" model to pre-experiment data\"\"\"\n    pre_experiment_df = X.copy()\n    pre_experiment_df[self.target_col] = y\n\n    self.pre_experiment_mean = self._get_pre_experiment_mean(pre_experiment_df)\n    self.pre_experiment_agg_df = (\n        pre_experiment_df.assign(count=1)\n        .groupby(self.agg_col, as_index=False)\n        .agg({self.target_col: \"sum\", \"count\": \"sum\"})\n        .assign(\n            **{\n                self.mean_target_col: lambda x: x[self.target_col] / x[\"count\"],\n                self.smooth_mean_target_col: lambda x: (\n                    x[self.target_col]\n                    + self.smoothing_factor * self.pre_experiment_mean\n                )\n                / (x[\"count\"] + self.smoothing_factor),\n            }\n        )\n        .drop(columns=[\"count\", self.target_col])\n    )\n    return self\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates TargetAggregation from PowerConfig</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates TargetAggregation from PowerConfig\"\"\"\n    return cls(\n        agg_col=config.agg_col,\n        target_col=config.target_col,\n        smoothing_factor=config.smoothing_factor,\n    )\n</code></pre>"},{"location":"api/cupac_model.html#cluster_experiments.cupac.TargetAggregation.predict","title":"<code>predict(X)</code>","text":"<p>Adds average target of pre-experiment data to experiment data</p> Source code in <code>cluster_experiments/cupac.py</code> <pre><code>def predict(self, X: pd.DataFrame) -&gt; ArrayLike:\n    \"\"\"Adds average target of pre-experiment data to experiment data\"\"\"\n    return (\n        X.merge(self.pre_experiment_agg_df, how=\"left\", on=self.agg_col)[\n            self.smooth_mean_target_col\n        ]\n        .fillna(self.pre_experiment_mean)\n        .values\n    )\n</code></pre>"},{"location":"api/dimension.html","title":"<code>from cluster_experiments.inference.dimension import *</code>","text":""},{"location":"api/dimension.html#cluster_experiments.inference.dimension.DefaultDimension","title":"<code>DefaultDimension</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dimension</code></p> <p>A class used to represent a Dimension with a default value representing total, i.e. no slicing.</p> Source code in <code>cluster_experiments/inference/dimension.py</code> <pre><code>@dataclass\nclass DefaultDimension(Dimension):\n    \"\"\"\n    A class used to represent a Dimension with a default value representing total, i.e. no slicing.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(name=\"__total_dimension\", values=[\"total\"])\n</code></pre>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension","title":"<code>Dimension</code>  <code>dataclass</code>","text":"<p>A class used to represent a Dimension with a name and values.</p>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension--attributes","title":"Attributes","text":"<p>name : str     The name of the dimension values : List[str]     A list of strings representing the possible values of the dimension</p> Source code in <code>cluster_experiments/inference/dimension.py</code> <pre><code>@dataclass\nclass Dimension:\n    \"\"\"\n    A class used to represent a Dimension with a name and values.\n\n    Attributes\n    ----------\n    name : str\n        The name of the dimension\n    values : List[str]\n        A list of strings representing the possible values of the dimension\n    \"\"\"\n\n    name: str\n    values: List[str]\n\n    def __post_init__(self):\n        \"\"\"\n        Validates the inputs after initialization.\n        \"\"\"\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"\n        Validates the inputs for the Dimension class.\n\n        Raises\n        ------\n        TypeError\n            If the name is not a string or if values is not a list of strings.\n        \"\"\"\n        if not isinstance(self.name, str):\n            raise TypeError(\"Dimension name must be a string\")\n        if not isinstance(self.values, list) or not all(\n            isinstance(val, str) for val in self.values\n        ):\n            raise TypeError(\"Dimension values must be a list of strings\")\n\n    def iterate_dimension_values(self):\n        \"\"\"\n        A generator method to yield name and values from the dimension.\n\n        Yields\n        ------\n        Any\n            A unique value from the dimension.\n        \"\"\"\n        seen = set()\n        for value in self.values:\n            if value not in seen:\n                seen.add(value)\n                yield value\n\n    @classmethod\n    def from_metrics_config(cls, config: dict) -&gt; \"Dimension\":\n        \"\"\"\n        Creates a Dimension object from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : dict\n            A dictionary containing the configuration for the Dimension\n\n        Returns\n        -------\n        Dimension\n            A Dimension object\n        \"\"\"\n        return cls(name=config[\"name\"], values=config[\"values\"])\n</code></pre>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validates the inputs after initialization.</p> Source code in <code>cluster_experiments/inference/dimension.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Validates the inputs after initialization.\n    \"\"\"\n    self._validate_inputs()\n</code></pre>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension._validate_inputs","title":"<code>_validate_inputs()</code>","text":"<p>Validates the inputs for the Dimension class.</p>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension._validate_inputs--raises","title":"Raises","text":"<p>TypeError     If the name is not a string or if values is not a list of strings.</p> Source code in <code>cluster_experiments/inference/dimension.py</code> <pre><code>def _validate_inputs(self):\n    \"\"\"\n    Validates the inputs for the Dimension class.\n\n    Raises\n    ------\n    TypeError\n        If the name is not a string or if values is not a list of strings.\n    \"\"\"\n    if not isinstance(self.name, str):\n        raise TypeError(\"Dimension name must be a string\")\n    if not isinstance(self.values, list) or not all(\n        isinstance(val, str) for val in self.values\n    ):\n        raise TypeError(\"Dimension values must be a list of strings\")\n</code></pre>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension.from_metrics_config","title":"<code>from_metrics_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a Dimension object from a configuration dictionary.</p>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension.from_metrics_config--parameters","title":"Parameters","text":"<p>config : dict     A dictionary containing the configuration for the Dimension</p>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension.from_metrics_config--returns","title":"Returns","text":"<p>Dimension     A Dimension object</p> Source code in <code>cluster_experiments/inference/dimension.py</code> <pre><code>@classmethod\ndef from_metrics_config(cls, config: dict) -&gt; \"Dimension\":\n    \"\"\"\n    Creates a Dimension object from a configuration dictionary.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing the configuration for the Dimension\n\n    Returns\n    -------\n    Dimension\n        A Dimension object\n    \"\"\"\n    return cls(name=config[\"name\"], values=config[\"values\"])\n</code></pre>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension.iterate_dimension_values","title":"<code>iterate_dimension_values()</code>","text":"<p>A generator method to yield name and values from the dimension.</p>"},{"location":"api/dimension.html#cluster_experiments.inference.dimension.Dimension.iterate_dimension_values--yields","title":"Yields","text":"<p>Any     A unique value from the dimension.</p> Source code in <code>cluster_experiments/inference/dimension.py</code> <pre><code>def iterate_dimension_values(self):\n    \"\"\"\n    A generator method to yield name and values from the dimension.\n\n    Yields\n    ------\n    Any\n        A unique value from the dimension.\n    \"\"\"\n    seen = set()\n    for value in self.values:\n        if value not in seen:\n            seen.add(value)\n            yield value\n</code></pre>"},{"location":"api/experiment_analysis.html","title":"<code>from cluster_experiments.experiment_analysis import *</code>","text":""},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ClusteredOLSAnalysis","title":"<code>ClusteredOLSAnalysis</code>","text":"<p>               Bases: <code>OLSAnalysis</code></p> <p>Class to run OLS clustered analysis</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates</p> <code>None</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> <code>add_covariate_interaction</code> <code>bool</code> <p>bool, if True, adds interaction terms between covariates and treatment</p> <code>False</code> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import ClusteredOLSAnalysis\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 0, 0, 1, 2, 0],\n    'treatment': [\"A\"] * 2 + [\"B\"] * 2 + [\"A\"] * 2 + [\"B\"] * 2,\n    'cluster': [1, 1, 2, 2, 3, 3, 4, 4],\n})\n\nClusteredOLSAnalysis(\n    cluster_cols=['cluster'],\n    target_col='x',\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class ClusteredOLSAnalysis(OLSAnalysis):\n    \"\"\"\n    Class to run OLS clustered analysis\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        covariates: list of columns to use as covariates\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n        add_covariate_interaction: bool, if True, adds interaction terms between covariates and treatment\n\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import ClusteredOLSAnalysis\n    import pandas as pd\n\n    df = pd.DataFrame({\n        'x': [1, 2, 3, 0, 0, 1, 2, 0],\n        'treatment': [\"A\"] * 2 + [\"B\"] * 2 + [\"A\"] * 2 + [\"B\"] * 2,\n        'cluster': [1, 1, 2, 2, 3, 3, 4, 4],\n    })\n\n    ClusteredOLSAnalysis(\n        cluster_cols=['cluster'],\n        target_col='x',\n    ).get_pvalue(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        covariates: Optional[List[str]] = None,\n        hypothesis: str = \"two-sided\",\n        add_covariate_interaction: bool = False,\n        relative_effect: bool = False,\n    ):\n        super().__init__(\n            target_col=target_col,\n            treatment_col=treatment_col,\n            treatment=treatment,\n            covariates=covariates,\n            hypothesis=hypothesis,\n            cov_type=\"cluster\",\n            add_covariate_interaction=add_covariate_interaction,\n            relative_effect=relative_effect,\n        )\n        self.cluster_cols = cluster_cols\n\n    def fit_ols(self, df: pd.DataFrame) -&gt; RegressionResultsProtocol:\n        \"\"\"Returns the fitted OLS model\"\"\"\n        if self.add_covariate_interaction:\n            df = self._add_interaction_covariates(df)\n        ols_fit = sm.OLS.from_formula(\n            self.formula,\n            data=df,\n        ).fit(\n            cov_type=self.cov_type,\n            cov_kwds={\"groups\": self._get_cluster_column(df)},\n        )\n\n        # create point estimate, pvalue and std error transformation in case of relative effects\n        if self.relative_effect:\n            relative_ols_fit = LiftRegressionTransformer(\n                treatment_col=self.treatment_col\n            )\n            relative_ols_fit.fit(\n                ols=ols_fit, df=df, covariate_cols=self.covariates_list\n            )\n            return relative_ols_fit\n\n        return ols_fit\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates an OLSAnalysis object from a PowerConfig object\"\"\"\n        return cls(\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            covariates=config.covariates,\n            hypothesis=config.hypothesis,\n            cluster_cols=config.cluster_cols,\n            add_covariate_interaction=config.add_covariate_interaction,\n            relative_effect=config.relative_effect,\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ClusteredOLSAnalysis.fit_ols","title":"<code>fit_ols(df)</code>","text":"<p>Returns the fitted OLS model</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def fit_ols(self, df: pd.DataFrame) -&gt; RegressionResultsProtocol:\n    \"\"\"Returns the fitted OLS model\"\"\"\n    if self.add_covariate_interaction:\n        df = self._add_interaction_covariates(df)\n    ols_fit = sm.OLS.from_formula(\n        self.formula,\n        data=df,\n    ).fit(\n        cov_type=self.cov_type,\n        cov_kwds={\"groups\": self._get_cluster_column(df)},\n    )\n\n    # create point estimate, pvalue and std error transformation in case of relative effects\n    if self.relative_effect:\n        relative_ols_fit = LiftRegressionTransformer(\n            treatment_col=self.treatment_col\n        )\n        relative_ols_fit.fit(\n            ols=ols_fit, df=df, covariate_cols=self.covariates_list\n        )\n        return relative_ols_fit\n\n    return ols_fit\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ClusteredOLSAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates an OLSAnalysis object from a PowerConfig object</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates an OLSAnalysis object from a PowerConfig object\"\"\"\n    return cls(\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        covariates=config.covariates,\n        hypothesis=config.hypothesis,\n        cluster_cols=config.cluster_cols,\n        add_covariate_interaction=config.add_covariate_interaction,\n        relative_effect=config.relative_effect,\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ConfidenceInterval","title":"<code>ConfidenceInterval</code>  <code>dataclass</code>","text":"<p>Class to define the structure of a confidence interval.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@dataclass\nclass ConfidenceInterval:\n    \"\"\"\n    Class to define the structure of a confidence interval.\n    \"\"\"\n\n    lower: float\n    upper: float\n    alpha: float\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis","title":"<code>DeltaMethodAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class DeltaMethodAnalysis(ExperimentAnalysis):\n    n_clusters_warning_limit = 1000\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        scale_col: str = \"scale\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        covariates: Optional[List[str]] = None,\n        hypothesis: str = \"two-sided\",\n    ):\n        \"\"\"\n        Class to run the Delta Method approximation for estimating the treatment effect on a ratio metric (target/scale) under a clustered design.\n        The analysis is done on the aggregated data at the cluster level, making computation more efficient.\n\n        Arguments:\n            cluster_cols: list of columns to use as clusters.\n            target_col: name of the column containing the variable to measure (the numerator of the ratio).\n            scale_col: name of the column containing the scale variable (the denominator of the ratio).\n            treatment_col: name of the column containing the treatment variable.\n            treatment: name of the treatment to use as the treated group.\n            covariates: list of columns to use as covariates. Have to be previously aggregated at the cluster level.\n            hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis.\n\n            Usage:\n            ```python\n            import pandas as pd\n\n            from cluster_experiments.experiment_analysis import DeltaMethodAnalysis\n\n            df = pd.DataFrame({\n                'x': [1, 2, 3, 0, 0, 1] * 2,\n                'y': [2, 2, 5, 1, 1, 1] * 2,\n                'treatment': [\"A\"] * 6 + [\"B\"] * 6,\n                'cluster': [1, 2, 3, 1, 2, 3] * 2,\n                'z': [1, 2, 3, 4, 5, 6] * 2,\n            })\n\n            DeltaMethodAnalysis(\n                cluster_cols=['cluster'],\n                target_col='x',\n                scale_col='y',\n                covariates=['z']\n            ).get_pvalue(df)\n            ```\n        \"\"\"\n\n        super().__init__(\n            target_col=target_col,\n            treatment_col=treatment_col,\n            cluster_cols=cluster_cols,\n            treatment=treatment,\n            covariates=covariates,\n            hypothesis=hypothesis,\n        )\n        self.scale_col = scale_col\n        self.cluster_cols = cluster_cols or []\n        self.covariates = covariates or []\n\n    def _compute_thetas(self, df: pd.DataFrame) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Computes the theta value for the CUPED method.\n        Thetas are computed as the inverse of the covariance matrix of covariates multiplied by the covariance between covariates and target metric.\n\n        Delta method with CUPED should work like the following. For each randomization unit i we observe $Y_i$, $N_i$.\n\n        Refer to delta.md for mathematical details.\n        \"\"\"\n        target = np.asarray(df[self.target_col])\n        scale = np.asarray(df[self.scale_col])\n        covariates = np.asarray(df[self.covariates])\n\n        if len(self.covariates) == 1:\n            # Code for n = 1 (deng et al)\n            Y, N = target, scale\n\n            # need to covariate * scale to get X_i = covariate_i * N_i\n            X, M = np.squeeze(covariates) * scale, scale\n            sigma = np.cov([Y, N, X, M])  # 4\n\n            mu_Y, mu_N = Y.mean(), N.mean()\n            mu_X, mu_M = X.mean(), M.mean()\n            beta1 = np.array([1 / mu_N, -mu_Y / mu_N**2, 0, 0]).T\n            beta2 = np.array([0, 0, 1 / mu_M, -mu_X / mu_M**2]).T\n\n            # formula from Deng et al. n's would cancel out\n            theta = np.dot(beta1, np.matmul(sigma, beta2)) / np.dot(\n                beta2, np.matmul(sigma, beta2)\n            )\n            return {\"theta\": np.array([theta])}\n\n        # Sample means\n        Y, N, M = target, scale, scale\n        X = covariates * scale.reshape(-1, 1)\n        sigma = np.cov(np.column_stack([Y, N, X, M]), rowvar=False, ddof=0)\n\n        mu_Y, mu_N = Y.mean(), N.mean()\n        mu_X, mu_M = X.mean(axis=0), M.mean()\n        k = len(self.covariates)\n\n        # numerator (follow delta.md)\n        cov_YX = sigma[0, 2 : 2 + k]\n        cov_NX = sigma[1, 2 : 2 + k]\n        cov_YN = sigma[0, 1]\n        cov_NN = sigma[1, 1]\n        numerator = (\n            cov_YX / mu_N**2\n            - (mu_Y / mu_N**2) * (cov_NX / mu_N)\n            - mu_X * (cov_YN / mu_N**3)\n            + mu_X * mu_Y * (cov_NN / mu_N**4)\n        )\n\n        # denominator (follow delta.md, K * D * K^T)\n        d_matrix = sigma[2:, 2:]  # (k + 1) x (k + 1)\n        k_matrix = np.zeros((k, k + 1))\n        k_matrix[np.diag_indices(k)] = 1 / mu_M\n        k_matrix[:, -1] = -mu_X / (mu_M**2)\n        denominator = k_matrix @ d_matrix @ k_matrix.T\n\n        theta = np.dot(np.linalg.pinv(denominator), numerator)\n        # return both theta and pieces for variance calculation\n        return {\"theta\": theta, \"numerator\": numerator, \"denominator\": denominator}\n\n    def _get_ratio_variance_simple(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"\n        Variance of the ratio of means (sum(target)/sum(scale)) via delta method.\n\n        Parameters\n        ----------\n        target : array-like\n            Numerator per unit.\n        scale : array-like\n            Denominator per unit.\n\n        Returns\n        -------\n        variance : float\n            Estimated variance of the ratio metric.\n        \"\"\"\n        target = np.asarray(df[self.target_col])\n        scale = np.asarray(df[self.scale_col])\n\n        target_mean, scale_mean = np.mean(target), np.mean(scale)\n\n        # Sample variances and covariance\n        var_target, var_scale = np.var(target, ddof=0), np.var(scale, ddof=0)\n        cov_target_scale = np.cov(target, scale, ddof=0)[0, 1]\n\n        # Gradient of g(\u0232, scale\u0304) = \u0232 / scale\u0304\n        grad_target = 1.0 / scale_mean\n        grad_scale = -target_mean / (scale_mean**2)\n\n        # Delta method variance\n        var_ratio = (\n            (grad_target**2) * var_target\n            + (grad_scale**2) * var_scale\n            + 2 * grad_target * grad_scale * cov_target_scale\n        )\n\n        return var_ratio / len(target)\n\n    def _get_ratio_variance_cuped(\n        self, df: pd.DataFrame, thetas: Optional[Dict[str, np.ndarray]]\n    ) -&gt; float:\n        \"\"\"\n        Y-only CUPED delta variance for the ratio of means on this group's rows.\n        Uses pooled theta, but per-arm moments for the variance pieces.\n        \"\"\"\n        # data\n        target = df[self.target_col].to_numpy()\n        scale = df[self.scale_col].to_numpy()\n        covariates = np.asarray(df[self.covariates])\n\n        if len(self.covariates) == 1:\n            # Code for n = 1 (deng et al)\n            Y, N = target, scale\n            X, M = np.squeeze(covariates) * scale, scale\n            sigma = np.cov([Y, N, X, M])  # 4\n\n            mu_Y, mu_N = Y.mean(), N.mean()\n            # M is the same as N, in general it could be different,\n            # but we're copying Deng et al. here and it's easier\n            mu_X, mu_M = X.mean(), M.mean()\n            # the betas are from the deng paper\n            beta1 = np.array([1 / mu_N, -mu_Y / mu_N**2, 0, 0]).T\n            beta2 = np.array([0, 0, 1 / mu_M, -mu_X / mu_M**2]).T\n\n            var_Y_div_N = np.dot(beta1, np.matmul(sigma, beta1.T))\n            var_X_div_M = np.dot(beta2, np.matmul(sigma, beta2))\n            cov = np.dot(beta1, np.matmul(sigma, beta2))\n\n            # theta is a scalar in this case\n            theta = thetas[\"theta\"][0]\n\n            # can also use traditional delta method for the var_Y_div_N type terms\n            return (var_Y_div_N + (theta**2) * var_X_div_M - 2 * theta * cov) / len(\n                target\n            )\n\n        # multiple covariates\n        variance_simple = self._get_ratio_variance_simple(df) * len(target)\n        theta = thetas[\"theta\"]\n        numerator = thetas[\"numerator\"]\n        denominator = thetas[\"denominator\"]\n\n        # follow delta.md notation, but in general it's var(Y/N) + theta Var(X/M) theta^T - 2 theta cov(Y/N, X/M)\n        var_cuped = (\n            variance_simple + (theta @ denominator @ theta) - 2 * (theta @ numerator)\n        )\n        return var_cuped / len(target)\n\n    def _aggregate_to_cluster(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns an aggregated dataframe of the target and scale variables at the cluster (and treatment) level.\n\n        Arguments:\n            df: dataframe containing the data to analyze\n        \"\"\"\n        group_cols = self.cluster_cols + [self.treatment_col]\n        aggregate_df = df.groupby(by=group_cols, as_index=False).agg(\n            {self.target_col: \"sum\", self.scale_col: \"sum\"}\n        )\n        return aggregate_df\n\n    def _correct_target(\n        self,\n        df: pd.DataFrame,\n        thetas: Optional[Dict[str, np.ndarray]],\n        covariates_means: List[float],\n    ) -&gt; pd.Series:\n        \"\"\"\n        Corrects the target variable using thetas and covariates means.\n        If thetas are not provided, it returns the original target.\n        \"\"\"\n        if len(self.covariates) == 0 or thetas is None:\n            return df[self.target_col]\n\n        # Apply correction using thetas and covariates means\n        corrected_target = df[self.target_col].copy()\n        for covariate, theta, mean in zip(\n            self.covariates, thetas[\"theta\"], covariates_means\n        ):\n            # According to deng, covariate is supposed to be at the lower granularity level\n            # so it predicts the ratio target/scale\n            corrected_target -= theta * (df[covariate] - mean) * df[self.scale_col]\n        return corrected_target\n\n    def _get_ratio_variance(\n        self, df: pd.DataFrame, thetas: Optional[Dict[str, np.ndarray]]\n    ) -&gt; float:\n        \"\"\"\n        Returns the variance of the ratio metric (target/scale) as estimated by the delta method.\n        If covariates are given, variance reduction is used.\n        \"\"\"\n        if self.covariates:\n            return self._get_ratio_variance_cuped(df, thetas)\n        else:\n            return self._get_ratio_variance_simple(df)\n\n    def _get_group_mean_and_variance(\n        self,\n        df: pd.DataFrame,\n        thetas: Optional[Dict[str, np.ndarray]],\n        covariates_means: List[float],\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Returns the mean and variance of the ratio metric (target/scale) as estimated by the delta method for a given group (treatment).\n        If covariates are given, variance reduction is used. For it to work, the dataframe must be aggregated first at the cluster level, so no assumptions on aggregation of covariates has to be done.\n\n        Arguments:\n            df: dataframe containing the data to analyze.\n        \"\"\"\n        corrected_target = self._correct_target(df, thetas, covariates_means)\n        group_mean = sum(corrected_target) / sum(df[self.scale_col])\n\n        if self.covariates:\n            group_variance = self._get_ratio_variance(df, thetas)\n        else:\n            group_variance = self._get_ratio_variance_simple(df)\n\n        # Return the mean and variance of the ratio metric\n        return group_mean, group_variance\n\n    def _get_mean_standard_error(self, df: pd.DataFrame) -&gt; tuple[float, float]:\n        \"\"\"\n        Returns mean and variance of the ratio metric (target/scale) for a given cluster (i.e. user) computed using the Delta Method.\n        Variance reduction is used if covariates are given.\n        \"\"\"\n\n        if (self._get_num_clusters(df) &lt; self.n_clusters_warning_limit).any():\n            self.__warn_small_group_size()\n\n        if self.covariates:\n            self.__check_data_is_aggregated(df)\n        else:\n            df = self._aggregate_to_cluster(df)\n\n        is_treatment = df[self.treatment_col] == 1\n\n        thetas_dict = self._compute_thetas(df) if self.covariates else None\n        covariates_means = [\n            df[covariate].sum() / df[self.scale_col].sum()\n            for covariate in self.covariates\n        ]\n\n        treat_mean, treat_var = self._get_group_mean_and_variance(\n            df[is_treatment], thetas_dict, covariates_means\n        )\n        ctrl_mean, ctrl_var = self._get_group_mean_and_variance(\n            df[~is_treatment], thetas_dict, covariates_means\n        )\n\n        mean_diff = treat_mean - ctrl_mean\n        standard_error = np.sqrt(treat_var + ctrl_var)\n\n        return mean_diff, standard_error\n\n    def analysis_pvalue(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"\n        Returns the p-value of the analysis.\n\n        Arguments:\n            df: dataframe containing the data to analyze.\n        \"\"\"\n\n        mean_diff, standard_error = self._get_mean_standard_error(df)\n\n        z_score = mean_diff / standard_error\n        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n        results_delta = ModelResults(\n            params={self.treatment_col: mean_diff},\n            pvalues={self.treatment_col: p_value},\n        )\n\n        p_value = self.pvalue_based_on_hypothesis(results_delta)\n\n        return p_value\n\n    def analysis_point_estimate(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"Returns the point estimate of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        mean_diff, _standard_error = self._get_mean_standard_error(df)\n        return mean_diff\n\n    def analysis_standard_error(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"Returns the standard error of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        _mean_diff, standard_error = self._get_mean_standard_error(df)\n        return standard_error\n\n    def analysis_confidence_interval(\n        self, df: pd.DataFrame, alpha: float, verbose: bool = False\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Returns the confidence interval of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n        \"\"\"\n        ate, std_error = self._get_mean_standard_error(df)\n\n        z_score = ate / std_error\n        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n        results_delta = ModelResults(\n            params={self.treatment_col: ate},\n            pvalues={self.treatment_col: p_value},\n        )\n\n        p_value = self.pvalue_based_on_hypothesis(results_delta)\n\n        # Extract the confidence interval for the treatment column\n        crit_z_score = norm.ppf(1 - alpha / 2)\n        conf_int = crit_z_score * std_error\n        lower_bound, upper_bound = ate - conf_int, ate + conf_int\n\n        # Return the confidence interval\n        return ConfidenceInterval(lower=lower_bound, upper=upper_bound, alpha=alpha)\n\n    def analysis_inference_results(\n        self, df: pd.DataFrame, alpha: float, verbose: bool = False\n    ) -&gt; InferenceResults:\n        \"\"\"Returns the inference results of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n        \"\"\"\n        ate, std_error = self._get_mean_standard_error(df)\n\n        z_score = ate / std_error\n        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n        results_delta = ModelResults(\n            params={self.treatment_col: ate},\n            pvalues={self.treatment_col: p_value},\n        )\n\n        p_value = self.pvalue_based_on_hypothesis(results_delta)\n\n        # Extract the confidence interval for the treatment column\n        crit_z_score = norm.ppf(1 - alpha / 2)\n        conf_int = crit_z_score * std_error\n        lower_bound, upper_bound = ate - conf_int, ate + conf_int\n\n        # Return the confidence interval\n        return InferenceResults(\n            ate=ate,\n            p_value=p_value,\n            std_error=std_error,\n            conf_int=ConfidenceInterval(\n                lower=lower_bound, upper=upper_bound, alpha=alpha\n            ),\n        )\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a DeltaMethodAnalysis object from a PowerConfig object\"\"\"\n        return cls(\n            cluster_cols=config.cluster_cols,\n            target_col=config.target_col,\n            scale_col=config.scale_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            hypothesis=config.hypothesis,\n            covariates=config.covariates,\n        )\n\n    def __check_data_is_aggregated(self, df):\n        \"\"\"\n        Check if the data is already aggregated at the cluster level.\n        \"\"\"\n\n        if df.groupby(self.cluster_cols).size().max() &gt; 1:\n            raise ValueError(\n                \"The data should be aggregated at the cluster level for the Delta Method analysis using covariates.\"\n            )\n\n    def _get_num_clusters(self, df):\n        \"\"\"\n        Check if there are enough clusters to run the analysis.\n        \"\"\"\n        return df.groupby(self.treatment_col).apply(\n            lambda x: self._get_cluster_column(x).nunique()\n        )\n\n    def __warn_small_group_size(self):\n        warnings.warn(\n            \"Delta Method approximation may not be accurate for small group sizes\"\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.__check_data_is_aggregated","title":"<code>__check_data_is_aggregated(df)</code>","text":"<p>Check if the data is already aggregated at the cluster level.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def __check_data_is_aggregated(self, df):\n    \"\"\"\n    Check if the data is already aggregated at the cluster level.\n    \"\"\"\n\n    if df.groupby(self.cluster_cols).size().max() &gt; 1:\n        raise ValueError(\n            \"The data should be aggregated at the cluster level for the Delta Method analysis using covariates.\"\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.__init__","title":"<code>__init__(cluster_cols, target_col='target', scale_col='scale', treatment_col='treatment', treatment='B', covariates=None, hypothesis='two-sided')</code>","text":"<p>Class to run the Delta Method approximation for estimating the treatment effect on a ratio metric (target/scale) under a clustered design. The analysis is done on the aggregated data at the cluster level, making computation more efficient.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters.</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure (the numerator of the ratio).</p> <code>'target'</code> <code>scale_col</code> <code>str</code> <p>name of the column containing the scale variable (the denominator of the ratio).</p> <code>'scale'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable.</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group.</p> <code>'B'</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates. Have to be previously aggregated at the cluster level.</p> <code>None</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis.</p> <code>'two-sided'</code> <code>Usage</code> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def __init__(\n    self,\n    cluster_cols: List[str],\n    target_col: str = \"target\",\n    scale_col: str = \"scale\",\n    treatment_col: str = \"treatment\",\n    treatment: str = \"B\",\n    covariates: Optional[List[str]] = None,\n    hypothesis: str = \"two-sided\",\n):\n    \"\"\"\n    Class to run the Delta Method approximation for estimating the treatment effect on a ratio metric (target/scale) under a clustered design.\n    The analysis is done on the aggregated data at the cluster level, making computation more efficient.\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters.\n        target_col: name of the column containing the variable to measure (the numerator of the ratio).\n        scale_col: name of the column containing the scale variable (the denominator of the ratio).\n        treatment_col: name of the column containing the treatment variable.\n        treatment: name of the treatment to use as the treated group.\n        covariates: list of columns to use as covariates. Have to be previously aggregated at the cluster level.\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis.\n\n        Usage:\n        ```python\n        import pandas as pd\n\n        from cluster_experiments.experiment_analysis import DeltaMethodAnalysis\n\n        df = pd.DataFrame({\n            'x': [1, 2, 3, 0, 0, 1] * 2,\n            'y': [2, 2, 5, 1, 1, 1] * 2,\n            'treatment': [\"A\"] * 6 + [\"B\"] * 6,\n            'cluster': [1, 2, 3, 1, 2, 3] * 2,\n            'z': [1, 2, 3, 4, 5, 6] * 2,\n        })\n\n        DeltaMethodAnalysis(\n            cluster_cols=['cluster'],\n            target_col='x',\n            scale_col='y',\n            covariates=['z']\n        ).get_pvalue(df)\n        ```\n    \"\"\"\n\n    super().__init__(\n        target_col=target_col,\n        treatment_col=treatment_col,\n        cluster_cols=cluster_cols,\n        treatment=treatment,\n        covariates=covariates,\n        hypothesis=hypothesis,\n    )\n    self.scale_col = scale_col\n    self.cluster_cols = cluster_cols or []\n    self.covariates = covariates or []\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._aggregate_to_cluster","title":"<code>_aggregate_to_cluster(df)</code>","text":"<p>Returns an aggregated dataframe of the target and scale variables at the cluster (and treatment) level.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _aggregate_to_cluster(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns an aggregated dataframe of the target and scale variables at the cluster (and treatment) level.\n\n    Arguments:\n        df: dataframe containing the data to analyze\n    \"\"\"\n    group_cols = self.cluster_cols + [self.treatment_col]\n    aggregate_df = df.groupby(by=group_cols, as_index=False).agg(\n        {self.target_col: \"sum\", self.scale_col: \"sum\"}\n    )\n    return aggregate_df\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._compute_thetas","title":"<code>_compute_thetas(df)</code>","text":"<p>Computes the theta value for the CUPED method. Thetas are computed as the inverse of the covariance matrix of covariates multiplied by the covariance between covariates and target metric.</p> <p>Delta method with CUPED should work like the following. For each randomization unit i we observe $Y_i$, $N_i$.</p> <p>Refer to delta.md for mathematical details.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _compute_thetas(self, df: pd.DataFrame) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Computes the theta value for the CUPED method.\n    Thetas are computed as the inverse of the covariance matrix of covariates multiplied by the covariance between covariates and target metric.\n\n    Delta method with CUPED should work like the following. For each randomization unit i we observe $Y_i$, $N_i$.\n\n    Refer to delta.md for mathematical details.\n    \"\"\"\n    target = np.asarray(df[self.target_col])\n    scale = np.asarray(df[self.scale_col])\n    covariates = np.asarray(df[self.covariates])\n\n    if len(self.covariates) == 1:\n        # Code for n = 1 (deng et al)\n        Y, N = target, scale\n\n        # need to covariate * scale to get X_i = covariate_i * N_i\n        X, M = np.squeeze(covariates) * scale, scale\n        sigma = np.cov([Y, N, X, M])  # 4\n\n        mu_Y, mu_N = Y.mean(), N.mean()\n        mu_X, mu_M = X.mean(), M.mean()\n        beta1 = np.array([1 / mu_N, -mu_Y / mu_N**2, 0, 0]).T\n        beta2 = np.array([0, 0, 1 / mu_M, -mu_X / mu_M**2]).T\n\n        # formula from Deng et al. n's would cancel out\n        theta = np.dot(beta1, np.matmul(sigma, beta2)) / np.dot(\n            beta2, np.matmul(sigma, beta2)\n        )\n        return {\"theta\": np.array([theta])}\n\n    # Sample means\n    Y, N, M = target, scale, scale\n    X = covariates * scale.reshape(-1, 1)\n    sigma = np.cov(np.column_stack([Y, N, X, M]), rowvar=False, ddof=0)\n\n    mu_Y, mu_N = Y.mean(), N.mean()\n    mu_X, mu_M = X.mean(axis=0), M.mean()\n    k = len(self.covariates)\n\n    # numerator (follow delta.md)\n    cov_YX = sigma[0, 2 : 2 + k]\n    cov_NX = sigma[1, 2 : 2 + k]\n    cov_YN = sigma[0, 1]\n    cov_NN = sigma[1, 1]\n    numerator = (\n        cov_YX / mu_N**2\n        - (mu_Y / mu_N**2) * (cov_NX / mu_N)\n        - mu_X * (cov_YN / mu_N**3)\n        + mu_X * mu_Y * (cov_NN / mu_N**4)\n    )\n\n    # denominator (follow delta.md, K * D * K^T)\n    d_matrix = sigma[2:, 2:]  # (k + 1) x (k + 1)\n    k_matrix = np.zeros((k, k + 1))\n    k_matrix[np.diag_indices(k)] = 1 / mu_M\n    k_matrix[:, -1] = -mu_X / (mu_M**2)\n    denominator = k_matrix @ d_matrix @ k_matrix.T\n\n    theta = np.dot(np.linalg.pinv(denominator), numerator)\n    # return both theta and pieces for variance calculation\n    return {\"theta\": theta, \"numerator\": numerator, \"denominator\": denominator}\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._correct_target","title":"<code>_correct_target(df, thetas, covariates_means)</code>","text":"<p>Corrects the target variable using thetas and covariates means. If thetas are not provided, it returns the original target.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _correct_target(\n    self,\n    df: pd.DataFrame,\n    thetas: Optional[Dict[str, np.ndarray]],\n    covariates_means: List[float],\n) -&gt; pd.Series:\n    \"\"\"\n    Corrects the target variable using thetas and covariates means.\n    If thetas are not provided, it returns the original target.\n    \"\"\"\n    if len(self.covariates) == 0 or thetas is None:\n        return df[self.target_col]\n\n    # Apply correction using thetas and covariates means\n    corrected_target = df[self.target_col].copy()\n    for covariate, theta, mean in zip(\n        self.covariates, thetas[\"theta\"], covariates_means\n    ):\n        # According to deng, covariate is supposed to be at the lower granularity level\n        # so it predicts the ratio target/scale\n        corrected_target -= theta * (df[covariate] - mean) * df[self.scale_col]\n    return corrected_target\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_group_mean_and_variance","title":"<code>_get_group_mean_and_variance(df, thetas, covariates_means)</code>","text":"<p>Returns the mean and variance of the ratio metric (target/scale) as estimated by the delta method for a given group (treatment). If covariates are given, variance reduction is used. For it to work, the dataframe must be aggregated first at the cluster level, so no assumptions on aggregation of covariates has to be done.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze.</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_group_mean_and_variance(\n    self,\n    df: pd.DataFrame,\n    thetas: Optional[Dict[str, np.ndarray]],\n    covariates_means: List[float],\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Returns the mean and variance of the ratio metric (target/scale) as estimated by the delta method for a given group (treatment).\n    If covariates are given, variance reduction is used. For it to work, the dataframe must be aggregated first at the cluster level, so no assumptions on aggregation of covariates has to be done.\n\n    Arguments:\n        df: dataframe containing the data to analyze.\n    \"\"\"\n    corrected_target = self._correct_target(df, thetas, covariates_means)\n    group_mean = sum(corrected_target) / sum(df[self.scale_col])\n\n    if self.covariates:\n        group_variance = self._get_ratio_variance(df, thetas)\n    else:\n        group_variance = self._get_ratio_variance_simple(df)\n\n    # Return the mean and variance of the ratio metric\n    return group_mean, group_variance\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_mean_standard_error","title":"<code>_get_mean_standard_error(df)</code>","text":"<p>Returns mean and variance of the ratio metric (target/scale) for a given cluster (i.e. user) computed using the Delta Method. Variance reduction is used if covariates are given.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_mean_standard_error(self, df: pd.DataFrame) -&gt; tuple[float, float]:\n    \"\"\"\n    Returns mean and variance of the ratio metric (target/scale) for a given cluster (i.e. user) computed using the Delta Method.\n    Variance reduction is used if covariates are given.\n    \"\"\"\n\n    if (self._get_num_clusters(df) &lt; self.n_clusters_warning_limit).any():\n        self.__warn_small_group_size()\n\n    if self.covariates:\n        self.__check_data_is_aggregated(df)\n    else:\n        df = self._aggregate_to_cluster(df)\n\n    is_treatment = df[self.treatment_col] == 1\n\n    thetas_dict = self._compute_thetas(df) if self.covariates else None\n    covariates_means = [\n        df[covariate].sum() / df[self.scale_col].sum()\n        for covariate in self.covariates\n    ]\n\n    treat_mean, treat_var = self._get_group_mean_and_variance(\n        df[is_treatment], thetas_dict, covariates_means\n    )\n    ctrl_mean, ctrl_var = self._get_group_mean_and_variance(\n        df[~is_treatment], thetas_dict, covariates_means\n    )\n\n    mean_diff = treat_mean - ctrl_mean\n    standard_error = np.sqrt(treat_var + ctrl_var)\n\n    return mean_diff, standard_error\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_num_clusters","title":"<code>_get_num_clusters(df)</code>","text":"<p>Check if there are enough clusters to run the analysis.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_num_clusters(self, df):\n    \"\"\"\n    Check if there are enough clusters to run the analysis.\n    \"\"\"\n    return df.groupby(self.treatment_col).apply(\n        lambda x: self._get_cluster_column(x).nunique()\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_ratio_variance","title":"<code>_get_ratio_variance(df, thetas)</code>","text":"<p>Returns the variance of the ratio metric (target/scale) as estimated by the delta method. If covariates are given, variance reduction is used.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_ratio_variance(\n    self, df: pd.DataFrame, thetas: Optional[Dict[str, np.ndarray]]\n) -&gt; float:\n    \"\"\"\n    Returns the variance of the ratio metric (target/scale) as estimated by the delta method.\n    If covariates are given, variance reduction is used.\n    \"\"\"\n    if self.covariates:\n        return self._get_ratio_variance_cuped(df, thetas)\n    else:\n        return self._get_ratio_variance_simple(df)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_ratio_variance_cuped","title":"<code>_get_ratio_variance_cuped(df, thetas)</code>","text":"<p>Y-only CUPED delta variance for the ratio of means on this group's rows. Uses pooled theta, but per-arm moments for the variance pieces.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_ratio_variance_cuped(\n    self, df: pd.DataFrame, thetas: Optional[Dict[str, np.ndarray]]\n) -&gt; float:\n    \"\"\"\n    Y-only CUPED delta variance for the ratio of means on this group's rows.\n    Uses pooled theta, but per-arm moments for the variance pieces.\n    \"\"\"\n    # data\n    target = df[self.target_col].to_numpy()\n    scale = df[self.scale_col].to_numpy()\n    covariates = np.asarray(df[self.covariates])\n\n    if len(self.covariates) == 1:\n        # Code for n = 1 (deng et al)\n        Y, N = target, scale\n        X, M = np.squeeze(covariates) * scale, scale\n        sigma = np.cov([Y, N, X, M])  # 4\n\n        mu_Y, mu_N = Y.mean(), N.mean()\n        # M is the same as N, in general it could be different,\n        # but we're copying Deng et al. here and it's easier\n        mu_X, mu_M = X.mean(), M.mean()\n        # the betas are from the deng paper\n        beta1 = np.array([1 / mu_N, -mu_Y / mu_N**2, 0, 0]).T\n        beta2 = np.array([0, 0, 1 / mu_M, -mu_X / mu_M**2]).T\n\n        var_Y_div_N = np.dot(beta1, np.matmul(sigma, beta1.T))\n        var_X_div_M = np.dot(beta2, np.matmul(sigma, beta2))\n        cov = np.dot(beta1, np.matmul(sigma, beta2))\n\n        # theta is a scalar in this case\n        theta = thetas[\"theta\"][0]\n\n        # can also use traditional delta method for the var_Y_div_N type terms\n        return (var_Y_div_N + (theta**2) * var_X_div_M - 2 * theta * cov) / len(\n            target\n        )\n\n    # multiple covariates\n    variance_simple = self._get_ratio_variance_simple(df) * len(target)\n    theta = thetas[\"theta\"]\n    numerator = thetas[\"numerator\"]\n    denominator = thetas[\"denominator\"]\n\n    # follow delta.md notation, but in general it's var(Y/N) + theta Var(X/M) theta^T - 2 theta cov(Y/N, X/M)\n    var_cuped = (\n        variance_simple + (theta @ denominator @ theta) - 2 * (theta @ numerator)\n    )\n    return var_cuped / len(target)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_ratio_variance_simple","title":"<code>_get_ratio_variance_simple(df)</code>","text":"<p>Variance of the ratio of means (sum(target)/sum(scale)) via delta method.</p>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_ratio_variance_simple--parameters","title":"Parameters","text":"<p>target : array-like     Numerator per unit. scale : array-like     Denominator per unit.</p>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis._get_ratio_variance_simple--returns","title":"Returns","text":"<p>variance : float     Estimated variance of the ratio metric.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_ratio_variance_simple(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Variance of the ratio of means (sum(target)/sum(scale)) via delta method.\n\n    Parameters\n    ----------\n    target : array-like\n        Numerator per unit.\n    scale : array-like\n        Denominator per unit.\n\n    Returns\n    -------\n    variance : float\n        Estimated variance of the ratio metric.\n    \"\"\"\n    target = np.asarray(df[self.target_col])\n    scale = np.asarray(df[self.scale_col])\n\n    target_mean, scale_mean = np.mean(target), np.mean(scale)\n\n    # Sample variances and covariance\n    var_target, var_scale = np.var(target, ddof=0), np.var(scale, ddof=0)\n    cov_target_scale = np.cov(target, scale, ddof=0)[0, 1]\n\n    # Gradient of g(\u0232, scale\u0304) = \u0232 / scale\u0304\n    grad_target = 1.0 / scale_mean\n    grad_scale = -target_mean / (scale_mean**2)\n\n    # Delta method variance\n    var_ratio = (\n        (grad_target**2) * var_target\n        + (grad_scale**2) * var_scale\n        + 2 * grad_target * grad_scale * cov_target_scale\n    )\n\n    return var_ratio / len(target)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.analysis_confidence_interval","title":"<code>analysis_confidence_interval(df, alpha, verbose=False)</code>","text":"<p>Returns the confidence interval of the analysis Arguments:     df: dataframe containing the data to analyze     alpha: significance level</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_confidence_interval(\n    self, df: pd.DataFrame, alpha: float, verbose: bool = False\n) -&gt; ConfidenceInterval:\n    \"\"\"Returns the confidence interval of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n    \"\"\"\n    ate, std_error = self._get_mean_standard_error(df)\n\n    z_score = ate / std_error\n    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n    results_delta = ModelResults(\n        params={self.treatment_col: ate},\n        pvalues={self.treatment_col: p_value},\n    )\n\n    p_value = self.pvalue_based_on_hypothesis(results_delta)\n\n    # Extract the confidence interval for the treatment column\n    crit_z_score = norm.ppf(1 - alpha / 2)\n    conf_int = crit_z_score * std_error\n    lower_bound, upper_bound = ate - conf_int, ate + conf_int\n\n    # Return the confidence interval\n    return ConfidenceInterval(lower=lower_bound, upper=upper_bound, alpha=alpha)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.analysis_inference_results","title":"<code>analysis_inference_results(df, alpha, verbose=False)</code>","text":"<p>Returns the inference results of the analysis Arguments:     df: dataframe containing the data to analyze     alpha: significance level</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_inference_results(\n    self, df: pd.DataFrame, alpha: float, verbose: bool = False\n) -&gt; InferenceResults:\n    \"\"\"Returns the inference results of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n    \"\"\"\n    ate, std_error = self._get_mean_standard_error(df)\n\n    z_score = ate / std_error\n    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n    results_delta = ModelResults(\n        params={self.treatment_col: ate},\n        pvalues={self.treatment_col: p_value},\n    )\n\n    p_value = self.pvalue_based_on_hypothesis(results_delta)\n\n    # Extract the confidence interval for the treatment column\n    crit_z_score = norm.ppf(1 - alpha / 2)\n    conf_int = crit_z_score * std_error\n    lower_bound, upper_bound = ate - conf_int, ate + conf_int\n\n    # Return the confidence interval\n    return InferenceResults(\n        ate=ate,\n        p_value=p_value,\n        std_error=std_error,\n        conf_int=ConfidenceInterval(\n            lower=lower_bound, upper=upper_bound, alpha=alpha\n        ),\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.analysis_point_estimate","title":"<code>analysis_point_estimate(df)</code>","text":"<p>Returns the point estimate of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_point_estimate(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"Returns the point estimate of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    mean_diff, _standard_error = self._get_mean_standard_error(df)\n    return mean_diff\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df)</code>","text":"<p>Returns the p-value of the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze.</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Returns the p-value of the analysis.\n\n    Arguments:\n        df: dataframe containing the data to analyze.\n    \"\"\"\n\n    mean_diff, standard_error = self._get_mean_standard_error(df)\n\n    z_score = mean_diff / standard_error\n    p_value = 2 * (1 - norm.cdf(abs(z_score)))\n\n    results_delta = ModelResults(\n        params={self.treatment_col: mean_diff},\n        pvalues={self.treatment_col: p_value},\n    )\n\n    p_value = self.pvalue_based_on_hypothesis(results_delta)\n\n    return p_value\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.analysis_standard_error","title":"<code>analysis_standard_error(df)</code>","text":"<p>Returns the standard error of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_standard_error(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"Returns the standard error of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    _mean_diff, standard_error = self._get_mean_standard_error(df)\n    return standard_error\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.DeltaMethodAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a DeltaMethodAnalysis object from a PowerConfig object</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a DeltaMethodAnalysis object from a PowerConfig object\"\"\"\n    return cls(\n        cluster_cols=config.cluster_cols,\n        target_col=config.target_col,\n        scale_col=config.scale_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        hypothesis=config.hypothesis,\n        covariates=config.covariates,\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis","title":"<code>ExperimentAnalysis</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class to run the analysis of a given experiment</p> <p>In order to create your own ExperimentAnalysis, you should create a derived class that implements the analysis_pvalue method.</p> <p>It can also be used as a component of the PowerAnalysis class.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates</p> <code>None</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class ExperimentAnalysis(ABC):\n    \"\"\"\n    Abstract class to run the analysis of a given experiment\n\n    In order to create your own ExperimentAnalysis,\n    you should create a derived class that implements the analysis_pvalue method.\n\n    It can also be used as a component of the PowerAnalysis class.\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        covariates: list of columns to use as covariates\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        covariates: Optional[List[str]] = None,\n        hypothesis: str = \"two-sided\",\n        add_covariate_interaction: bool = False,\n    ):\n        self.target_col = target_col\n        self.treatment = treatment\n        self.treatment_col = treatment_col\n        self.cluster_cols = cluster_cols\n        self.covariates = covariates or []\n        self.hypothesis = hypothesis\n        self.add_covariate_interaction = add_covariate_interaction\n\n    def _get_cluster_column(self, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Paste all strings of cluster_cols in one single column\"\"\"\n        df = df.copy()\n        return df[self.cluster_cols].astype(str).sum(axis=1)\n\n    def _create_binary_treatment(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Transforms treatment column into 0 - 1 column\"\"\"\n        df = df.copy()\n        df[self.treatment_col] = (df[self.treatment_col] == self.treatment).astype(int)\n        return df\n\n    def _add_interaction_covariates(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"For each covariate, adds a column with treatment * (x - mean(x))\n        This is used to build a more efficient estimator of the ATE\n\n        Args\n        ----\n            df (pd.DataFrame): input data frame\n\n        Returns\n        -------\n            pd.DataFrame: data frame with additional columns\n\n        \"\"\"\n        df = df.copy()\n        if self.covariates is None:\n            return df\n\n        for covariate in self.covariates:\n            df[f\"__{covariate}__interaction\"] = (\n                df[covariate] - df[covariate].mean()\n            ) * df[self.treatment_col]\n        return df\n\n    @property\n    def covariates_list(self) -&gt; List[str]:\n        if len(self.covariates) == 0:\n            # simple case, no covariates\n            return []\n\n        if not self.add_covariate_interaction:\n            # second case: covariates but not interaction\n            return self.covariates\n\n        # third case: covariates and interaction\n        return self.covariates + [\n            f\"__{covariate}__interaction\" for covariate in self.covariates\n        ]\n\n    @property\n    def formula(self):\n        if len(self.covariates) == 0:\n            # simple case, no covariates\n            return f\"{self.target_col} ~ {self.treatment_col}\"\n\n        if not self.add_covariate_interaction:\n            # second case: covariates but not interaction\n            return f\"{self.target_col} ~ {self.treatment_col} + {' + '.join(self.covariates)}\"\n\n        # third case: covariates and interaction\n        return f\"{self.target_col} ~ {self.treatment_col} + {' + '.join(self.covariates)} + {' + '.join([f'__{covariate}__interaction' for covariate in self.covariates])}\"\n\n    @abstractmethod\n    def analysis_pvalue(\n        self,\n        df: pd.DataFrame,\n        verbose: bool = False,\n    ) -&gt; float:\n        \"\"\"\n        Returns the p-value of the analysis. Expects treatment to be 0-1 variable\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n\n    def analysis_point_estimate(\n        self,\n        df: pd.DataFrame,\n        verbose: bool = False,\n    ) -&gt; float:\n        \"\"\"\n        Returns the point estimate of the analysis. Expects treatment to be 0-1 variable\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        raise NotImplementedError(\"Point estimate not implemented for this analysis\")\n\n    def analysis_standard_error(\n        self,\n        df: pd.DataFrame,\n        verbose: bool = False,\n    ) -&gt; float:\n        \"\"\"\n        Returns the standard error of the analysis. Expects treatment to be 0-1 variable\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        raise NotImplementedError(\"Standard error not implemented for this analysis\")\n\n    def analysis_confidence_interval(\n        self,\n        df: pd.DataFrame,\n        alpha: float,\n        verbose: bool = False,\n    ) -&gt; ConfidenceInterval:\n        \"\"\"\n        Returns the confidence interval of the analysis. Expects treatment to be 0-1 variable\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        raise NotImplementedError(\n            \"Confidence Interval not implemented for this analysis\"\n        )\n\n    def analysis_inference_results(\n        self,\n        df: pd.DataFrame,\n        alpha: float,\n        verbose: bool = False,\n    ) -&gt; InferenceResults:\n        \"\"\"\n        Returns the InferenceResults object of the analysis. Expects treatment to be 0-1 variable\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        raise NotImplementedError(\n            \"Inference results are not implemented for this analysis\"\n        )\n\n    def _data_checks(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Checks that the data is correct\"\"\"\n        if df[self.target_col].isnull().any():\n            raise ValueError(\n                f\"There are null values in outcome column {self.treatment_col}\"\n            )\n\n        if not is_numeric_dtype(df[self.target_col]):\n            raise ValueError(\n                f\"Outcome column {self.target_col} should be numeric and not {df[self.target_col].dtype}\"\n            )\n\n    def get_pvalue(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n\n        Arguments:\n            df: dataframe containing the data to analyze\n        \"\"\"\n        df = df.copy()\n        df = self._create_binary_treatment(df)\n        self._data_checks(df=df)\n        return self.analysis_pvalue(df)\n\n    def get_point_estimate(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"Returns the point estimate of the analysis\n\n        Arguments:\n            df: dataframe containing the data to analyze\n        \"\"\"\n        df = df.copy()\n        df = self._create_binary_treatment(df)\n        self._data_checks(df=df)\n        return self.analysis_point_estimate(df)\n\n    def get_standard_error(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"Returns the standard error of the analysis\n\n        Arguments:\n            df: dataframe containing the data to analyze\n        \"\"\"\n        df = df.copy()\n        df = self._create_binary_treatment(df)\n        self._data_checks(df=df)\n        return self.analysis_standard_error(df)\n\n    def get_confidence_interval(\n        self, df: pd.DataFrame, alpha: float\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Returns the confidence interval of the analysis\n\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n        \"\"\"\n        df = df.copy()\n        df = self._create_binary_treatment(df)\n        self._data_checks(df=df)\n        return self.analysis_confidence_interval(df, alpha)\n\n    def get_inference_results(self, df: pd.DataFrame, alpha: float) -&gt; InferenceResults:\n        \"\"\"Returns the inference results of the analysis\n\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n        \"\"\"\n        df = df.copy()\n        df = self._create_binary_treatment(df)\n        self._data_checks(df=df)\n        return self.analysis_inference_results(df, alpha)\n\n    def pvalue_based_on_hypothesis(\n        self, model_result: RegressionResultsProtocol\n    ) -&gt; float:  # todo add typehint statsmodels result\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            model_result: statsmodels result object\n            verbose (Optional): bool, prints the regression summary if True\n\n        \"\"\"\n        treatment_effect = model_result.params[self.treatment_col]\n        p_value = model_result.pvalues[self.treatment_col]\n\n        if HypothesisEntries(self.hypothesis) == HypothesisEntries.LESS:\n            return p_value / 2 if treatment_effect &lt;= 0 else 1 - p_value / 2\n        if HypothesisEntries(self.hypothesis) == HypothesisEntries.GREATER:\n            return p_value / 2 if treatment_effect &gt;= 0 else 1 - p_value / 2\n        if HypothesisEntries(self.hypothesis) == HypothesisEntries.TWO_SIDED:\n            return p_value\n        raise ValueError(f\"{self.hypothesis} is not a valid HypothesisEntries\")\n\n    def _split_pre_experiment_df(self, df: pd.DataFrame):\n        raise NotImplementedError(\n            \"This method should be implemented in the child class\"\n        )\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates an ExperimentAnalysis object from a PowerConfig object\"\"\"\n        return cls(\n            cluster_cols=config.cluster_cols,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            covariates=config.covariates,\n            hypothesis=config.hypothesis,\n            add_covariate_interaction=config.add_covariate_interaction,\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis._add_interaction_covariates","title":"<code>_add_interaction_covariates(df)</code>","text":"<p>For each covariate, adds a column with treatment * (x - mean(x)) This is used to build a more efficient estimator of the ATE</p>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis._add_interaction_covariates--args","title":"Args","text":"<pre><code>df (pd.DataFrame): input data frame\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis._add_interaction_covariates--returns","title":"Returns","text":"<pre><code>pd.DataFrame: data frame with additional columns\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _add_interaction_covariates(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"For each covariate, adds a column with treatment * (x - mean(x))\n    This is used to build a more efficient estimator of the ATE\n\n    Args\n    ----\n        df (pd.DataFrame): input data frame\n\n    Returns\n    -------\n        pd.DataFrame: data frame with additional columns\n\n    \"\"\"\n    df = df.copy()\n    if self.covariates is None:\n        return df\n\n    for covariate in self.covariates:\n        df[f\"__{covariate}__interaction\"] = (\n            df[covariate] - df[covariate].mean()\n        ) * df[self.treatment_col]\n    return df\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis._create_binary_treatment","title":"<code>_create_binary_treatment(df)</code>","text":"<p>Transforms treatment column into 0 - 1 column</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _create_binary_treatment(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transforms treatment column into 0 - 1 column\"\"\"\n    df = df.copy()\n    df[self.treatment_col] = (df[self.treatment_col] == self.treatment).astype(int)\n    return df\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis._data_checks","title":"<code>_data_checks(df)</code>","text":"<p>Checks that the data is correct</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _data_checks(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Checks that the data is correct\"\"\"\n    if df[self.target_col].isnull().any():\n        raise ValueError(\n            f\"There are null values in outcome column {self.treatment_col}\"\n        )\n\n    if not is_numeric_dtype(df[self.target_col]):\n        raise ValueError(\n            f\"Outcome column {self.target_col} should be numeric and not {df[self.target_col].dtype}\"\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis._get_cluster_column","title":"<code>_get_cluster_column(df)</code>","text":"<p>Paste all strings of cluster_cols in one single column</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_cluster_column(self, df: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Paste all strings of cluster_cols in one single column\"\"\"\n    df = df.copy()\n    return df[self.cluster_cols].astype(str).sum(axis=1)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.analysis_confidence_interval","title":"<code>analysis_confidence_interval(df, alpha, verbose=False)</code>","text":"<p>Returns the confidence interval of the analysis. Expects treatment to be 0-1 variable Arguments:     df: dataframe containing the data to analyze     alpha: significance level     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_confidence_interval(\n    self,\n    df: pd.DataFrame,\n    alpha: float,\n    verbose: bool = False,\n) -&gt; ConfidenceInterval:\n    \"\"\"\n    Returns the confidence interval of the analysis. Expects treatment to be 0-1 variable\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    raise NotImplementedError(\n        \"Confidence Interval not implemented for this analysis\"\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.analysis_inference_results","title":"<code>analysis_inference_results(df, alpha, verbose=False)</code>","text":"<p>Returns the InferenceResults object of the analysis. Expects treatment to be 0-1 variable Arguments:     df: dataframe containing the data to analyze     alpha: significance level     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_inference_results(\n    self,\n    df: pd.DataFrame,\n    alpha: float,\n    verbose: bool = False,\n) -&gt; InferenceResults:\n    \"\"\"\n    Returns the InferenceResults object of the analysis. Expects treatment to be 0-1 variable\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    raise NotImplementedError(\n        \"Inference results are not implemented for this analysis\"\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.analysis_point_estimate","title":"<code>analysis_point_estimate(df, verbose=False)</code>","text":"<p>Returns the point estimate of the analysis. Expects treatment to be 0-1 variable Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_point_estimate(\n    self,\n    df: pd.DataFrame,\n    verbose: bool = False,\n) -&gt; float:\n    \"\"\"\n    Returns the point estimate of the analysis. Expects treatment to be 0-1 variable\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    raise NotImplementedError(\"Point estimate not implemented for this analysis\")\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>  <code>abstractmethod</code>","text":"<p>Returns the p-value of the analysis. Expects treatment to be 0-1 variable Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@abstractmethod\ndef analysis_pvalue(\n    self,\n    df: pd.DataFrame,\n    verbose: bool = False,\n) -&gt; float:\n    \"\"\"\n    Returns the p-value of the analysis. Expects treatment to be 0-1 variable\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.analysis_standard_error","title":"<code>analysis_standard_error(df, verbose=False)</code>","text":"<p>Returns the standard error of the analysis. Expects treatment to be 0-1 variable Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_standard_error(\n    self,\n    df: pd.DataFrame,\n    verbose: bool = False,\n) -&gt; float:\n    \"\"\"\n    Returns the standard error of the analysis. Expects treatment to be 0-1 variable\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    raise NotImplementedError(\"Standard error not implemented for this analysis\")\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates an ExperimentAnalysis object from a PowerConfig object</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates an ExperimentAnalysis object from a PowerConfig object\"\"\"\n    return cls(\n        cluster_cols=config.cluster_cols,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        covariates=config.covariates,\n        hypothesis=config.hypothesis,\n        add_covariate_interaction=config.add_covariate_interaction,\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.get_confidence_interval","title":"<code>get_confidence_interval(df, alpha)</code>","text":"<p>Returns the confidence interval of the analysis</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze</p> required <code>alpha</code> <code>float</code> <p>significance level</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def get_confidence_interval(\n    self, df: pd.DataFrame, alpha: float\n) -&gt; ConfidenceInterval:\n    \"\"\"Returns the confidence interval of the analysis\n\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n    \"\"\"\n    df = df.copy()\n    df = self._create_binary_treatment(df)\n    self._data_checks(df=df)\n    return self.analysis_confidence_interval(df, alpha)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.get_inference_results","title":"<code>get_inference_results(df, alpha)</code>","text":"<p>Returns the inference results of the analysis</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze</p> required <code>alpha</code> <code>float</code> <p>significance level</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def get_inference_results(self, df: pd.DataFrame, alpha: float) -&gt; InferenceResults:\n    \"\"\"Returns the inference results of the analysis\n\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n    \"\"\"\n    df = df.copy()\n    df = self._create_binary_treatment(df)\n    self._data_checks(df=df)\n    return self.analysis_inference_results(df, alpha)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.get_point_estimate","title":"<code>get_point_estimate(df)</code>","text":"<p>Returns the point estimate of the analysis</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def get_point_estimate(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"Returns the point estimate of the analysis\n\n    Arguments:\n        df: dataframe containing the data to analyze\n    \"\"\"\n    df = df.copy()\n    df = self._create_binary_treatment(df)\n    self._data_checks(df=df)\n    return self.analysis_point_estimate(df)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.get_pvalue","title":"<code>get_pvalue(df)</code>","text":"<p>Returns the p-value of the analysis</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def get_pvalue(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"Returns the p-value of the analysis\n\n    Arguments:\n        df: dataframe containing the data to analyze\n    \"\"\"\n    df = df.copy()\n    df = self._create_binary_treatment(df)\n    self._data_checks(df=df)\n    return self.analysis_pvalue(df)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.get_standard_error","title":"<code>get_standard_error(df)</code>","text":"<p>Returns the standard error of the analysis</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing the data to analyze</p> required Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def get_standard_error(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"Returns the standard error of the analysis\n\n    Arguments:\n        df: dataframe containing the data to analyze\n    \"\"\"\n    df = df.copy()\n    df = self._create_binary_treatment(df)\n    self._data_checks(df=df)\n    return self.analysis_standard_error(df)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.ExperimentAnalysis.pvalue_based_on_hypothesis","title":"<code>pvalue_based_on_hypothesis(model_result)</code>","text":"<p>Returns the p-value of the analysis Arguments:     model_result: statsmodels result object     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def pvalue_based_on_hypothesis(\n    self, model_result: RegressionResultsProtocol\n) -&gt; float:  # todo add typehint statsmodels result\n    \"\"\"Returns the p-value of the analysis\n    Arguments:\n        model_result: statsmodels result object\n        verbose (Optional): bool, prints the regression summary if True\n\n    \"\"\"\n    treatment_effect = model_result.params[self.treatment_col]\n    p_value = model_result.pvalues[self.treatment_col]\n\n    if HypothesisEntries(self.hypothesis) == HypothesisEntries.LESS:\n        return p_value / 2 if treatment_effect &lt;= 0 else 1 - p_value / 2\n    if HypothesisEntries(self.hypothesis) == HypothesisEntries.GREATER:\n        return p_value / 2 if treatment_effect &gt;= 0 else 1 - p_value / 2\n    if HypothesisEntries(self.hypothesis) == HypothesisEntries.TWO_SIDED:\n        return p_value\n    raise ValueError(f\"{self.hypothesis} is not a valid HypothesisEntries\")\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis","title":"<code>GeeExperimentAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> <p>Class to run GEE clustered analysis</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates</p> <code>None</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import GeeExperimentAnalysis\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 0, 0, 1],\n    'treatment': [\"A\"] * 3 + [\"B\"] * 3,\n    'cluster': [1] * 6,\n})\n\nGeeExperimentAnalysis(\n    cluster_cols=['cluster'],\n    target_col='x',\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class GeeExperimentAnalysis(ExperimentAnalysis):\n    \"\"\"\n    Class to run GEE clustered analysis\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        covariates: list of columns to use as covariates\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import GeeExperimentAnalysis\n    import pandas as pd\n\n    df = pd.DataFrame({\n        'x': [1, 2, 3, 0, 0, 1],\n        'treatment': [\"A\"] * 3 + [\"B\"] * 3,\n        'cluster': [1] * 6,\n    })\n\n    GeeExperimentAnalysis(\n        cluster_cols=['cluster'],\n        target_col='x',\n    ).get_pvalue(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        covariates: Optional[List[str]] = None,\n        hypothesis: str = \"two-sided\",\n        add_covariate_interaction: bool = False,\n    ):\n        super().__init__(\n            target_col=target_col,\n            treatment_col=treatment_col,\n            cluster_cols=cluster_cols,\n            treatment=treatment,\n            covariates=covariates,\n            hypothesis=hypothesis,\n            add_covariate_interaction=add_covariate_interaction,\n        )\n        self.fam = sm.families.Gaussian()\n        self.va = sm.cov_struct.Exchangeable()\n\n    def fit_gee(self, df: pd.DataFrame) -&gt; sm.GEE:\n        \"\"\"Returns the fitted GEE model\"\"\"\n        if self.add_covariate_interaction:\n            df = self._add_interaction_covariates(df)\n        return sm.GEE.from_formula(\n            self.formula,\n            data=df,\n            groups=self._get_cluster_column(df),\n            family=self.fam,\n            cov_struct=self.va,\n        ).fit()\n\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_gee = self.fit_gee(df)\n        if verbose:\n            print(results_gee.summary())\n\n        p_value = self.pvalue_based_on_hypothesis(results_gee)\n        return p_value\n\n    def analysis_point_estimate(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the point estimate of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_gee = self.fit_gee(df)\n        return results_gee.params[self.treatment_col]\n\n    def analysis_standard_error(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the standard error of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_gee = self.fit_gee(df)\n        return results_gee.bse[self.treatment_col]\n\n    def analysis_confidence_interval(\n        self, df: pd.DataFrame, alpha: float, verbose: bool = False\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Returns the confidence interval of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_gee = self.fit_gee(df)\n        # Extract the confidence interval for the treatment column\n        conf_int_df = results_gee.conf_int(alpha=alpha)\n        lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n        if verbose:\n            print(results_gee.summary())\n\n        # Return the confidence interval\n        return ConfidenceInterval(lower=lower_bound, upper=upper_bound, alpha=alpha)\n\n    def analysis_inference_results(\n        self, df: pd.DataFrame, alpha: float, verbose: bool = False\n    ) -&gt; InferenceResults:\n        \"\"\"Returns the inference results of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_gee = self.fit_gee(df)\n\n        std_error = results_gee.bse[self.treatment_col]\n        ate = results_gee.params[self.treatment_col]\n        p_value = self.pvalue_based_on_hypothesis(results_gee)\n\n        # Extract the confidence interval for the treatment column\n        conf_int_df = results_gee.conf_int(alpha=alpha)\n        lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n        if verbose:\n            print(results_gee.summary())\n\n        # Return the confidence interval\n        return InferenceResults(\n            ate=ate,\n            p_value=p_value,\n            std_error=std_error,\n            conf_int=ConfidenceInterval(\n                lower=lower_bound, upper=upper_bound, alpha=alpha\n            ),\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.analysis_confidence_interval","title":"<code>analysis_confidence_interval(df, alpha, verbose=False)</code>","text":"<p>Returns the confidence interval of the analysis Arguments:     df: dataframe containing the data to analyze     alpha: significance level     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_confidence_interval(\n    self, df: pd.DataFrame, alpha: float, verbose: bool = False\n) -&gt; ConfidenceInterval:\n    \"\"\"Returns the confidence interval of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_gee = self.fit_gee(df)\n    # Extract the confidence interval for the treatment column\n    conf_int_df = results_gee.conf_int(alpha=alpha)\n    lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n    if verbose:\n        print(results_gee.summary())\n\n    # Return the confidence interval\n    return ConfidenceInterval(lower=lower_bound, upper=upper_bound, alpha=alpha)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.analysis_inference_results","title":"<code>analysis_inference_results(df, alpha, verbose=False)</code>","text":"<p>Returns the inference results of the analysis Arguments:     df: dataframe containing the data to analyze     alpha: significance level     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_inference_results(\n    self, df: pd.DataFrame, alpha: float, verbose: bool = False\n) -&gt; InferenceResults:\n    \"\"\"Returns the inference results of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_gee = self.fit_gee(df)\n\n    std_error = results_gee.bse[self.treatment_col]\n    ate = results_gee.params[self.treatment_col]\n    p_value = self.pvalue_based_on_hypothesis(results_gee)\n\n    # Extract the confidence interval for the treatment column\n    conf_int_df = results_gee.conf_int(alpha=alpha)\n    lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n    if verbose:\n        print(results_gee.summary())\n\n    # Return the confidence interval\n    return InferenceResults(\n        ate=ate,\n        p_value=p_value,\n        std_error=std_error,\n        conf_int=ConfidenceInterval(\n            lower=lower_bound, upper=upper_bound, alpha=alpha\n        ),\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.analysis_point_estimate","title":"<code>analysis_point_estimate(df, verbose=False)</code>","text":"<p>Returns the point estimate of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_point_estimate(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the point estimate of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_gee = self.fit_gee(df)\n    return results_gee.params[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>","text":"<p>Returns the p-value of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the p-value of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_gee = self.fit_gee(df)\n    if verbose:\n        print(results_gee.summary())\n\n    p_value = self.pvalue_based_on_hypothesis(results_gee)\n    return p_value\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.analysis_standard_error","title":"<code>analysis_standard_error(df, verbose=False)</code>","text":"<p>Returns the standard error of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_standard_error(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the standard error of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_gee = self.fit_gee(df)\n    return results_gee.bse[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.GeeExperimentAnalysis.fit_gee","title":"<code>fit_gee(df)</code>","text":"<p>Returns the fitted GEE model</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def fit_gee(self, df: pd.DataFrame) -&gt; sm.GEE:\n    \"\"\"Returns the fitted GEE model\"\"\"\n    if self.add_covariate_interaction:\n        df = self._add_interaction_covariates(df)\n    return sm.GEE.from_formula(\n        self.formula,\n        data=df,\n        groups=self._get_cluster_column(df),\n        family=self.fam,\n        cov_struct=self.va,\n    ).fit()\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.InferenceResults","title":"<code>InferenceResults</code>  <code>dataclass</code>","text":"<p>Class to define the structure of complete statistical analysis results.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@dataclass\nclass InferenceResults:\n    \"\"\"\n    Class to define the structure of complete statistical analysis results.\n    \"\"\"\n\n    ate: float\n    p_value: float\n    std_error: float\n    conf_int: ConfidenceInterval\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.MLMExperimentAnalysis","title":"<code>MLMExperimentAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> <p>Class to run Mixed Linear Models clustered analysis</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates</p> <code>None</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import MLMExperimentAnalysis\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 0, 0, 1],\n    'treatment': [\"A\"] * 3 + [\"B\"] * 3,\n    'cluster': [1] * 6,\n})\n\nMLMExperimentAnalysis(\n    cluster_cols=['cluster'],\n    target_col='x',\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class MLMExperimentAnalysis(ExperimentAnalysis):\n    \"\"\"\n    Class to run Mixed Linear Models clustered analysis\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        covariates: list of columns to use as covariates\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import MLMExperimentAnalysis\n    import pandas as pd\n\n    df = pd.DataFrame({\n        'x': [1, 2, 3, 0, 0, 1],\n        'treatment': [\"A\"] * 3 + [\"B\"] * 3,\n        'cluster': [1] * 6,\n    })\n\n    MLMExperimentAnalysis(\n        cluster_cols=['cluster'],\n        target_col='x',\n    ).get_pvalue(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        covariates: Optional[List[str]] = None,\n        hypothesis: str = \"two-sided\",\n        add_covariate_interaction: bool = False,\n    ):\n        super().__init__(\n            target_col=target_col,\n            treatment_col=treatment_col,\n            cluster_cols=cluster_cols,\n            treatment=treatment,\n            covariates=covariates,\n            hypothesis=hypothesis,\n            add_covariate_interaction=add_covariate_interaction,\n        )\n        self.re_formula = None\n        self.vc_formula = None\n\n    def fit_mlm(self, df: pd.DataFrame) -&gt; sm.MixedLM:\n        \"\"\"Returns the fitted MLM model\"\"\"\n        if self.add_covariate_interaction:\n            df = self._add_interaction_covariates(df)\n        return sm.MixedLM.from_formula(\n            formula=self.formula,\n            data=df,\n            groups=self._get_cluster_column(df),\n            re_formula=self.re_formula,\n            vc_formula=self.vc_formula,\n        ).fit()\n\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_mlm = self.fit_mlm(df)\n        if verbose:\n            print(results_mlm.summary())\n\n        p_value = self.pvalue_based_on_hypothesis(results_mlm)\n        return p_value\n\n    def analysis_point_estimate(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the point estimate of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_mlm = self.fit_mlm(df)\n        return results_mlm.params[self.treatment_col]\n\n    def analysis_standard_error(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the standard error of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_mlm = self.fit_mlm(df)\n        return results_mlm.bse[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.MLMExperimentAnalysis.analysis_point_estimate","title":"<code>analysis_point_estimate(df, verbose=False)</code>","text":"<p>Returns the point estimate of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_point_estimate(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the point estimate of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_mlm = self.fit_mlm(df)\n    return results_mlm.params[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.MLMExperimentAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>","text":"<p>Returns the p-value of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the p-value of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_mlm = self.fit_mlm(df)\n    if verbose:\n        print(results_mlm.summary())\n\n    p_value = self.pvalue_based_on_hypothesis(results_mlm)\n    return p_value\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.MLMExperimentAnalysis.analysis_standard_error","title":"<code>analysis_standard_error(df, verbose=False)</code>","text":"<p>Returns the standard error of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_standard_error(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the standard error of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_mlm = self.fit_mlm(df)\n    return results_mlm.bse[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.MLMExperimentAnalysis.fit_mlm","title":"<code>fit_mlm(df)</code>","text":"<p>Returns the fitted MLM model</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def fit_mlm(self, df: pd.DataFrame) -&gt; sm.MixedLM:\n    \"\"\"Returns the fitted MLM model\"\"\"\n    if self.add_covariate_interaction:\n        df = self._add_interaction_covariates(df)\n    return sm.MixedLM.from_formula(\n        formula=self.formula,\n        data=df,\n        groups=self._get_cluster_column(df),\n        re_formula=self.re_formula,\n        vc_formula=self.vc_formula,\n    ).fit()\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis","title":"<code>OLSAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> <p>Class to run OLS analysis</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates</p> <code>None</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> <code>cov_type</code> <code>Optional[Literal['nonrobust', 'fixed scale', 'HC0', 'HC1', 'HC2', 'HC3', 'HAC', 'hac-panel', 'hac-groupsum', 'cluster']]</code> <p>one of \"nonrobust\", \"fixed scale\", \"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HAC\", \"hac-panel\", \"hac-groupsum\", \"cluster\"</p> <code>None</code> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import OLSAnalysis\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 0, 0, 1],\n    'treatment': [\"A\"] * 3 + [\"B\"] * 3,\n})\n\nOLSAnalysis(\n    target_col='x',\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class OLSAnalysis(ExperimentAnalysis):\n    \"\"\"\n    Class to run OLS analysis\n\n    Arguments:\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        covariates: list of columns to use as covariates\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n        cov_type: one of \"nonrobust\", \"fixed scale\", \"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HAC\", \"hac-panel\", \"hac-groupsum\", \"cluster\"\n\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import OLSAnalysis\n    import pandas as pd\n\n    df = pd.DataFrame({\n        'x': [1, 2, 3, 0, 0, 1],\n        'treatment': [\"A\"] * 3 + [\"B\"] * 3,\n    })\n\n    OLSAnalysis(\n        target_col='x',\n    ).get_pvalue(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        covariates: Optional[List[str]] = None,\n        hypothesis: str = \"two-sided\",\n        cov_type: Optional[\n            Literal[\n                \"nonrobust\",\n                \"fixed scale\",\n                \"HC0\",\n                \"HC1\",\n                \"HC2\",\n                \"HC3\",\n                \"HAC\",\n                \"hac-panel\",\n                \"hac-groupsum\",\n                \"cluster\",\n            ]\n        ] = None,\n        add_covariate_interaction: bool = False,\n        relative_effect: bool = False,\n    ):\n        self.target_col = target_col\n        self.treatment = treatment\n        self.treatment_col = treatment_col\n        self.covariates = covariates or []\n        self.hypothesis = hypothesis\n        self.cov_type: Literal[\n            \"nonrobust\",\n            \"fixed scale\",\n            \"HC0\",\n            \"HC1\",\n            \"HC2\",\n            \"HC3\",\n            \"HAC\",\n            \"hac-panel\",\n            \"hac-groupsum\",\n            \"cluster\",\n        ] = (\n            \"HC3\" if cov_type is None else cov_type\n        )\n        self.add_covariate_interaction = add_covariate_interaction\n        self.relative_effect = relative_effect\n\n    def fit_ols(self, df: pd.DataFrame) -&gt; RegressionResultsProtocol:\n        \"\"\"Returns the fitted OLS model\"\"\"\n        if self.add_covariate_interaction:\n            df = self._add_interaction_covariates(df)\n\n        ols_fit = sm.OLS.from_formula(self.formula, data=df).fit(cov_type=self.cov_type)\n\n        # create point estimate, pvalue and std error transformation in case of relative effects\n        if self.relative_effect:\n            relative_ols_fit = LiftRegressionTransformer(\n                treatment_col=self.treatment_col\n            )\n            relative_ols_fit.fit(\n                ols=ols_fit, df=df, covariate_cols=self.covariates_list\n            )\n            return relative_ols_fit\n\n        return ols_fit\n\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_ols = self.fit_ols(df=df)\n        if verbose:\n            print(results_ols.summary())\n\n        p_value = self.pvalue_based_on_hypothesis(results_ols)\n        return p_value\n\n    def analysis_point_estimate(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the point estimate of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_ols = self.fit_ols(df=df)\n        return results_ols.params[self.treatment_col]\n\n    def analysis_standard_error(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the standard error of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_ols = self.fit_ols(df=df)\n        return results_ols.bse[self.treatment_col]\n\n    def analysis_confidence_interval(\n        self, df: pd.DataFrame, alpha: float, verbose: bool = False\n    ) -&gt; ConfidenceInterval:\n        \"\"\"Returns the confidence interval of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_ols = self.fit_ols(df)\n        # Extract the confidence interval for the treatment column\n        conf_int_df = results_ols.conf_int(alpha=alpha)\n        lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n        if verbose:\n            print(results_ols.summary())\n\n        # Return the confidence interval\n        return ConfidenceInterval(lower=lower_bound, upper=upper_bound, alpha=alpha)\n\n    def analysis_inference_results(\n        self, df: pd.DataFrame, alpha: float, verbose: bool = False\n    ) -&gt; InferenceResults:\n        \"\"\"Returns the inference results of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            alpha: significance level\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n        results_ols = self.fit_ols(df)\n\n        std_error = results_ols.bse[self.treatment_col]\n        ate = results_ols.params[self.treatment_col]\n        p_value = self.pvalue_based_on_hypothesis(results_ols)\n\n        # Extract the confidence interval for the treatment column\n        conf_int_df = results_ols.conf_int(alpha=alpha)\n        lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n        if verbose:\n            print(results_ols.summary())\n\n        # Return the confidence interval\n        return InferenceResults(\n            ate=ate,\n            p_value=p_value,\n            std_error=std_error,\n            conf_int=ConfidenceInterval(\n                lower=lower_bound, upper=upper_bound, alpha=alpha\n            ),\n        )\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates an OLSAnalysis object from a PowerConfig object\"\"\"\n        return cls(\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            covariates=config.covariates,\n            hypothesis=config.hypothesis,\n            cov_type=config.cov_type,\n            add_covariate_interaction=config.add_covariate_interaction,\n            relative_effect=config.relative_effect,\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.analysis_confidence_interval","title":"<code>analysis_confidence_interval(df, alpha, verbose=False)</code>","text":"<p>Returns the confidence interval of the analysis Arguments:     df: dataframe containing the data to analyze     alpha: significance level     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_confidence_interval(\n    self, df: pd.DataFrame, alpha: float, verbose: bool = False\n) -&gt; ConfidenceInterval:\n    \"\"\"Returns the confidence interval of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_ols = self.fit_ols(df)\n    # Extract the confidence interval for the treatment column\n    conf_int_df = results_ols.conf_int(alpha=alpha)\n    lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n    if verbose:\n        print(results_ols.summary())\n\n    # Return the confidence interval\n    return ConfidenceInterval(lower=lower_bound, upper=upper_bound, alpha=alpha)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.analysis_inference_results","title":"<code>analysis_inference_results(df, alpha, verbose=False)</code>","text":"<p>Returns the inference results of the analysis Arguments:     df: dataframe containing the data to analyze     alpha: significance level     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_inference_results(\n    self, df: pd.DataFrame, alpha: float, verbose: bool = False\n) -&gt; InferenceResults:\n    \"\"\"Returns the inference results of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        alpha: significance level\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_ols = self.fit_ols(df)\n\n    std_error = results_ols.bse[self.treatment_col]\n    ate = results_ols.params[self.treatment_col]\n    p_value = self.pvalue_based_on_hypothesis(results_ols)\n\n    # Extract the confidence interval for the treatment column\n    conf_int_df = results_ols.conf_int(alpha=alpha)\n    lower_bound, upper_bound = conf_int_df.loc[self.treatment_col]\n\n    if verbose:\n        print(results_ols.summary())\n\n    # Return the confidence interval\n    return InferenceResults(\n        ate=ate,\n        p_value=p_value,\n        std_error=std_error,\n        conf_int=ConfidenceInterval(\n            lower=lower_bound, upper=upper_bound, alpha=alpha\n        ),\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.analysis_point_estimate","title":"<code>analysis_point_estimate(df, verbose=False)</code>","text":"<p>Returns the point estimate of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_point_estimate(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the point estimate of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_ols = self.fit_ols(df=df)\n    return results_ols.params[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>","text":"<p>Returns the p-value of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the p-value of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_ols = self.fit_ols(df=df)\n    if verbose:\n        print(results_ols.summary())\n\n    p_value = self.pvalue_based_on_hypothesis(results_ols)\n    return p_value\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.analysis_standard_error","title":"<code>analysis_standard_error(df, verbose=False)</code>","text":"<p>Returns the standard error of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_standard_error(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the standard error of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n    results_ols = self.fit_ols(df=df)\n    return results_ols.bse[self.treatment_col]\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.fit_ols","title":"<code>fit_ols(df)</code>","text":"<p>Returns the fitted OLS model</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def fit_ols(self, df: pd.DataFrame) -&gt; RegressionResultsProtocol:\n    \"\"\"Returns the fitted OLS model\"\"\"\n    if self.add_covariate_interaction:\n        df = self._add_interaction_covariates(df)\n\n    ols_fit = sm.OLS.from_formula(self.formula, data=df).fit(cov_type=self.cov_type)\n\n    # create point estimate, pvalue and std error transformation in case of relative effects\n    if self.relative_effect:\n        relative_ols_fit = LiftRegressionTransformer(\n            treatment_col=self.treatment_col\n        )\n        relative_ols_fit.fit(\n            ols=ols_fit, df=df, covariate_cols=self.covariates_list\n        )\n        return relative_ols_fit\n\n    return ols_fit\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.OLSAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates an OLSAnalysis object from a PowerConfig object</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates an OLSAnalysis object from a PowerConfig object\"\"\"\n    return cls(\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        covariates=config.covariates,\n        hypothesis=config.hypothesis,\n        cov_type=config.cov_type,\n        add_covariate_interaction=config.add_covariate_interaction,\n        relative_effect=config.relative_effect,\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.PairedTTestClusteredAnalysis","title":"<code>PairedTTestClusteredAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> <p>Class to run paired T-test analysis on aggregated data</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>strata_cols</code> <code>List[str]</code> <p>list of index columns for paired t test. Should be a subset or equal to cluster_cols</p> required <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import PairedTTestClusteredAnalysis\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 0, 0, 1, 1],\n    'treatment': [\"A\", \"B\", \"A\", \"B\"] * 2,\n    'cluster': [1, 2, 3, 4, 1, 2, 3, 4],\n})\n\nPairedTTestClusteredAnalysis(\n    cluster_cols=['cluster'],\n    strata_cols=['cluster'],\n    target_col='x',\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class PairedTTestClusteredAnalysis(ExperimentAnalysis):\n    \"\"\"\n    Class to run paired T-test analysis on aggregated data\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        strata_cols: list of index columns for paired t test. Should be a subset or equal to cluster_cols\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import PairedTTestClusteredAnalysis\n    import pandas as pd\n\n    df = pd.DataFrame({\n        'x': [1, 2, 3, 4, 0, 0, 1, 1],\n        'treatment': [\"A\", \"B\", \"A\", \"B\"] * 2,\n        'cluster': [1, 2, 3, 4, 1, 2, 3, 4],\n    })\n\n    PairedTTestClusteredAnalysis(\n        cluster_cols=['cluster'],\n        strata_cols=['cluster'],\n        target_col='x',\n    ).get_pvalue(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        strata_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        hypothesis: str = \"two-sided\",\n    ):\n        self.strata_cols = strata_cols\n        self.target_col = target_col\n        self.treatment = treatment\n        self.treatment_col = treatment_col\n        self.cluster_cols = cluster_cols\n        self.hypothesis = hypothesis\n\n    def _preprocessing(self, df: pd.DataFrame, verbose: bool = False) -&gt; pd.DataFrame:\n        df_grouped = df.groupby(\n            self.cluster_cols + [self.treatment_col], as_index=False\n        )[self.target_col].mean()\n\n        n_control = df_grouped[self.treatment_col].value_counts()[0]\n        n_treatment = df_grouped[self.treatment_col].value_counts()[1]\n\n        if n_control != n_treatment:\n            logging.warning(\n                f\"groups don't have same number of observations, {n_treatment =} and  {n_control =}\"\n            )\n\n        assert all(\n            [x in self.cluster_cols for x in self.strata_cols]\n        ), f\"strata should be a subset or equal to cluster_cols ({self.cluster_cols = }, {self.strata_cols = })\"\n\n        df_pivot = df_grouped.pivot_table(\n            columns=self.treatment_col,\n            index=self.strata_cols,\n            values=self.target_col,\n        )\n\n        if df_pivot.isna().sum().sum() &gt; 0:\n            logging.warning(\n                f\"There are missing pairs for some clusters, removing the lonely ones: {df_pivot[df_pivot.isna().any(axis=1)].to_dict()}\"\n            )\n\n        if verbose:\n            print(f\"performing paired t test in this data \\n {df_pivot} \\n\")\n\n        df_pivot = df_pivot.dropna()\n\n        return df_pivot\n\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the extra info if True\n        \"\"\"\n        assert (\n            type(self.cluster_cols) is list\n        ), \"cluster_cols needs to be a list of strings (even with one element)\"\n        assert (\n            type(self.strata_cols) is list\n        ), \"strata_cols needs to be a list of strings (even with one element)\"\n\n        df_pivot = self._preprocessing(df=df)\n\n        t_test_results = ttest_rel(\n            df_pivot.iloc[:, 0], df_pivot.iloc[:, 1], alternative=self.hypothesis\n        )\n\n        if verbose:\n            print(f\"paired t test results: \\n {t_test_results} \\n\")\n\n        return t_test_results.pvalue\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a PairedTTestClusteredAnalysis object from a PowerConfig object\"\"\"\n        return cls(\n            cluster_cols=config.cluster_cols,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            strata_cols=config.strata_cols,\n            hypothesis=config.hypothesis,\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.PairedTTestClusteredAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>","text":"<p>Returns the p-value of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the extra info if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the p-value of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the extra info if True\n    \"\"\"\n    assert (\n        type(self.cluster_cols) is list\n    ), \"cluster_cols needs to be a list of strings (even with one element)\"\n    assert (\n        type(self.strata_cols) is list\n    ), \"strata_cols needs to be a list of strings (even with one element)\"\n\n    df_pivot = self._preprocessing(df=df)\n\n    t_test_results = ttest_rel(\n        df_pivot.iloc[:, 0], df_pivot.iloc[:, 1], alternative=self.hypothesis\n    )\n\n    if verbose:\n        print(f\"paired t test results: \\n {t_test_results} \\n\")\n\n    return t_test_results.pvalue\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.PairedTTestClusteredAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a PairedTTestClusteredAnalysis object from a PowerConfig object</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a PairedTTestClusteredAnalysis object from a PowerConfig object\"\"\"\n    return cls(\n        cluster_cols=config.cluster_cols,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        strata_cols=config.strata_cols,\n        hypothesis=config.hypothesis,\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis","title":"<code>SyntheticControlAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> <p>Class to run Synthetic control analysis. It expects only one treatment cluster.</p> <p>Arguments:</p> <pre><code>target_col (str): The name of the column containing the variable to measure.\ntreatment_col (str): The name of the column containing the treatment variable.\ntreatment (str): The name of the treatment to use as the treated group.\ncluster_cols (list): A list of columns to use as clusters.\nhypothesis (str): One of \"two-sided\", \"less\", \"greater\" indicating the hypothesis.\ntime_col (str): The name of the column containing the time data.\nintervention_date (str): The date when the intervention occurred.\n</code></pre> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import SyntheticControlAnalysis\nimport pandas as pd\nimport numpy as np\nfrom itertools import product\n\ndates = pd.date_range(\"2022-01-01\", \"2022-01-31\", freq=\"d\")\n\nusers = [f\"User {i}\" for i in range(10)]\n\n# Create a combination of each date with each user\ncombinations = list(product(users, dates))\n\ntarget_values = np.random.normal(0, 1, size=len(combinations))\n\ndf = pd.DataFrame(combinations, columns=[\"user\", \"date\"])\ndf[\"target\"] = target_values\n\ndf[\"treatment\"] = \"A\"\ndf.loc[(df[\"user\"] == \"User 5\"), \"treatment\"] = \"B\"\n\nSyntheticControlAnalysis(\n    cluster_cols=[\"user\"], time_col=\"date\", intervention_date=\"2022-01-15\"\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class SyntheticControlAnalysis(ExperimentAnalysis):\n    \"\"\"\n    Class to run Synthetic control analysis. It expects only one treatment cluster.\n\n    Arguments:\n\n        target_col (str): The name of the column containing the variable to measure.\n        treatment_col (str): The name of the column containing the treatment variable.\n        treatment (str): The name of the treatment to use as the treated group.\n        cluster_cols (list): A list of columns to use as clusters.\n        hypothesis (str): One of \"two-sided\", \"less\", \"greater\" indicating the hypothesis.\n        time_col (str): The name of the column containing the time data.\n        intervention_date (str): The date when the intervention occurred.\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import SyntheticControlAnalysis\n    import pandas as pd\n    import numpy as np\n    from itertools import product\n\n    dates = pd.date_range(\"2022-01-01\", \"2022-01-31\", freq=\"d\")\n\n    users = [f\"User {i}\" for i in range(10)]\n\n    # Create a combination of each date with each user\n    combinations = list(product(users, dates))\n\n    target_values = np.random.normal(0, 1, size=len(combinations))\n\n    df = pd.DataFrame(combinations, columns=[\"user\", \"date\"])\n    df[\"target\"] = target_values\n\n    df[\"treatment\"] = \"A\"\n    df.loc[(df[\"user\"] == \"User 5\"), \"treatment\"] = \"B\"\n\n    SyntheticControlAnalysis(\n        cluster_cols=[\"user\"], time_col=\"date\", intervention_date=\"2022-01-15\"\n    ).get_pvalue(df)\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        intervention_date: str,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        hypothesis: str = \"two-sided\",\n        time_col: str = \"date\",\n    ):\n        super().__init__(\n            treatment=treatment,\n            target_col=target_col,\n            treatment_col=treatment_col,\n            hypothesis=hypothesis,\n            cluster_cols=cluster_cols,\n        )\n\n        self.time_col = time_col\n        self.intervention_date = intervention_date\n\n        if time_col in cluster_cols:\n            raise ValueError(\"time columns should not be in cluster columns\")\n\n    def _fit(self, pre_experiment_df: pd.DataFrame, verbose: bool) -&gt; np.ndarray:\n        \"\"\"Returns the weight of each donor\"\"\"\n\n        if not any(pre_experiment_df[self.treatment_col] == 1):\n            raise ValueError(\"No treatment unit found in the data.\")\n\n        X = (\n            pre_experiment_df.query(f\"{self.treatment_col} == 0\")\n            .pivot(index=self.cluster_cols, columns=self.time_col)[self.target_col]\n            .T\n        )\n\n        y = (\n            pre_experiment_df.query(f\"{self.treatment_col} == 1\")\n            .pivot(index=self.cluster_cols, columns=self.time_col)[self.target_col]\n            .T.iloc[:, 0]\n        )\n\n        weights = get_w(X, y, verbose)\n\n        return weights\n\n    def _predict(\n        self, df: pd.DataFrame, weights: np.ndarray, treatment_cluster: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        This method adds a column with the synthetic results and filter only the treatment unit.\n\n        First, it calculates the weights of each donor in the control group using the `fit_synthetic` method.\n        It then uses these weights to create a synthetic control group that closely matches the treatment unit before the intervention.\n        The synthetic control group is added to the treatment unit in the dataframe.\n        \"\"\"\n        synthetic = (\n            df[self._get_cluster_column(df) != treatment_cluster]\n            .pivot(index=self.time_col, columns=self.cluster_cols)[self.target_col]\n            .values.dot(weights)\n        )\n\n        # add synthetic to treatment cluster\n        return df[self._get_cluster_column(df) == treatment_cluster].assign(\n            synthetic=synthetic\n        )\n\n    def fit_predict_synthetic(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: pd.DataFrame,\n        treatment_cluster: str,\n        verbose: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Fit the synthetic control model and predict the results for the treatment cluster.\n        Args:\n            df: The dataframe containing the data after the intervention.\n            pre_experiment_df: The dataframe containing the data before the intervention.\n            treatment_cluster: The name of the treatment cluster.\n            verbose: If True, print the status of the optimization of weights.\n\n        Returns:\n            The dataframe with the synthetic results added to the treatment cluster.\n        \"\"\"\n        weights = self._fit(pre_experiment_df=pre_experiment_df, verbose=verbose)\n\n        prediction = self._predict(\n            df=df, weights=weights, treatment_cluster=treatment_cluster\n        )\n        return prediction\n\n    def pvalue_based_on_hypothesis(\n        self, ate: np.float64, avg_effects: Dict[str, float]\n    ) -&gt; float:\n        \"\"\"\n        Returns the p-value of the analysis.\n        1. Count how many times the average effect is greater than the real treatment unit\n        2. Average it with the number of units. The result is the p-value using Fisher permutation exact test.\n        \"\"\"\n\n        avg_effects = list(avg_effects.values())\n\n        if HypothesisEntries(self.hypothesis) == HypothesisEntries.LESS:\n            return np.mean(avg_effects &lt; ate)\n        if HypothesisEntries(self.hypothesis) == HypothesisEntries.GREATER:\n            return np.mean(avg_effects &gt; ate)\n        if HypothesisEntries(self.hypothesis) == HypothesisEntries.TWO_SIDED:\n            avg_effects = np.abs(avg_effects)\n            return np.mean(avg_effects &gt; ate)\n\n        raise ValueError(f\"{self.hypothesis} is not a valid HypothesisEntries\")\n\n    def _get_treatment_cluster(self, df: pd.DataFrame) -&gt; str:\n        \"\"\"Returns the first treatment cluster. The current implementation of Synthetic Control only accepts one treatment cluster.\n        This will be left inside Synthetic class because it doesn't apply for other analyses\n        \"\"\"\n        treatment_df = df[df[self.treatment_col] == 1]\n        treatment_cluster = self._get_cluster_column(treatment_df).unique()[0]\n        return treatment_cluster\n\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"\n        Returns the p-value of the analysis.\n        1. Calculate the average effect after intervention for each unit.\n        2. Count how many times the average effect is greater than the real treatment unit\n        3. Average it with the number of units. The result is the p-value using Fisher permutation test\n        \"\"\"\n\n        clusters = self._get_cluster_column(df).unique()\n        treatment_cluster = self._get_treatment_cluster(df)\n\n        synthetic_donors = {\n            cluster: self.analysis_point_estimate(\n                treatment_cluster=cluster,\n                df=df,\n                verbose=verbose,\n            )\n            for cluster in clusters\n        }\n\n        ate = synthetic_donors[treatment_cluster]\n        synthetic_donors.pop(treatment_cluster)\n\n        return self.pvalue_based_on_hypothesis(ate=ate, avg_effects=synthetic_donors)\n\n    def analysis_point_estimate(\n        self,\n        df: pd.DataFrame,\n        treatment_cluster: Optional[str] = None,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Calculate the point estimate for the treatment effect for a specified cluster by averaging across the time windows.\n        \"\"\"\n        df, pre_experiment_df = self._split_pre_experiment_df(df)\n\n        if treatment_cluster is None:\n            treatment_cluster = self._get_treatment_cluster(df)\n\n        df = self.fit_predict_synthetic(\n            df, pre_experiment_df, treatment_cluster, verbose=verbose\n        )\n\n        df[\"effect\"] = df[self.target_col] - df[\"synthetic\"]\n        avg_effect = df[\"effect\"].mean()\n        return avg_effect\n\n    def _split_pre_experiment_df(self, df: pd.DataFrame):\n        \"\"\"Split the dataframe into pre-experiment and experiment dataframes\"\"\"\n        pre_experiment_df = df[(df[self.time_col] &lt;= self.intervention_date)]\n        df = df[(df[self.time_col] &gt; self.intervention_date)]\n        return df, pre_experiment_df\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis._fit","title":"<code>_fit(pre_experiment_df, verbose)</code>","text":"<p>Returns the weight of each donor</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _fit(self, pre_experiment_df: pd.DataFrame, verbose: bool) -&gt; np.ndarray:\n    \"\"\"Returns the weight of each donor\"\"\"\n\n    if not any(pre_experiment_df[self.treatment_col] == 1):\n        raise ValueError(\"No treatment unit found in the data.\")\n\n    X = (\n        pre_experiment_df.query(f\"{self.treatment_col} == 0\")\n        .pivot(index=self.cluster_cols, columns=self.time_col)[self.target_col]\n        .T\n    )\n\n    y = (\n        pre_experiment_df.query(f\"{self.treatment_col} == 1\")\n        .pivot(index=self.cluster_cols, columns=self.time_col)[self.target_col]\n        .T.iloc[:, 0]\n    )\n\n    weights = get_w(X, y, verbose)\n\n    return weights\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis._get_treatment_cluster","title":"<code>_get_treatment_cluster(df)</code>","text":"<p>Returns the first treatment cluster. The current implementation of Synthetic Control only accepts one treatment cluster. This will be left inside Synthetic class because it doesn't apply for other analyses</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _get_treatment_cluster(self, df: pd.DataFrame) -&gt; str:\n    \"\"\"Returns the first treatment cluster. The current implementation of Synthetic Control only accepts one treatment cluster.\n    This will be left inside Synthetic class because it doesn't apply for other analyses\n    \"\"\"\n    treatment_df = df[df[self.treatment_col] == 1]\n    treatment_cluster = self._get_cluster_column(treatment_df).unique()[0]\n    return treatment_cluster\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis._predict","title":"<code>_predict(df, weights, treatment_cluster)</code>","text":"<p>This method adds a column with the synthetic results and filter only the treatment unit.</p> <p>First, it calculates the weights of each donor in the control group using the <code>fit_synthetic</code> method. It then uses these weights to create a synthetic control group that closely matches the treatment unit before the intervention. The synthetic control group is added to the treatment unit in the dataframe.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _predict(\n    self, df: pd.DataFrame, weights: np.ndarray, treatment_cluster: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    This method adds a column with the synthetic results and filter only the treatment unit.\n\n    First, it calculates the weights of each donor in the control group using the `fit_synthetic` method.\n    It then uses these weights to create a synthetic control group that closely matches the treatment unit before the intervention.\n    The synthetic control group is added to the treatment unit in the dataframe.\n    \"\"\"\n    synthetic = (\n        df[self._get_cluster_column(df) != treatment_cluster]\n        .pivot(index=self.time_col, columns=self.cluster_cols)[self.target_col]\n        .values.dot(weights)\n    )\n\n    # add synthetic to treatment cluster\n    return df[self._get_cluster_column(df) == treatment_cluster].assign(\n        synthetic=synthetic\n    )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis._split_pre_experiment_df","title":"<code>_split_pre_experiment_df(df)</code>","text":"<p>Split the dataframe into pre-experiment and experiment dataframes</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def _split_pre_experiment_df(self, df: pd.DataFrame):\n    \"\"\"Split the dataframe into pre-experiment and experiment dataframes\"\"\"\n    pre_experiment_df = df[(df[self.time_col] &lt;= self.intervention_date)]\n    df = df[(df[self.time_col] &gt; self.intervention_date)]\n    return df, pre_experiment_df\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis.analysis_point_estimate","title":"<code>analysis_point_estimate(df, treatment_cluster=None, verbose=False)</code>","text":"<p>Calculate the point estimate for the treatment effect for a specified cluster by averaging across the time windows.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_point_estimate(\n    self,\n    df: pd.DataFrame,\n    treatment_cluster: Optional[str] = None,\n    verbose: bool = False,\n):\n    \"\"\"\n    Calculate the point estimate for the treatment effect for a specified cluster by averaging across the time windows.\n    \"\"\"\n    df, pre_experiment_df = self._split_pre_experiment_df(df)\n\n    if treatment_cluster is None:\n        treatment_cluster = self._get_treatment_cluster(df)\n\n    df = self.fit_predict_synthetic(\n        df, pre_experiment_df, treatment_cluster, verbose=verbose\n    )\n\n    df[\"effect\"] = df[self.target_col] - df[\"synthetic\"]\n    avg_effect = df[\"effect\"].mean()\n    return avg_effect\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>","text":"<p>Returns the p-value of the analysis. 1. Calculate the average effect after intervention for each unit. 2. Count how many times the average effect is greater than the real treatment unit 3. Average it with the number of units. The result is the p-value using Fisher permutation test</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"\n    Returns the p-value of the analysis.\n    1. Calculate the average effect after intervention for each unit.\n    2. Count how many times the average effect is greater than the real treatment unit\n    3. Average it with the number of units. The result is the p-value using Fisher permutation test\n    \"\"\"\n\n    clusters = self._get_cluster_column(df).unique()\n    treatment_cluster = self._get_treatment_cluster(df)\n\n    synthetic_donors = {\n        cluster: self.analysis_point_estimate(\n            treatment_cluster=cluster,\n            df=df,\n            verbose=verbose,\n        )\n        for cluster in clusters\n    }\n\n    ate = synthetic_donors[treatment_cluster]\n    synthetic_donors.pop(treatment_cluster)\n\n    return self.pvalue_based_on_hypothesis(ate=ate, avg_effects=synthetic_donors)\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis.fit_predict_synthetic","title":"<code>fit_predict_synthetic(df, pre_experiment_df, treatment_cluster, verbose=False)</code>","text":"<p>Fit the synthetic control model and predict the results for the treatment cluster. Args:     df: The dataframe containing the data after the intervention.     pre_experiment_df: The dataframe containing the data before the intervention.     treatment_cluster: The name of the treatment cluster.     verbose: If True, print the status of the optimization of weights.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataframe with the synthetic results added to the treatment cluster.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def fit_predict_synthetic(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: pd.DataFrame,\n    treatment_cluster: str,\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit the synthetic control model and predict the results for the treatment cluster.\n    Args:\n        df: The dataframe containing the data after the intervention.\n        pre_experiment_df: The dataframe containing the data before the intervention.\n        treatment_cluster: The name of the treatment cluster.\n        verbose: If True, print the status of the optimization of weights.\n\n    Returns:\n        The dataframe with the synthetic results added to the treatment cluster.\n    \"\"\"\n    weights = self._fit(pre_experiment_df=pre_experiment_df, verbose=verbose)\n\n    prediction = self._predict(\n        df=df, weights=weights, treatment_cluster=treatment_cluster\n    )\n    return prediction\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.SyntheticControlAnalysis.pvalue_based_on_hypothesis","title":"<code>pvalue_based_on_hypothesis(ate, avg_effects)</code>","text":"<p>Returns the p-value of the analysis. 1. Count how many times the average effect is greater than the real treatment unit 2. Average it with the number of units. The result is the p-value using Fisher permutation exact test.</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def pvalue_based_on_hypothesis(\n    self, ate: np.float64, avg_effects: Dict[str, float]\n) -&gt; float:\n    \"\"\"\n    Returns the p-value of the analysis.\n    1. Count how many times the average effect is greater than the real treatment unit\n    2. Average it with the number of units. The result is the p-value using Fisher permutation exact test.\n    \"\"\"\n\n    avg_effects = list(avg_effects.values())\n\n    if HypothesisEntries(self.hypothesis) == HypothesisEntries.LESS:\n        return np.mean(avg_effects &lt; ate)\n    if HypothesisEntries(self.hypothesis) == HypothesisEntries.GREATER:\n        return np.mean(avg_effects &gt; ate)\n    if HypothesisEntries(self.hypothesis) == HypothesisEntries.TWO_SIDED:\n        avg_effects = np.abs(avg_effects)\n        return np.mean(avg_effects &gt; ate)\n\n    raise ValueError(f\"{self.hypothesis} is not a valid HypothesisEntries\")\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.TTestClusteredAnalysis","title":"<code>TTestClusteredAnalysis</code>","text":"<p>               Bases: <code>ExperimentAnalysis</code></p> <p>Class to run T-test analysis on aggregated data</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>list of columns to use as clusters</p> required <code>target_col</code> <code>str</code> <p>name of the column containing the variable to measure</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>name of the column containing the treatment variable</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> <code>hypothesis</code> <code>str</code> <p>one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis</p> <code>'two-sided'</code> <p>Usage:</p> <pre><code>from cluster_experiments.experiment_analysis import TTestClusteredAnalysis\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 0, 0, 1, 1],\n    'treatment': [\"A\", \"B\", \"A\", \"B\"] * 2,\n    'cluster': [1, 2, 3, 4, 1, 2, 3, 4],\n})\n\nTTestClusteredAnalysis(\n    cluster_cols=['cluster'],\n    target_col='x',\n).get_pvalue(df)\n</code></pre> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>class TTestClusteredAnalysis(ExperimentAnalysis):\n    \"\"\"\n    Class to run T-test analysis on aggregated data\n\n    Arguments:\n        cluster_cols: list of columns to use as clusters\n        target_col: name of the column containing the variable to measure\n        treatment_col: name of the column containing the treatment variable\n        treatment: name of the treatment to use as the treated group\n        hypothesis: one of \"two-sided\", \"less\", \"greater\" indicating the alternative hypothesis\n\n    Usage:\n\n    ```python\n    from cluster_experiments.experiment_analysis import TTestClusteredAnalysis\n    import pandas as pd\n\n    df = pd.DataFrame({\n        'x': [1, 2, 3, 4, 0, 0, 1, 1],\n        'treatment': [\"A\", \"B\", \"A\", \"B\"] * 2,\n        'cluster': [1, 2, 3, 4, 1, 2, 3, 4],\n    })\n\n    TTestClusteredAnalysis(\n        cluster_cols=['cluster'],\n        target_col='x',\n    ).get_pvalue(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        hypothesis: str = \"two-sided\",\n    ):\n        self.target_col = target_col\n        self.treatment = treatment\n        self.treatment_col = treatment_col\n        self.cluster_cols = cluster_cols\n        self.hypothesis = hypothesis\n\n    def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n        \"\"\"Returns the p-value of the analysis\n        Arguments:\n            df: dataframe containing the data to analyze\n            verbose (Optional): bool, prints the regression summary if True\n        \"\"\"\n\n        df_grouped = df.groupby(\n            self.cluster_cols + [self.treatment_col], as_index=False\n        )[self.target_col].mean()\n\n        treatment_data = df_grouped.query(f\"{self.treatment_col} == 1\")[self.target_col]\n        control_data = df_grouped.query(f\"{self.treatment_col} == 0\")[self.target_col]\n        assert len(treatment_data), \"treatment data should have more than 1 cluster\"\n        assert len(control_data), \"control data should have more than 1 cluster\"\n        t_test_results = ttest_ind(\n            treatment_data, control_data, equal_var=False, alternative=self.hypothesis\n        )\n        return t_test_results.pvalue\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a TTestClusteredAnalysis object from a PowerConfig object\"\"\"\n        return cls(\n            cluster_cols=config.cluster_cols,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            hypothesis=config.hypothesis,\n        )\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.TTestClusteredAnalysis.analysis_pvalue","title":"<code>analysis_pvalue(df, verbose=False)</code>","text":"<p>Returns the p-value of the analysis Arguments:     df: dataframe containing the data to analyze     verbose (Optional): bool, prints the regression summary if True</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>def analysis_pvalue(self, df: pd.DataFrame, verbose: bool = False) -&gt; float:\n    \"\"\"Returns the p-value of the analysis\n    Arguments:\n        df: dataframe containing the data to analyze\n        verbose (Optional): bool, prints the regression summary if True\n    \"\"\"\n\n    df_grouped = df.groupby(\n        self.cluster_cols + [self.treatment_col], as_index=False\n    )[self.target_col].mean()\n\n    treatment_data = df_grouped.query(f\"{self.treatment_col} == 1\")[self.target_col]\n    control_data = df_grouped.query(f\"{self.treatment_col} == 0\")[self.target_col]\n    assert len(treatment_data), \"treatment data should have more than 1 cluster\"\n    assert len(control_data), \"control data should have more than 1 cluster\"\n    t_test_results = ttest_ind(\n        treatment_data, control_data, equal_var=False, alternative=self.hypothesis\n    )\n    return t_test_results.pvalue\n</code></pre>"},{"location":"api/experiment_analysis.html#cluster_experiments.experiment_analysis.TTestClusteredAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a TTestClusteredAnalysis object from a PowerConfig object</p> Source code in <code>cluster_experiments/experiment_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a TTestClusteredAnalysis object from a PowerConfig object\"\"\"\n    return cls(\n        cluster_cols=config.cluster_cols,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        hypothesis=config.hypothesis,\n    )\n</code></pre>"},{"location":"api/hypothesis_test.html","title":"<code>from cluster_experiments.inference.hypothesis_test import *</code>","text":""},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest","title":"<code>HypothesisTest</code>","text":"<p>A class used to represent a Hypothesis Test with a metric, analysis, optional analysis configuration, and optional dimensions.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest--attributes","title":"Attributes","text":"<p>metric : Metric     An instance of the Metric class analysis_type : str     string mapping to an ExperimentAnalysis class. Must be either in the built-in analysis_mapping or in the custom_analysis_type_mapper if provided. analysis_config : Optional[dict]     An optional dictionary representing the configuration for the analysis dimensions : Optional[List[Dimension]]     An optional list of Dimension instances cupac_config : Optional[dict]     An optional dictionary representing the configuration for the cupac model custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]     An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>class HypothesisTest:\n    \"\"\"\n    A class used to represent a Hypothesis Test with a metric, analysis, optional analysis configuration, and optional dimensions.\n\n    Attributes\n    ----------\n    metric : Metric\n        An instance of the Metric class\n    analysis_type : str\n        string mapping to an ExperimentAnalysis class. Must be either in the built-in analysis_mapping or in the custom_analysis_type_mapper if provided.\n    analysis_config : Optional[dict]\n        An optional dictionary representing the configuration for the analysis\n    dimensions : Optional[List[Dimension]]\n        An optional list of Dimension instances\n    cupac_config : Optional[dict]\n        An optional dictionary representing the configuration for the cupac model\n    custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]\n        An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: Metric,\n        analysis_type: str,\n        analysis_config: Optional[dict] = None,\n        dimensions: Optional[List[Dimension]] = None,\n        cupac_config: Optional[dict] = None,\n        custom_analysis_type_mapper: Optional[Dict[str, ExperimentAnalysis]] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        metric : Metric\n            An instance of the Metric class\n        analysis_type : str\n            string mapping to an ExperimentAnalysis class. Must be either in the built-in analysis_mapping or in the custom_analysis_type_mapper if provided.\n        analysis_config : Optional[dict]\n            An optional dictionary representing the configuration for the analysis\n        dimensions : Optional[List[Dimension]]\n            An optional list of Dimension instances\n        cupac_config : Optional[dict]\n            An optional dictionary representing the configuration for the cupac model\n        custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]\n            An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n        \"\"\"\n        self._validate_inputs(\n            metric,\n            analysis_type,\n            analysis_config,\n            dimensions,\n            cupac_config,\n            custom_analysis_type_mapper,\n        )\n        self.metric = metric\n        self.analysis_type = analysis_type\n        self.analysis_config = analysis_config or {}\n        self.dimensions = [DefaultDimension()] + (dimensions or [])\n        self.cupac_config = cupac_config or {}\n        self.custom_analysis_type_mapper = custom_analysis_type_mapper or {}\n\n        self.analysis_type_mapper = self.custom_analysis_type_mapper or analysis_mapping\n        self.analysis_class = self.analysis_type_mapper[self.analysis_type]\n        self.is_cupac = bool(cupac_config)\n        self.cupac_handler = (\n            CupacHandler(**self.cupac_config) if self.is_cupac else None\n        )\n        self.cupac_covariate_col = (\n            self.cupac_handler.cupac_outcome_name if self.is_cupac else None\n        )\n\n        self.new_analysis_config = None\n        self.experiment_analysis = None\n\n    @staticmethod\n    def _validate_inputs(\n        metric: Metric,\n        analysis_type: str,\n        analysis_config: Optional[dict],\n        dimensions: Optional[List[Dimension]],\n        cupac_config: Optional[dict] = None,\n        custom_analysis_type_mapper: Optional[Dict[str, ExperimentAnalysis]] = None,\n    ):\n        \"\"\"\n        Validates the inputs for the HypothesisTest class.\n\n        Parameters\n        ----------\n        metric : Metric\n            An instance of the Metric class\n        analysis_type : str\n            string mapper to an ExperimentAnalysis\n        analysis_config : Optional[dict]\n            An optional dictionary representing the configuration for the analysis\n        dimensions : Optional[List[Dimension]]\n            An optional list of Dimension instances\n        cupac_config : Optional[dict]\n            An optional dictionary representing the configuration for the cupac model\n        custom_analysis_type_mapper : Optional[dict[str, ExperimentAnalysis]]\n            An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n        \"\"\"\n        # Check if metric is a valid Metric instance\n        if not isinstance(metric, Metric):\n            raise TypeError(\"Metric must be an instance of Metric\")\n\n        # Check if analysis_type is a string\n        if not isinstance(analysis_type, str):\n            raise TypeError(\"Analysis must be a string\")\n\n        # Check if analysis_config is a dictionary when provided\n        if analysis_config is not None and not isinstance(analysis_config, dict):\n            raise TypeError(\"analysis_config must be a dictionary if provided\")\n\n        # Check if cupac_config is a dictionary when provided\n        if cupac_config is not None and not isinstance(cupac_config, dict):\n            raise TypeError(\"cupac_config must be a dictionary if provided\")\n\n        # Check if dimensions is a list of Dimension instances when provided\n        if dimensions is not None and (\n            not isinstance(dimensions, list)\n            or not all(isinstance(dim, Dimension) for dim in dimensions)\n        ):\n            raise TypeError(\n                f\"Dimensions must be a list of Dimension instances if provided, got {dimensions}\"\n            )\n\n        # Validate custom_analysis_type_mapper if provided\n        if custom_analysis_type_mapper:\n            # Ensure it's a dictionary\n            if not isinstance(custom_analysis_type_mapper, dict):\n                raise TypeError(\n                    \"custom_analysis_type_mapper must be a dictionary if provided\"\n                )\n\n            # Ensure all keys are strings and values are ExperimentAnalysis classes\n            for key, value in custom_analysis_type_mapper.items():\n                if not isinstance(key, str):\n                    raise TypeError(\n                        f\"Key '{key}' in custom_analysis_type_mapper must be a string\"\n                    )\n                if not issubclass(value, ExperimentAnalysis):\n                    raise TypeError(\n                        f\"Value '{value}' for key '{key}' in custom_analysis_type_mapper must be a subclass of ExperimentAnalysis\"\n                    )\n\n            # Ensure the analysis_type is in the custom mapper if a custom mapper is provided\n            if analysis_type not in custom_analysis_type_mapper:\n                raise ValueError(\n                    f\"Analysis type '{analysis_type}' not found in the provided custom_analysis_type_mapper\"\n                )\n\n        # If no custom_analysis_type_mapper, check if analysis_type exists in the default mapping\n        elif analysis_type not in analysis_mapping:\n            raise ValueError(\n                f\"Analysis type '{analysis_type}' not found in analysis_mapping\"\n            )\n\n        # If a RatioMetric is provided, the only analysis_type allowed is 'delta'\n        if isinstance(metric, RatioMetric) and analysis_type != \"delta\":\n            raise ValueError(\"RatioMetric can only be used with analysis_type 'delta'\")\n\n    def get_inference_results(self, df: pd.DataFrame, alpha: float) -&gt; InferenceResults:\n        \"\"\"\n        Performs inference analysis on the provided DataFrame using the analysis class.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            The dataframe containing the data for analysis.\n        alpha : float\n            The significance level to be used in the inference analysis.\n\n        Returns\n        -------\n        InferenceResults\n            The results containing the statistics of the inference procedure.\n        \"\"\"\n\n        self.experiment_analysis = self.analysis_class(**self.new_analysis_config)\n        inference_results = self.experiment_analysis.get_inference_results(\n            df=df, alpha=alpha\n        )\n\n        return inference_results\n\n    def _prepare_analysis_config(self, treatment_col: str, treatment: str) -&gt; None:\n        \"\"\"\n        Extends the analysis_config provided by the user, by adding or overriding the following keys:\n        - target_col\n        - treatment_col\n        - treatment\n\n        Also handles cupac covariate.\n\n        Returns\n        -------\n        dict\n            The prepared analysis configuration, ready to be ingested by the experiment analysis class\n        \"\"\"\n        new_analysis_config = copy.deepcopy(self.analysis_config)\n\n        new_analysis_config[\"target_col\"] = self.metric.target_column\n        new_analysis_config[\"treatment_col\"] = treatment_col\n        new_analysis_config[\"treatment\"] = treatment\n        if self.metric.scale_column:\n            new_analysis_config[\"scale_col\"] = self.metric.scale_column\n\n        covariates = new_analysis_config.get(\"covariates\", [])\n\n        if self.cupac_covariate_col and self.cupac_covariate_col not in covariates:\n            raise ValueError(\n                f\"You provided a cupac configuration but did not provide the cupac covariate called {self.cupac_covariate_col} in the analysis_config\"\n            )\n\n        self.new_analysis_config = new_analysis_config\n\n    @staticmethod\n    def prepare_data(\n        data: pd.DataFrame,\n        variant_col: str,\n        treatment_variant: Variant,\n        control_variant: Variant,\n        dimension_name: str,\n        dimension_value: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Prepares the data for the experiment analysis pipeline\n        \"\"\"\n        prepared_df = data.copy()\n\n        prepared_df = prepared_df.assign(__total_dimension=\"total\")\n\n        prepared_df = prepared_df.query(\n            f\"{variant_col}.isin(['{treatment_variant.name}','{control_variant.name}'])\"\n        ).query(f\"{dimension_name} == '{dimension_value}'\")\n\n        return prepared_df\n\n    def add_covariates(\n        self, exp_data: pd.DataFrame, pre_exp_data: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        If the test is a cupac test, adds the covariates to the experimental data.\n        \"\"\"\n        if self.is_cupac:\n            exp_data = self.cupac_handler.add_covariates(\n                df=exp_data, pre_experiment_df=pre_exp_data\n            )\n\n        return exp_data\n\n    def get_test_results(\n        self,\n        control_variant: Variant,\n        treatment_variant: Variant,\n        variant_col: str,\n        exp_data: pd.DataFrame,\n        dimension: Dimension,\n        dimension_value: str,\n        alpha: float,\n    ) -&gt; AnalysisPlanResults:\n        \"\"\"\n        Performs the hypothesis test on the provided data, for the given dimension value.\n\n        Parameters\n        ----------\n        control_variant : Variant\n            The control variant\n        treatment_variant : Variant\n            The treatment variant\n        variant_col : str\n            The column name representing the variant\n        exp_data : pd.DataFrame\n            The dataframe containing the data for analysis.\n        dimension : Dimension\n            The dimension instance\n        dimension_value : str\n            The value of the dimension\n        alpha : float\n            The significance level to be used in the inference analysis.\n\n        Returns\n        -------\n        AnalysisPlanResults\n            The results of the hypothesis test\n        \"\"\"\n        self._prepare_analysis_config(\n            treatment_col=variant_col,\n            treatment=treatment_variant.name,\n        )\n\n        prepared_df = self.prepare_data(\n            data=exp_data,\n            variant_col=variant_col,\n            treatment_variant=treatment_variant,\n            control_variant=control_variant,\n            dimension_name=dimension.name,\n            dimension_value=dimension_value,\n        )\n\n        inference_results = self.get_inference_results(df=prepared_df, alpha=alpha)\n\n        control_variant_mean = self.metric.get_mean(\n            prepared_df.query(f\"{variant_col}=='{control_variant.name}'\")\n        )\n        treatment_variant_mean = self.metric.get_mean(\n            prepared_df.query(f\"{variant_col}=='{treatment_variant.name}'\")\n        )\n\n        test_results = AnalysisPlanResults(\n            metric_alias=[self.metric.alias],\n            control_variant_name=[control_variant.name],\n            treatment_variant_name=[treatment_variant.name],\n            control_variant_mean=[control_variant_mean],\n            treatment_variant_mean=[treatment_variant_mean],\n            analysis_type=[self.analysis_type],\n            ate=[inference_results.ate],\n            ate_ci_lower=[inference_results.conf_int.lower],\n            ate_ci_upper=[inference_results.conf_int.upper],\n            p_value=[inference_results.p_value],\n            std_error=[inference_results.std_error],\n            dimension_name=[dimension.name],\n            dimension_value=[dimension_value],\n            alpha=[alpha],\n        )\n\n        return test_results\n\n    @classmethod\n    def from_config(cls, config: dict) -&gt; \"HypothesisTest\":\n        \"\"\"\n        Class method to create an HypothesisTest instance from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : dict\n            A dictionary containing the configuration of the HypothesisTest\n\n        Returns\n        -------\n        Metric\n            A HypothesisTest instance\n        \"\"\"\n        metric = Metric.from_metrics_config(config[\"metric\"])\n        dimensions = [\n            Dimension.from_metrics_config(dimension_config)\n            for dimension_config in config.get(\"dimensions\", [])\n        ]\n        return cls(\n            metric=metric,\n            analysis_type=config[\"analysis_type\"],\n            analysis_config=config.get(\"analysis_config\"),\n            dimensions=dimensions,\n            cupac_config=config.get(\"cupac_config\"),\n            custom_analysis_type_mapper=config.get(\"custom_analysis_type_mapper\"),\n        )\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.__init__","title":"<code>__init__(metric, analysis_type, analysis_config=None, dimensions=None, cupac_config=None, custom_analysis_type_mapper=None)</code>","text":""},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.__init__--parameters","title":"Parameters","text":"<p>metric : Metric     An instance of the Metric class analysis_type : str     string mapping to an ExperimentAnalysis class. Must be either in the built-in analysis_mapping or in the custom_analysis_type_mapper if provided. analysis_config : Optional[dict]     An optional dictionary representing the configuration for the analysis dimensions : Optional[List[Dimension]]     An optional list of Dimension instances cupac_config : Optional[dict]     An optional dictionary representing the configuration for the cupac model custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]     An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>def __init__(\n    self,\n    metric: Metric,\n    analysis_type: str,\n    analysis_config: Optional[dict] = None,\n    dimensions: Optional[List[Dimension]] = None,\n    cupac_config: Optional[dict] = None,\n    custom_analysis_type_mapper: Optional[Dict[str, ExperimentAnalysis]] = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    metric : Metric\n        An instance of the Metric class\n    analysis_type : str\n        string mapping to an ExperimentAnalysis class. Must be either in the built-in analysis_mapping or in the custom_analysis_type_mapper if provided.\n    analysis_config : Optional[dict]\n        An optional dictionary representing the configuration for the analysis\n    dimensions : Optional[List[Dimension]]\n        An optional list of Dimension instances\n    cupac_config : Optional[dict]\n        An optional dictionary representing the configuration for the cupac model\n    custom_analysis_type_mapper : Optional[Dict[str, ExperimentAnalysis]]\n        An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n    \"\"\"\n    self._validate_inputs(\n        metric,\n        analysis_type,\n        analysis_config,\n        dimensions,\n        cupac_config,\n        custom_analysis_type_mapper,\n    )\n    self.metric = metric\n    self.analysis_type = analysis_type\n    self.analysis_config = analysis_config or {}\n    self.dimensions = [DefaultDimension()] + (dimensions or [])\n    self.cupac_config = cupac_config or {}\n    self.custom_analysis_type_mapper = custom_analysis_type_mapper or {}\n\n    self.analysis_type_mapper = self.custom_analysis_type_mapper or analysis_mapping\n    self.analysis_class = self.analysis_type_mapper[self.analysis_type]\n    self.is_cupac = bool(cupac_config)\n    self.cupac_handler = (\n        CupacHandler(**self.cupac_config) if self.is_cupac else None\n    )\n    self.cupac_covariate_col = (\n        self.cupac_handler.cupac_outcome_name if self.is_cupac else None\n    )\n\n    self.new_analysis_config = None\n    self.experiment_analysis = None\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest._prepare_analysis_config","title":"<code>_prepare_analysis_config(treatment_col, treatment)</code>","text":"<p>Extends the analysis_config provided by the user, by adding or overriding the following keys: - target_col - treatment_col - treatment</p> <p>Also handles cupac covariate.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest._prepare_analysis_config--returns","title":"Returns","text":"<p>dict     The prepared analysis configuration, ready to be ingested by the experiment analysis class</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>def _prepare_analysis_config(self, treatment_col: str, treatment: str) -&gt; None:\n    \"\"\"\n    Extends the analysis_config provided by the user, by adding or overriding the following keys:\n    - target_col\n    - treatment_col\n    - treatment\n\n    Also handles cupac covariate.\n\n    Returns\n    -------\n    dict\n        The prepared analysis configuration, ready to be ingested by the experiment analysis class\n    \"\"\"\n    new_analysis_config = copy.deepcopy(self.analysis_config)\n\n    new_analysis_config[\"target_col\"] = self.metric.target_column\n    new_analysis_config[\"treatment_col\"] = treatment_col\n    new_analysis_config[\"treatment\"] = treatment\n    if self.metric.scale_column:\n        new_analysis_config[\"scale_col\"] = self.metric.scale_column\n\n    covariates = new_analysis_config.get(\"covariates\", [])\n\n    if self.cupac_covariate_col and self.cupac_covariate_col not in covariates:\n        raise ValueError(\n            f\"You provided a cupac configuration but did not provide the cupac covariate called {self.cupac_covariate_col} in the analysis_config\"\n        )\n\n    self.new_analysis_config = new_analysis_config\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest._validate_inputs","title":"<code>_validate_inputs(metric, analysis_type, analysis_config, dimensions, cupac_config=None, custom_analysis_type_mapper=None)</code>  <code>staticmethod</code>","text":"<p>Validates the inputs for the HypothesisTest class.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest._validate_inputs--parameters","title":"Parameters","text":"<p>metric : Metric     An instance of the Metric class analysis_type : str     string mapper to an ExperimentAnalysis analysis_config : Optional[dict]     An optional dictionary representing the configuration for the analysis dimensions : Optional[List[Dimension]]     An optional list of Dimension instances cupac_config : Optional[dict]     An optional dictionary representing the configuration for the cupac model custom_analysis_type_mapper : Optional[dict[str, ExperimentAnalysis]]     An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>@staticmethod\ndef _validate_inputs(\n    metric: Metric,\n    analysis_type: str,\n    analysis_config: Optional[dict],\n    dimensions: Optional[List[Dimension]],\n    cupac_config: Optional[dict] = None,\n    custom_analysis_type_mapper: Optional[Dict[str, ExperimentAnalysis]] = None,\n):\n    \"\"\"\n    Validates the inputs for the HypothesisTest class.\n\n    Parameters\n    ----------\n    metric : Metric\n        An instance of the Metric class\n    analysis_type : str\n        string mapper to an ExperimentAnalysis\n    analysis_config : Optional[dict]\n        An optional dictionary representing the configuration for the analysis\n    dimensions : Optional[List[Dimension]]\n        An optional list of Dimension instances\n    cupac_config : Optional[dict]\n        An optional dictionary representing the configuration for the cupac model\n    custom_analysis_type_mapper : Optional[dict[str, ExperimentAnalysis]]\n        An optional dictionary mapping the names of custom analysis types to the corresponding ExperimentAnalysis classes\n    \"\"\"\n    # Check if metric is a valid Metric instance\n    if not isinstance(metric, Metric):\n        raise TypeError(\"Metric must be an instance of Metric\")\n\n    # Check if analysis_type is a string\n    if not isinstance(analysis_type, str):\n        raise TypeError(\"Analysis must be a string\")\n\n    # Check if analysis_config is a dictionary when provided\n    if analysis_config is not None and not isinstance(analysis_config, dict):\n        raise TypeError(\"analysis_config must be a dictionary if provided\")\n\n    # Check if cupac_config is a dictionary when provided\n    if cupac_config is not None and not isinstance(cupac_config, dict):\n        raise TypeError(\"cupac_config must be a dictionary if provided\")\n\n    # Check if dimensions is a list of Dimension instances when provided\n    if dimensions is not None and (\n        not isinstance(dimensions, list)\n        or not all(isinstance(dim, Dimension) for dim in dimensions)\n    ):\n        raise TypeError(\n            f\"Dimensions must be a list of Dimension instances if provided, got {dimensions}\"\n        )\n\n    # Validate custom_analysis_type_mapper if provided\n    if custom_analysis_type_mapper:\n        # Ensure it's a dictionary\n        if not isinstance(custom_analysis_type_mapper, dict):\n            raise TypeError(\n                \"custom_analysis_type_mapper must be a dictionary if provided\"\n            )\n\n        # Ensure all keys are strings and values are ExperimentAnalysis classes\n        for key, value in custom_analysis_type_mapper.items():\n            if not isinstance(key, str):\n                raise TypeError(\n                    f\"Key '{key}' in custom_analysis_type_mapper must be a string\"\n                )\n            if not issubclass(value, ExperimentAnalysis):\n                raise TypeError(\n                    f\"Value '{value}' for key '{key}' in custom_analysis_type_mapper must be a subclass of ExperimentAnalysis\"\n                )\n\n        # Ensure the analysis_type is in the custom mapper if a custom mapper is provided\n        if analysis_type not in custom_analysis_type_mapper:\n            raise ValueError(\n                f\"Analysis type '{analysis_type}' not found in the provided custom_analysis_type_mapper\"\n            )\n\n    # If no custom_analysis_type_mapper, check if analysis_type exists in the default mapping\n    elif analysis_type not in analysis_mapping:\n        raise ValueError(\n            f\"Analysis type '{analysis_type}' not found in analysis_mapping\"\n        )\n\n    # If a RatioMetric is provided, the only analysis_type allowed is 'delta'\n    if isinstance(metric, RatioMetric) and analysis_type != \"delta\":\n        raise ValueError(\"RatioMetric can only be used with analysis_type 'delta'\")\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.add_covariates","title":"<code>add_covariates(exp_data, pre_exp_data)</code>","text":"<p>If the test is a cupac test, adds the covariates to the experimental data.</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>def add_covariates(\n    self, exp_data: pd.DataFrame, pre_exp_data: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    If the test is a cupac test, adds the covariates to the experimental data.\n    \"\"\"\n    if self.is_cupac:\n        exp_data = self.cupac_handler.add_covariates(\n            df=exp_data, pre_experiment_df=pre_exp_data\n        )\n\n    return exp_data\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Class method to create an HypothesisTest instance from a configuration dictionary.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.from_config--parameters","title":"Parameters","text":"<p>config : dict     A dictionary containing the configuration of the HypothesisTest</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.from_config--returns","title":"Returns","text":"<p>Metric     A HypothesisTest instance</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>@classmethod\ndef from_config(cls, config: dict) -&gt; \"HypothesisTest\":\n    \"\"\"\n    Class method to create an HypothesisTest instance from a configuration dictionary.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing the configuration of the HypothesisTest\n\n    Returns\n    -------\n    Metric\n        A HypothesisTest instance\n    \"\"\"\n    metric = Metric.from_metrics_config(config[\"metric\"])\n    dimensions = [\n        Dimension.from_metrics_config(dimension_config)\n        for dimension_config in config.get(\"dimensions\", [])\n    ]\n    return cls(\n        metric=metric,\n        analysis_type=config[\"analysis_type\"],\n        analysis_config=config.get(\"analysis_config\"),\n        dimensions=dimensions,\n        cupac_config=config.get(\"cupac_config\"),\n        custom_analysis_type_mapper=config.get(\"custom_analysis_type_mapper\"),\n    )\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.get_inference_results","title":"<code>get_inference_results(df, alpha)</code>","text":"<p>Performs inference analysis on the provided DataFrame using the analysis class.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.get_inference_results--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The dataframe containing the data for analysis. alpha : float     The significance level to be used in the inference analysis.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.get_inference_results--returns","title":"Returns","text":"<p>InferenceResults     The results containing the statistics of the inference procedure.</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>def get_inference_results(self, df: pd.DataFrame, alpha: float) -&gt; InferenceResults:\n    \"\"\"\n    Performs inference analysis on the provided DataFrame using the analysis class.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe containing the data for analysis.\n    alpha : float\n        The significance level to be used in the inference analysis.\n\n    Returns\n    -------\n    InferenceResults\n        The results containing the statistics of the inference procedure.\n    \"\"\"\n\n    self.experiment_analysis = self.analysis_class(**self.new_analysis_config)\n    inference_results = self.experiment_analysis.get_inference_results(\n        df=df, alpha=alpha\n    )\n\n    return inference_results\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.get_test_results","title":"<code>get_test_results(control_variant, treatment_variant, variant_col, exp_data, dimension, dimension_value, alpha)</code>","text":"<p>Performs the hypothesis test on the provided data, for the given dimension value.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.get_test_results--parameters","title":"Parameters","text":"<p>control_variant : Variant     The control variant treatment_variant : Variant     The treatment variant variant_col : str     The column name representing the variant exp_data : pd.DataFrame     The dataframe containing the data for analysis. dimension : Dimension     The dimension instance dimension_value : str     The value of the dimension alpha : float     The significance level to be used in the inference analysis.</p>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.get_test_results--returns","title":"Returns","text":"<p>AnalysisPlanResults     The results of the hypothesis test</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>def get_test_results(\n    self,\n    control_variant: Variant,\n    treatment_variant: Variant,\n    variant_col: str,\n    exp_data: pd.DataFrame,\n    dimension: Dimension,\n    dimension_value: str,\n    alpha: float,\n) -&gt; AnalysisPlanResults:\n    \"\"\"\n    Performs the hypothesis test on the provided data, for the given dimension value.\n\n    Parameters\n    ----------\n    control_variant : Variant\n        The control variant\n    treatment_variant : Variant\n        The treatment variant\n    variant_col : str\n        The column name representing the variant\n    exp_data : pd.DataFrame\n        The dataframe containing the data for analysis.\n    dimension : Dimension\n        The dimension instance\n    dimension_value : str\n        The value of the dimension\n    alpha : float\n        The significance level to be used in the inference analysis.\n\n    Returns\n    -------\n    AnalysisPlanResults\n        The results of the hypothesis test\n    \"\"\"\n    self._prepare_analysis_config(\n        treatment_col=variant_col,\n        treatment=treatment_variant.name,\n    )\n\n    prepared_df = self.prepare_data(\n        data=exp_data,\n        variant_col=variant_col,\n        treatment_variant=treatment_variant,\n        control_variant=control_variant,\n        dimension_name=dimension.name,\n        dimension_value=dimension_value,\n    )\n\n    inference_results = self.get_inference_results(df=prepared_df, alpha=alpha)\n\n    control_variant_mean = self.metric.get_mean(\n        prepared_df.query(f\"{variant_col}=='{control_variant.name}'\")\n    )\n    treatment_variant_mean = self.metric.get_mean(\n        prepared_df.query(f\"{variant_col}=='{treatment_variant.name}'\")\n    )\n\n    test_results = AnalysisPlanResults(\n        metric_alias=[self.metric.alias],\n        control_variant_name=[control_variant.name],\n        treatment_variant_name=[treatment_variant.name],\n        control_variant_mean=[control_variant_mean],\n        treatment_variant_mean=[treatment_variant_mean],\n        analysis_type=[self.analysis_type],\n        ate=[inference_results.ate],\n        ate_ci_lower=[inference_results.conf_int.lower],\n        ate_ci_upper=[inference_results.conf_int.upper],\n        p_value=[inference_results.p_value],\n        std_error=[inference_results.std_error],\n        dimension_name=[dimension.name],\n        dimension_value=[dimension_value],\n        alpha=[alpha],\n    )\n\n    return test_results\n</code></pre>"},{"location":"api/hypothesis_test.html#cluster_experiments.inference.hypothesis_test.HypothesisTest.prepare_data","title":"<code>prepare_data(data, variant_col, treatment_variant, control_variant, dimension_name, dimension_value)</code>  <code>staticmethod</code>","text":"<p>Prepares the data for the experiment analysis pipeline</p> Source code in <code>cluster_experiments/inference/hypothesis_test.py</code> <pre><code>@staticmethod\ndef prepare_data(\n    data: pd.DataFrame,\n    variant_col: str,\n    treatment_variant: Variant,\n    control_variant: Variant,\n    dimension_name: str,\n    dimension_value: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Prepares the data for the experiment analysis pipeline\n    \"\"\"\n    prepared_df = data.copy()\n\n    prepared_df = prepared_df.assign(__total_dimension=\"total\")\n\n    prepared_df = prepared_df.query(\n        f\"{variant_col}.isin(['{treatment_variant.name}','{control_variant.name}'])\"\n    ).query(f\"{dimension_name} == '{dimension_value}'\")\n\n    return prepared_df\n</code></pre>"},{"location":"api/metric.html","title":"<code>from cluster_experiments.inference.metric import *</code>","text":""},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class used to represent a Metric with an alias.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric--attributes","title":"Attributes","text":"<p>alias : str     A string representing the alias of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>class Metric(ABC):\n    \"\"\"\n    An abstract base class used to represent a Metric with an alias.\n\n    Attributes\n    ----------\n    alias : str\n        A string representing the alias of the metric\n    \"\"\"\n\n    def __init__(self, alias: str):\n        \"\"\"\n        Parameters\n        ----------\n        alias : str\n            The alias of the metric\n        \"\"\"\n        self.alias = alias\n        self._validate_alias()\n\n    def _validate_alias(self):\n        \"\"\"\n        Validates the alias input for the Metric class.\n\n        Raises\n        ------\n        TypeError\n            If the alias is not a string\n        \"\"\"\n        if not isinstance(self.alias, str):\n            raise TypeError(\"Metric alias must be a string\")\n\n    @property\n    @abstractmethod\n    def target_column(self) -&gt; str:\n        \"\"\"\n        Abstract property to return the target column to feed the experiment analysis class, from the metric definition.\n\n        Returns\n        -------\n        str\n            The target column name\n        \"\"\"\n        pass\n\n    @property\n    def scale_column(self) -&gt; Optional[str]:\n        \"\"\"\n        Abstract property to return the scale column to feed the experiment analysis class, from the metric definition.\n\n        Returns\n        -------\n        str\n            The scale column name\n        \"\"\"\n        return None\n\n    @abstractmethod\n    def get_mean(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"\n        Abstract method to return the mean value of the metric, given a dataframe.\n\n        Returns\n        -------\n        float\n            The mean value of the metric\n        \"\"\"\n        pass\n\n    @classmethod\n    def from_metrics_config(cls, config: dict) -&gt; \"Metric\":\n        \"\"\"\n        Class method to create a Metric instance from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : dict\n            A dictionary containing the configuration of the metric\n\n        Returns\n        -------\n        Metric\n            A Metric instance\n        \"\"\"\n        if \"numerator_name\" in config:\n            return RatioMetric.from_metrics_config(config)\n        return SimpleMetric.from_metrics_config(config)\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.scale_column","title":"<code>scale_column</code>  <code>property</code>","text":"<p>Abstract property to return the scale column to feed the experiment analysis class, from the metric definition.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.scale_column--returns","title":"Returns","text":"<p>str     The scale column name</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.target_column","title":"<code>target_column</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Abstract property to return the target column to feed the experiment analysis class, from the metric definition.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.target_column--returns","title":"Returns","text":"<p>str     The target column name</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.__init__","title":"<code>__init__(alias)</code>","text":""},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.__init__--parameters","title":"Parameters","text":"<p>alias : str     The alias of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def __init__(self, alias: str):\n    \"\"\"\n    Parameters\n    ----------\n    alias : str\n        The alias of the metric\n    \"\"\"\n    self.alias = alias\n    self._validate_alias()\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric._validate_alias","title":"<code>_validate_alias()</code>","text":"<p>Validates the alias input for the Metric class.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric._validate_alias--raises","title":"Raises","text":"<p>TypeError     If the alias is not a string</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def _validate_alias(self):\n    \"\"\"\n    Validates the alias input for the Metric class.\n\n    Raises\n    ------\n    TypeError\n        If the alias is not a string\n    \"\"\"\n    if not isinstance(self.alias, str):\n        raise TypeError(\"Metric alias must be a string\")\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.from_metrics_config","title":"<code>from_metrics_config(config)</code>  <code>classmethod</code>","text":"<p>Class method to create a Metric instance from a configuration dictionary.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.from_metrics_config--parameters","title":"Parameters","text":"<p>config : dict     A dictionary containing the configuration of the metric</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.from_metrics_config--returns","title":"Returns","text":"<p>Metric     A Metric instance</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>@classmethod\ndef from_metrics_config(cls, config: dict) -&gt; \"Metric\":\n    \"\"\"\n    Class method to create a Metric instance from a configuration dictionary.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing the configuration of the metric\n\n    Returns\n    -------\n    Metric\n        A Metric instance\n    \"\"\"\n    if \"numerator_name\" in config:\n        return RatioMetric.from_metrics_config(config)\n    return SimpleMetric.from_metrics_config(config)\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.get_mean","title":"<code>get_mean(df)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to return the mean value of the metric, given a dataframe.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.Metric.get_mean--returns","title":"Returns","text":"<p>float     The mean value of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>@abstractmethod\ndef get_mean(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Abstract method to return the mean value of the metric, given a dataframe.\n\n    Returns\n    -------\n    float\n        The mean value of the metric\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric","title":"<code>RatioMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>A class used to represent a Ratio Metric with an alias, a numerator name, and a denominator name. To be used when the metric is defined at a lower level than the data used for the analysis.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric--example","title":"Example","text":"<p>In a clustered experiment the participants were randomised based on their country of residence. The metric of interest is the salary of each participant. If the dataset fed into the analysis is at country-level, then a RatioMetric must be used: the numerator would be the sum of all salaries in the country, the denominator would be the number of participants in the country.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric--attributes","title":"Attributes","text":"<p>alias : str     A string representing the alias of the metric numerator_name : str     A string representing the numerator name of the metric denominator_name : str     A string representing the denominator name of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>class RatioMetric(Metric):\n    \"\"\"\n    A class used to represent a Ratio Metric with an alias, a numerator name, and a denominator name.\n    To be used when the metric is defined at a lower level than the data used for the analysis.\n\n    Example\n    ----------\n    In a clustered experiment the participants were randomised based on their country of residence.\n    The metric of interest is the salary of each participant. If the dataset fed into the analysis is at country-level,\n    then a RatioMetric must be used: the numerator would be the sum of all salaries in the country,\n    the denominator would be the number of participants in the country.\n\n    Attributes\n    ----------\n    alias : str\n        A string representing the alias of the metric\n    numerator_name : str\n        A string representing the numerator name of the metric\n    denominator_name : str\n        A string representing the denominator name of the metric\n    \"\"\"\n\n    def __init__(self, alias: str, numerator_name: str, denominator_name: str):\n        \"\"\"\n        Parameters\n        ----------\n        alias : str\n            The alias of the metric\n        numerator_name : str\n            The numerator name of the metric\n        denominator_name : str\n            The denominator name of the metric\n        \"\"\"\n        super().__init__(alias)\n        self.numerator_name = numerator_name\n        self.denominator_name = denominator_name\n        self._validate_names()\n\n    def _validate_names(self):\n        \"\"\"\n        Validates the numerator and denominator names input for the RatioMetric class.\n\n        Raises\n        ------\n        TypeError\n            If the numerator or denominator names are not strings\n        \"\"\"\n        if not isinstance(self.numerator_name, str) or not isinstance(\n            self.denominator_name, str\n        ):\n            raise TypeError(\"RatioMetric names must be strings\")\n\n    @property\n    def target_column(self) -&gt; str:\n        \"\"\"\n        Returns the target column for the RatioMetric.\n\n        Returns\n        -------\n        str\n            The numerator name of the metric\n        \"\"\"\n        return self.numerator_name\n\n    @property\n    def scale_column(self) -&gt; str:\n        \"\"\"\n        Returns the scale column for the RatioMetric.\n\n        Returns\n        -------\n        str\n            The denominator name of the metric\n        \"\"\"\n        return self.denominator_name\n\n    def get_mean(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"\n        Returns the mean value of the metric, given a dataframe.\n\n        Returns\n        -------\n        float\n            The mean value of the metric\n        \"\"\"\n        return df[self.numerator_name].mean() / df[self.denominator_name].mean()\n\n    @classmethod\n    def from_metrics_config(cls, config: dict) -&gt; \"Metric\":\n        \"\"\"\n        Class method to create a RatioMetric instance from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : dict\n            A dictionary containing the configuration of the metric\n\n        Returns\n        -------\n        RatioMetric\n            A RatioMetric instance\n        \"\"\"\n        return cls(\n            alias=config[\"alias\"],\n            numerator_name=config[\"numerator_name\"],\n            denominator_name=config[\"denominator_name\"],\n        )\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.scale_column","title":"<code>scale_column</code>  <code>property</code>","text":"<p>Returns the scale column for the RatioMetric.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.scale_column--returns","title":"Returns","text":"<p>str     The denominator name of the metric</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.target_column","title":"<code>target_column</code>  <code>property</code>","text":"<p>Returns the target column for the RatioMetric.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.target_column--returns","title":"Returns","text":"<p>str     The numerator name of the metric</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.__init__","title":"<code>__init__(alias, numerator_name, denominator_name)</code>","text":""},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.__init__--parameters","title":"Parameters","text":"<p>alias : str     The alias of the metric numerator_name : str     The numerator name of the metric denominator_name : str     The denominator name of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def __init__(self, alias: str, numerator_name: str, denominator_name: str):\n    \"\"\"\n    Parameters\n    ----------\n    alias : str\n        The alias of the metric\n    numerator_name : str\n        The numerator name of the metric\n    denominator_name : str\n        The denominator name of the metric\n    \"\"\"\n    super().__init__(alias)\n    self.numerator_name = numerator_name\n    self.denominator_name = denominator_name\n    self._validate_names()\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric._validate_names","title":"<code>_validate_names()</code>","text":"<p>Validates the numerator and denominator names input for the RatioMetric class.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric._validate_names--raises","title":"Raises","text":"<p>TypeError     If the numerator or denominator names are not strings</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def _validate_names(self):\n    \"\"\"\n    Validates the numerator and denominator names input for the RatioMetric class.\n\n    Raises\n    ------\n    TypeError\n        If the numerator or denominator names are not strings\n    \"\"\"\n    if not isinstance(self.numerator_name, str) or not isinstance(\n        self.denominator_name, str\n    ):\n        raise TypeError(\"RatioMetric names must be strings\")\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.from_metrics_config","title":"<code>from_metrics_config(config)</code>  <code>classmethod</code>","text":"<p>Class method to create a RatioMetric instance from a configuration dictionary.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.from_metrics_config--parameters","title":"Parameters","text":"<p>config : dict     A dictionary containing the configuration of the metric</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.from_metrics_config--returns","title":"Returns","text":"<p>RatioMetric     A RatioMetric instance</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>@classmethod\ndef from_metrics_config(cls, config: dict) -&gt; \"Metric\":\n    \"\"\"\n    Class method to create a RatioMetric instance from a configuration dictionary.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing the configuration of the metric\n\n    Returns\n    -------\n    RatioMetric\n        A RatioMetric instance\n    \"\"\"\n    return cls(\n        alias=config[\"alias\"],\n        numerator_name=config[\"numerator_name\"],\n        denominator_name=config[\"denominator_name\"],\n    )\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.get_mean","title":"<code>get_mean(df)</code>","text":"<p>Returns the mean value of the metric, given a dataframe.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.RatioMetric.get_mean--returns","title":"Returns","text":"<p>float     The mean value of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def get_mean(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Returns the mean value of the metric, given a dataframe.\n\n    Returns\n    -------\n    float\n        The mean value of the metric\n    \"\"\"\n    return df[self.numerator_name].mean() / df[self.denominator_name].mean()\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric","title":"<code>SimpleMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>A class used to represent a Simple Metric with an alias and a name. To be used when the metric is defined at the same level of the data used for the analysis.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric--example","title":"Example","text":"<p>In a clustered experiment the participants were randomised based on their country of residence. The metric of interest is the salary of each participant. If the dataset fed into the analysis is at participant-level, then a SimpleMetric must be used. However, if the dataset fed into the analysis is at country-level, then a RatioMetric must be used.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric--attributes","title":"Attributes","text":"<p>alias : str     A string representing the alias of the metric name : str     A string representing the name of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>class SimpleMetric(Metric):\n    \"\"\"\n    A class used to represent a Simple Metric with an alias and a name.\n    To be used when the metric is defined at the same level of the data used for the analysis.\n\n    Example\n    ----------\n    In a clustered experiment the participants were randomised based on their country of residence.\n    The metric of interest is the salary of each participant. If the dataset fed into the analysis is at participant-level,\n    then a SimpleMetric must be used. However, if the dataset fed into the analysis is at country-level, then a RatioMetric must be used.\n\n    Attributes\n    ----------\n    alias : str\n        A string representing the alias of the metric\n    name : str\n        A string representing the name of the metric\n    \"\"\"\n\n    def __init__(self, alias: str, name: str):\n        \"\"\"\n        Parameters\n        ----------\n        alias : str\n            The alias of the metric\n        name : str\n            The name of the metric\n        \"\"\"\n        super().__init__(alias)\n        self.name = name\n        self._validate_name()\n\n    def _validate_name(self):\n        \"\"\"\n        Validates the name input for the SimpleMetric class.\n\n        Raises\n        ------\n        TypeError\n            If the name is not a string\n        \"\"\"\n        if not isinstance(self.name, str):\n            raise TypeError(\"SimpleMetric name must be a string\")\n\n    @property\n    def target_column(self) -&gt; str:\n        \"\"\"\n        Returns the target column for the SimpleMetric.\n\n        Returns\n        -------\n        str\n            The name of the metric\n        \"\"\"\n        return self.name\n\n    def get_mean(self, df: pd.DataFrame) -&gt; float:\n        \"\"\"\n        Returns the mean value of the metric, given a dataframe.\n\n        Returns\n        -------\n        float\n            The mean value of the metric\n        \"\"\"\n        return df[self.name].mean()\n\n    @classmethod\n    def from_metrics_config(cls, config: dict) -&gt; \"Metric\":\n        \"\"\"\n        Class method to create a SimpleMetric instance from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : dict\n            A dictionary containing the configuration of the metric\n\n        Returns\n        -------\n        SimpleMetric\n            A SimpleMetric instance\n        \"\"\"\n        return cls(alias=config[\"alias\"], name=config[\"name\"])\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.target_column","title":"<code>target_column</code>  <code>property</code>","text":"<p>Returns the target column for the SimpleMetric.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.target_column--returns","title":"Returns","text":"<p>str     The name of the metric</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.__init__","title":"<code>__init__(alias, name)</code>","text":""},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.__init__--parameters","title":"Parameters","text":"<p>alias : str     The alias of the metric name : str     The name of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def __init__(self, alias: str, name: str):\n    \"\"\"\n    Parameters\n    ----------\n    alias : str\n        The alias of the metric\n    name : str\n        The name of the metric\n    \"\"\"\n    super().__init__(alias)\n    self.name = name\n    self._validate_name()\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric._validate_name","title":"<code>_validate_name()</code>","text":"<p>Validates the name input for the SimpleMetric class.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric._validate_name--raises","title":"Raises","text":"<p>TypeError     If the name is not a string</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def _validate_name(self):\n    \"\"\"\n    Validates the name input for the SimpleMetric class.\n\n    Raises\n    ------\n    TypeError\n        If the name is not a string\n    \"\"\"\n    if not isinstance(self.name, str):\n        raise TypeError(\"SimpleMetric name must be a string\")\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.from_metrics_config","title":"<code>from_metrics_config(config)</code>  <code>classmethod</code>","text":"<p>Class method to create a SimpleMetric instance from a configuration dictionary.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.from_metrics_config--parameters","title":"Parameters","text":"<p>config : dict     A dictionary containing the configuration of the metric</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.from_metrics_config--returns","title":"Returns","text":"<p>SimpleMetric     A SimpleMetric instance</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>@classmethod\ndef from_metrics_config(cls, config: dict) -&gt; \"Metric\":\n    \"\"\"\n    Class method to create a SimpleMetric instance from a configuration dictionary.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing the configuration of the metric\n\n    Returns\n    -------\n    SimpleMetric\n        A SimpleMetric instance\n    \"\"\"\n    return cls(alias=config[\"alias\"], name=config[\"name\"])\n</code></pre>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.get_mean","title":"<code>get_mean(df)</code>","text":"<p>Returns the mean value of the metric, given a dataframe.</p>"},{"location":"api/metric.html#cluster_experiments.inference.metric.SimpleMetric.get_mean--returns","title":"Returns","text":"<p>float     The mean value of the metric</p> Source code in <code>cluster_experiments/inference/metric.py</code> <pre><code>def get_mean(self, df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Returns the mean value of the metric, given a dataframe.\n\n    Returns\n    -------\n    float\n        The mean value of the metric\n    \"\"\"\n    return df[self.name].mean()\n</code></pre>"},{"location":"api/perturbator.html","title":"<code>from cluster_experiments.perturbator import *</code>","text":""},{"location":"api/perturbator.html#cluster_experiments.perturbator.BetaRelativePerturbator","title":"<code>BetaRelativePerturbator</code>","text":"<p>               Bases: <code>NormalPerturbator</code>, <code>RelativePositivePerturbator</code></p> <p>A stochastic Perturbator for continuous targets that applies a  sampled effect from a scaled Beta distribution. It applies the effect multiplicatively.</p> <p>The sampled effect is defined for values in the specified range (range_min, range_max). It's recommended to set -1&lt;range_min&lt;0 and range_max&gt;0 in a \"symmetric way\" around 0, such that log(1 + range_min) = -log(1 + range_max). This ensures to have an \"symmetric range\" of perturbations that relatively decrease the target as perturbations that relatively increase the target. By \"symmetry\" of relative effects we mean that for an effect c &gt; 0, an increase of the target t via t*(1 + c) is \"symmetric\" to a decrease of t via t/(1 + c). For example, an increase of 5x (i.e. by +400%, corresponding to c_inc=4) is \"symmetric\" to a decrease of 5x (i.e. a decrease of -80%, corresponding to c_dec = -0.8). In this case, 1 + c_dec = 1/(1 + c_inc), so the relative effects c_inc and c_dec are \"symmetric\" in the sense that they are inverse to each other.</p> <p>The number of samples with 0 as target remains unchanged.</p> <p>The stochastic effect is sampled from a beta distribution with parameters mean and variance, which is linearly scaled to the range (range_min, range_max). If variance is not provided, the variance is abs(mean).</p> <pre><code>target -&gt; target * (1 + effect), where effect ~ Beta(a, b)\n</code></pre> <p>The common beta parameters are derived from the mean and scale parameters, combined with linear transformations to ensure the support in the given range. The resulting beta parameters are scaled by abs(mu) to narrow the beta distribution around the mean.</p> <pre><code>mu_transformed &lt;- (mu - range_min) / (range_max - range_min)\nscale_transformed &lt;- (scale - range_min) / (range_max - range_min)\na &lt;- mu_transformed / (scale_transformed * scale_transformed)\nb &lt;- (1-mu_transformed) / (scale_transformed * scale_transformed)\neffect_transformed ~ beta(a/abs(mu), b/abs(mu))\neffect = effect_transformed * (range_max - range_min) + range_min\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>average_effect</code> <code>Optional[float]</code> <p>the average effect of the treatment. Defaults to None.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>name of the target_col to use as the outcome. Defaults to \"target\".</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>the name of the column that contains the treatment. Defaults to \"treatment\".</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group. Defaults to \"B\".</p> <code>'B'</code> <code>scale</code> <code>Optional[float]</code> <p>the scale of the effect distribution. Defaults to None. If not provided, the variance of the beta distribution is abs(mean).</p> <code>None</code> <code>range_min</code> <code>float</code> <p>the minimum value of the target range, must be &gt;-1. Defaults to -0.8, which allows for up to 5x decreases of the target.</p> <code>None</code> <code>range_max</code> <code>float</code> <p>the maximum value of the target range. Defaults to 4, which allows for up to 5x increases of the target.</p> <code>None</code> <code>reduce_variance</code> <code>Optional[bool]</code> <p>if True and if abs(average_effect)&lt;1, we reduce the variance of the beta distribution by multiplying the beta parameters by 1/abs(average_effect). Defaults to None, which is equivalent to True.</p> <code>None</code> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class BetaRelativePerturbator(NormalPerturbator, RelativePositivePerturbator):\n    \"\"\"\n    A stochastic Perturbator for continuous targets that applies a  sampled\n    effect from a scaled Beta distribution. It applies the effect multiplicatively.\n\n    The sampled effect is defined for values in the specified range\n    (range_min, range_max). It's recommended to set -1&lt;range_min&lt;0 and\n    range_max&gt;0 in a \"symmetric way\" around 0, such that\n    log(1 + range_min) = -log(1 + range_max).\n    This ensures to have an \"symmetric range\" of perturbations that relatively\n    decrease the target as perturbations that relatively increase the target.\n    By \"symmetry\" of relative effects we mean that for an effect c &gt; 0, an\n    increase of the target t via t*(1 + c) is \"symmetric\" to a decrease of t\n    via t/(1 + c). For example, an increase of 5x (i.e. by +400%, corresponding\n    to c_inc=4) is \"symmetric\" to a decrease of 5x (i.e. a decrease of -80%,\n    corresponding to c_dec = -0.8). In this case, 1 + c_dec = 1/(1 + c_inc), so\n    the relative effects c_inc and c_dec are \"symmetric\" in the sense that they\n    are inverse to each other.\n\n    The number of samples with 0 as target remains unchanged.\n\n    The stochastic effect is sampled from a beta distribution with parameters\n    mean and variance, which is linearly scaled to the range\n    (range_min, range_max).\n    If variance is not provided, the variance is abs(mean).\n\n    ```\n    target -&gt; target * (1 + effect), where effect ~ Beta(a, b)\n    ```\n\n    The common beta parameters are derived from the mean and scale parameters,\n    combined with linear transformations to ensure the support in the given\n    range. The resulting beta parameters are scaled by abs(mu) to narrow the\n    beta distribution around the mean.\n\n    ```\n    mu_transformed &lt;- (mu - range_min) / (range_max - range_min)\n    scale_transformed &lt;- (scale - range_min) / (range_max - range_min)\n    a &lt;- mu_transformed / (scale_transformed * scale_transformed)\n    b &lt;- (1-mu_transformed) / (scale_transformed * scale_transformed)\n    effect_transformed ~ beta(a/abs(mu), b/abs(mu))\n    effect = effect_transformed * (range_max - range_min) + range_min\n    ```\n\n    Arguments:\n        average_effect (Optional[float], optional): the average effect of the treatment. Defaults to None.\n        target_col (str, optional): name of the target_col to use as the outcome. Defaults to \"target\".\n        treatment_col (str, optional): the name of the column that contains the treatment. Defaults to \"treatment\".\n        treatment (str, optional): name of the treatment to use as the treated group. Defaults to \"B\".\n        scale (Optional[float], optional): the scale of the effect distribution. Defaults to None.\n            If not provided, the variance of the beta distribution is abs(mean).\n        range_min (float, optional): the minimum value of the target range, must be &gt;-1.\n            Defaults to -0.8, which allows for up to 5x decreases of the target.\n        range_max (float, optional): the maximum value of the target range.\n            Defaults to 4, which allows for up to 5x increases of the target.\n        reduce_variance (Optional[bool], optional): if True and if abs(average_effect)&lt;1, we reduce\n            the variance of the beta distribution by multiplying the beta parameters by 1/abs(average_effect).\n            Defaults to None, which is equivalent to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        average_effect: Optional[float] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        scale: Optional[float] = None,\n        range_min: Optional[float] = None,\n        range_max: Optional[float] = None,\n        reduce_variance: Optional[bool] = None,\n    ):\n        self._check_range(range_min, range_max)\n        super().__init__(average_effect, target_col, treatment_col, treatment, scale)\n        self._range_min = range_min or -0.8\n        self._range_max = range_max or 4\n        self._reduce_variance = reduce_variance or True\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Usage:\n        ```python\n        from cluster_experiments.perturbator import BetaRelativePerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = BetaRelativePerturbator(range_min = -0.5, range_max = 2)\n        # Increase target metric by 20% on average\n        perturbator.perturbate(df, average_effect=0.2)\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        self.check_relative_effect_bounds(average_effect)\n        scale = self.get_scale(average_effect)\n        self.check_relative_effect_bounds(scale)\n        n = self.get_number_of_treated(df)\n        sampled_effect = self._sample_scaled_beta_effect(average_effect, scale, n)\n        df = self.apply_multiplicative_effect(df, sampled_effect)\n        return df\n\n    @staticmethod\n    def _check_range(range_min: float, range_max: float):\n        if range_min &lt; -1:\n            raise ValueError(f\"range_min needs to be greater than -1, got {range_min}\")\n        if range_min &gt;= range_max:\n            raise ValueError(\n                f\"range_min needs to be smaller than range_max, got \"\n                f\"{range_min = } and {range_max = }\"\n            )\n\n    def check_relative_effect_bounds(self, average_effect: float) -&gt; None:\n        self.check_average_effect_greater_than(average_effect, x=self._range_min)\n        self.check_average_effect_smaller_than(average_effect, x=self._range_max)\n\n    def check_average_effect_greater_than(\n        self, average_effect: float, x: float\n    ) -&gt; Optional[NoReturn]:\n        if average_effect &lt;= x:\n            raise ValueError(\n                f\"Simulated effect needs to be greater than range_min={x}, got {average_effect}\"\n            )\n\n    def check_average_effect_smaller_than(\n        self, average_effect: float, x: float\n    ) -&gt; Optional[NoReturn]:\n        if average_effect &gt;= x:\n            raise ValueError(\n                f\"Simulated effect needs to be smaller than range_max={x}, got {average_effect}\"\n            )\n\n    def _reduce_variance_beta_params(\n        self, average_effect: float, a: float, b: float\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Multiplying the parameters of the beta distribution with a factor &gt;1\n        reduces variance\n        \"\"\"\n        if abs(average_effect) &lt; 1:\n            a *= 1 / abs(average_effect)\n            b *= 1 / abs(average_effect)\n        return a, b\n\n    def _sample_scaled_beta_effect(\n        self, average_effect: float, scale: float, n: int\n    ) -&gt; np.ndarray:\n        average_effect_inv_transf = self._inv_transform_to_range(average_effect)\n        scale_inv_transf = self._inv_transform_to_range(scale)\n        a = average_effect_inv_transf / (scale_inv_transf * scale_inv_transf)\n        b = (1 - average_effect_inv_transf) / (scale_inv_transf * scale_inv_transf)\n\n        if self._reduce_variance:\n            a, b = self._reduce_variance_beta_params(average_effect, a, b)\n        beta = np.random.beta(a, b, n)\n\n        return self._transform_to_range(beta)\n\n    def _transform_to_range(self, x: Union[float, np.ndarray]):\n        return x * (self._range_max - self._range_min) + self._range_min\n\n    def _inv_transform_to_range(self, x: Union[float, np.ndarray]):\n        return (x - self._range_min) / (self._range_max - self._range_min)\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n        return cls(\n            average_effect=config.average_effect,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            scale=config.scale,\n            range_min=config.range_min,\n            range_max=config.range_max,\n        )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BetaRelativePerturbator._reduce_variance_beta_params","title":"<code>_reduce_variance_beta_params(average_effect, a, b)</code>","text":"<p>Multiplying the parameters of the beta distribution with a factor &gt;1 reduces variance</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def _reduce_variance_beta_params(\n    self, average_effect: float, a: float, b: float\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Multiplying the parameters of the beta distribution with a factor &gt;1\n    reduces variance\n    \"\"\"\n    if abs(average_effect) &lt; 1:\n        a *= 1 / abs(average_effect)\n        b *= 1 / abs(average_effect)\n    return a, b\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BetaRelativePerturbator.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a Perturbator object from a PowerConfig object</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n    return cls(\n        average_effect=config.average_effect,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        scale=config.scale,\n        range_min=config.range_min,\n        range_max=config.range_max,\n    )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BetaRelativePerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Usage: <pre><code>from cluster_experiments.perturbator import BetaRelativePerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = BetaRelativePerturbator(range_min = -0.5, range_max = 2)\n# Increase target metric by 20% on average\nperturbator.perturbate(df, average_effect=0.2)\n</code></pre></p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Usage:\n    ```python\n    from cluster_experiments.perturbator import BetaRelativePerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = BetaRelativePerturbator(range_min = -0.5, range_max = 2)\n    # Increase target metric by 20% on average\n    perturbator.perturbate(df, average_effect=0.2)\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    self.check_relative_effect_bounds(average_effect)\n    scale = self.get_scale(average_effect)\n    self.check_relative_effect_bounds(scale)\n    n = self.get_number_of_treated(df)\n    sampled_effect = self._sample_scaled_beta_effect(average_effect, scale, n)\n    df = self.apply_multiplicative_effect(df, sampled_effect)\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BetaRelativePositivePerturbator","title":"<code>BetaRelativePositivePerturbator</code>","text":"<p>               Bases: <code>NormalPerturbator</code>, <code>RelativePositivePerturbator</code></p> <p>A stochastic Perturbator for continuous, positively-defined targets that applies a sampled effect from the Beta distribution. It applies the effect multiplicatively.</p> <p>WARNING: the average effect is only defined for values between 0 and 1 (not included). Therefore, it only increments the target for the treated samples.</p> <p>The number of samples with 0 as target remains unchanged.</p> <p>The stochastic effect is sampled from a beta distribution with parameters mean and variance. If variance is not provided, the variance is abs(mean). Hence, the effect is bounded by 0 and 1.</p> <pre><code>target -&gt; target * (1 + effect), where effect ~ Beta(a, b); a, b &gt; 0\n                                   and target &gt; 0 for all samples\n</code></pre> <p>The common beta parameters are derived from the mean and scale parameters (see how below). That's why the average effect is only defined for values between 0 and 1, otherwise one of the beta parameters would be negative or zero:</p> <p><pre><code>a &lt;- mu / (scale * scale)\nb &lt;- (1-mu) / (scale * scale)\neffect ~ beta(a, b)\n</code></pre> source: https://stackoverflow.com/a/51143208</p> <p>Example: a mean = 0.2 and variance = 0.1, give a = 20 and b = 80 Plot: https://www.wolframalpha.com/input?i=plot+distribution+of+beta%2820%2C+80%29</p> <p>Parameters:</p> Name Type Description Default <code>average_effect</code> <code>Optional[float]</code> <p>the average effect of the treatment. Defaults to None.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>name of the target_col to use as the outcome. Defaults to \"target\".</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>the name of the column that contains the treatment. Defaults to \"treatment\".</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group. Defaults to \"B\".</p> <code>'B'</code> <code>scale</code> <code>Optional[float]</code> <p>the scale of the effect distribution. Defaults to None.</p> <code>None</code> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class BetaRelativePositivePerturbator(NormalPerturbator, RelativePositivePerturbator):\n    \"\"\"\n    A stochastic Perturbator for continuous, positively-defined targets that applies a\n    sampled effect from the Beta distribution. It applies the effect multiplicatively.\n\n    *WARNING*: the average effect is only defined for values between 0 and 1 (not\n    included). Therefore, it only increments the target for the treated samples.\n\n    The number of samples with 0 as target remains unchanged.\n\n    The stochastic effect is sampled from a beta distribution with parameters mean and\n    variance. If variance is not provided, the variance is abs(mean). Hence, the effect\n    is bounded by 0 and 1.\n\n    ```\n    target -&gt; target * (1 + effect), where effect ~ Beta(a, b); a, b &gt; 0\n                                       and target &gt; 0 for all samples\n    ```\n\n    The common beta parameters are derived from the mean and scale parameters (see\n    how below). That's why the average effect is only defined for values between 0\n    and 1, otherwise one of the beta parameters would be negative or zero:\n\n    ```\n    a &lt;- mu / (scale * scale)\n    b &lt;- (1-mu) / (scale * scale)\n    effect ~ beta(a, b)\n    ```\n    source: https://stackoverflow.com/a/51143208\n\n    Example: a mean = 0.2 and variance = 0.1, give a = 20 and b = 80\n    Plot: https://www.wolframalpha.com/input?i=plot+distribution+of+beta%2820%2C+80%29\n\n    Arguments:\n        average_effect (Optional[float], optional): the average effect of the treatment. Defaults to None.\n        target_col (str, optional): name of the target_col to use as the outcome. Defaults to \"target\".\n        treatment_col (str, optional): the name of the column that contains the treatment. Defaults to \"treatment\".\n        treatment (str, optional): name of the treatment to use as the treated group. Defaults to \"B\".\n        scale (Optional[float], optional): the scale of the effect distribution. Defaults to None.\n    \"\"\"\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Usage:\n        ```python\n        from cluster_experiments.perturbator import BetaRelativePositivePerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = BetaRelativePositivePerturbator()\n        # Increase target metric by 20%\n        perturbator.perturbate(df, average_effect=0.2)\n        # returns pd.DataFrame({\"target\": [1, 3, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        self.check_beta_positive_effect(df, average_effect)\n        scale = self.get_scale(average_effect)\n        n = self.get_number_of_treated(df)\n        sampled_effect = self._sample_beta_effect(average_effect, scale, n)\n        df = self.apply_multiplicative_effect(df, sampled_effect)\n        return df\n\n    def check_beta_positive_effect(self, df, average_effect):\n        self.check_average_effect_greater_than(average_effect, x=0)\n        self.check_average_effect_less_than(average_effect, x=1)\n        self.check_target_is_not_negative(df)\n        self.check_target_is_not_constant_zero(df, average_effect)\n\n    def check_average_effect_less_than(\n        self, average_effect: float, x: float\n    ) -&gt; Optional[NoReturn]:\n        if average_effect &gt;= x:\n            raise ValueError(\n                f\"Simulated effect needs to be less than {x*100:.0f}%, got \"\n                f\"{average_effect*100:.1f}%\"\n            )\n\n    def _sample_beta_effect(\n        self, average_effect: float, scale: float, n: int\n    ) -&gt; np.ndarray:\n        a = average_effect / (scale * scale)\n        b = (1 - average_effect) / (scale * scale)\n        return np.random.beta(a, b, n)\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BetaRelativePositivePerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Usage: <pre><code>from cluster_experiments.perturbator import BetaRelativePositivePerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = BetaRelativePositivePerturbator()\n# Increase target metric by 20%\nperturbator.perturbate(df, average_effect=0.2)\n# returns pd.DataFrame({\"target\": [1, 3, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n</code></pre></p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Usage:\n    ```python\n    from cluster_experiments.perturbator import BetaRelativePositivePerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = BetaRelativePositivePerturbator()\n    # Increase target metric by 20%\n    perturbator.perturbate(df, average_effect=0.2)\n    # returns pd.DataFrame({\"target\": [1, 3, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    self.check_beta_positive_effect(df, average_effect)\n    scale = self.get_scale(average_effect)\n    n = self.get_number_of_treated(df)\n    sampled_effect = self._sample_beta_effect(average_effect, scale, n)\n    df = self.apply_multiplicative_effect(df, sampled_effect)\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BinaryPerturbator","title":"<code>BinaryPerturbator</code>","text":"<p>               Bases: <code>Perturbator</code></p> <p>BinaryPerturbator is a Perturbator that adds is used to deal with binary outcome variables. It randomly selects some treated instances and flips their outcome from 0 to 1 or 1 to 0, depending on the effect being positive or negative</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class BinaryPerturbator(Perturbator):\n    \"\"\"\n    BinaryPerturbator is a Perturbator that adds is used to deal with binary outcome variables.\n    It randomly selects some treated instances and flips their outcome from 0 to 1 or 1 to 0, depending on the effect being positive or negative\n    \"\"\"\n\n    def _sample_max(self, df: pd.DataFrame, n: int) -&gt; pd.DataFrame:\n        \"\"\"Like sample without replacement,\n        but if you are to sample more than 100% of the data,\n        it just returns the whole dataframe.\"\"\"\n        if n &gt;= len(df):\n            return df\n        return df.sample(n=n)\n\n    def _data_checks(self, df: pd.DataFrame, average_effect: float) -&gt; None:\n        \"\"\"Check that outcome is indeed binary, and average effect is in (-1, 1)\"\"\"\n\n        if set(df[self.target_col].unique()) - {0, 1}:\n            raise ValueError(\n                f\"Target column must be binary, found {set(df[self.target_col].unique())}\"\n            )\n\n        if average_effect &gt; 1 or average_effect &lt; -1:\n            raise ValueError(\n                f\"Average effect must be in (-1, 1), found {average_effect}\"\n            )\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Usage:\n\n        ```python\n        from cluster_experiments.perturbator import BinaryPerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 0, 1], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = BinaryPerturbator()\n        perturbator.perturbate(df, average_effect=0.1)\n        ```\n        \"\"\"\n\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n\n        self._data_checks(df, average_effect)\n\n        from_target, to_target = 1, 0\n        if average_effect &gt; 0:\n            from_target, to_target = 0, 1\n\n        n_transformed = abs(int(average_effect * len(df.query(self.treated_query))))\n        idx = list(\n            # Sample of negative cases in group B\n            df.query(f\"{self.target_col} == {from_target} &amp; {self.treated_query}\")\n            .pipe(self._sample_max, n=n_transformed)\n            .index.drop_duplicates()\n        )\n        df.loc[idx, self.target_col] = to_target\n        return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BinaryPerturbator._data_checks","title":"<code>_data_checks(df, average_effect)</code>","text":"<p>Check that outcome is indeed binary, and average effect is in (-1, 1)</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def _data_checks(self, df: pd.DataFrame, average_effect: float) -&gt; None:\n    \"\"\"Check that outcome is indeed binary, and average effect is in (-1, 1)\"\"\"\n\n    if set(df[self.target_col].unique()) - {0, 1}:\n        raise ValueError(\n            f\"Target column must be binary, found {set(df[self.target_col].unique())}\"\n        )\n\n    if average_effect &gt; 1 or average_effect &lt; -1:\n        raise ValueError(\n            f\"Average effect must be in (-1, 1), found {average_effect}\"\n        )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BinaryPerturbator._sample_max","title":"<code>_sample_max(df, n)</code>","text":"<p>Like sample without replacement, but if you are to sample more than 100% of the data, it just returns the whole dataframe.</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def _sample_max(self, df: pd.DataFrame, n: int) -&gt; pd.DataFrame:\n    \"\"\"Like sample without replacement,\n    but if you are to sample more than 100% of the data,\n    it just returns the whole dataframe.\"\"\"\n    if n &gt;= len(df):\n        return df\n    return df.sample(n=n)\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.BinaryPerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Usage:</p> <pre><code>from cluster_experiments.perturbator import BinaryPerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 0, 1], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = BinaryPerturbator()\nperturbator.perturbate(df, average_effect=0.1)\n</code></pre> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Usage:\n\n    ```python\n    from cluster_experiments.perturbator import BinaryPerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 0, 1], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = BinaryPerturbator()\n    perturbator.perturbate(df, average_effect=0.1)\n    ```\n    \"\"\"\n\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n\n    self._data_checks(df, average_effect)\n\n    from_target, to_target = 1, 0\n    if average_effect &gt; 0:\n        from_target, to_target = 0, 1\n\n    n_transformed = abs(int(average_effect * len(df.query(self.treated_query))))\n    idx = list(\n        # Sample of negative cases in group B\n        df.query(f\"{self.target_col} == {from_target} &amp; {self.treated_query}\")\n        .pipe(self._sample_max, n=n_transformed)\n        .index.drop_duplicates()\n    )\n    df.loc[idx, self.target_col] = to_target\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.ConstantPerturbator","title":"<code>ConstantPerturbator</code>","text":"<p>               Bases: <code>Perturbator</code></p> <p>ConstantPerturbator is a Perturbator that adds a constant effect to the target column of the treated instances.</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class ConstantPerturbator(Perturbator):\n    \"\"\"\n    ConstantPerturbator is a Perturbator that adds a constant effect to the target column of the treated instances.\n    \"\"\"\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Usage:\n\n        ```python\n        from cluster_experiments.perturbator import ConstantPerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = ConstantPerturbator()\n        perturbator.perturbate(df, average_effect=1)\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        df = self.apply_additive_effect(df, average_effect)\n        return df\n\n    def apply_additive_effect(\n        self, df: pd.DataFrame, effect: Union[float, np.ndarray]\n    ) -&gt; pd.DataFrame:\n        df.loc[df[self.treatment_col] == self.treatment, self.target_col] += effect\n        return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.ConstantPerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Usage:</p> <pre><code>from cluster_experiments.perturbator import ConstantPerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = ConstantPerturbator()\nperturbator.perturbate(df, average_effect=1)\n</code></pre> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Usage:\n\n    ```python\n    from cluster_experiments.perturbator import ConstantPerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = ConstantPerturbator()\n    perturbator.perturbate(df, average_effect=1)\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    df = self.apply_additive_effect(df, average_effect)\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.NormalPerturbator","title":"<code>NormalPerturbator</code>","text":"<p>               Bases: <code>ConstantPerturbator</code></p> <p>The NormalPerturbator class implements a perturbator that adds a stochastic effect to the target column of the treated instances. The stochastic effect is sampled from a normal distribution with mean average_effect and variance scale. If scale is not provided, the variance is abs(average_effect). If scale is provided, a value not much bigger than the average_effect is suggested.</p> <pre><code>target -&gt; target + effect, where effect ~ Normal(average_effect, scale)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>average_effect</code> <code>Optional[float]</code> <p>the average effect of the treatment. Defaults to None.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>name of the target_col to use as the outcome. Defaults to \"target\".</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>the name of the column that contains the treatment. Defaults to \"treatment\".</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group. Defaults to \"B\".</p> <code>'B'</code> <code>scale</code> <code>Optional[float]</code> <p>the scale of the effect distribution. Defaults to None.</p> <code>None</code> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class NormalPerturbator(ConstantPerturbator):\n    \"\"\"The NormalPerturbator class implements a perturbator that adds a stochastic effect\n    to the target column of the treated instances. The stochastic effect is sampled from a\n    normal distribution with mean average_effect and variance scale. If scale is not\n    provided, the variance is abs(average_effect). If scale is provided, a\n    value not much bigger than the average_effect is suggested.\n\n    ```\n    target -&gt; target + effect, where effect ~ Normal(average_effect, scale)\n    ```\n\n    Arguments:\n        average_effect (Optional[float], optional): the average effect of the treatment. Defaults to None.\n        target_col (str, optional): name of the target_col to use as the outcome. Defaults to \"target\".\n        treatment_col (str, optional): the name of the column that contains the treatment. Defaults to \"treatment\".\n        treatment (str, optional): name of the treatment to use as the treated group. Defaults to \"B\".\n        scale (Optional[float], optional): the scale of the effect distribution. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        average_effect: Optional[float] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        scale: Optional[float] = None,\n    ):\n        super().__init__(average_effect, target_col, treatment_col, treatment)\n        self._scale = scale\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Perturbate with a normal effect with mean average_effect and\n        std abs(average_effect).\n\n        Arguments:\n            df (pd.DataFrame): the dataframe to perturbate.\n            average_effect (Optional[float], optional): the average effect. Defaults to None.\n\n        Returns:\n            pd.DataFrame: the perturbated dataframe.\n\n        Usage:\n\n        ```python\n        from cluster_experiments.perturbator import NormalPerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = NormalPerturbator()\n        perturbator.perturbate(df, average_effect=1)\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        scale = self.get_scale(average_effect)\n        n = self.get_number_of_treated(df)\n        sampled_effect = self._sample_normal_effect(average_effect, scale, n)\n        df = self.apply_additive_effect(df, sampled_effect)\n        return df\n\n    def get_scale(self, average_effect: float) -&gt; float:\n        \"\"\"Get the scale of the normal distribution. If scale is not provided, the\n        variance is abs(average_effect). Raises a ValueError if scale is not positive.\n        \"\"\"\n        scale = abs(average_effect) if self._scale is None else self._scale\n        if scale &lt;= 0:\n            raise ValueError(f\"scale must be positive, got {scale}\")\n        return scale\n\n    def get_number_of_treated(self, df: pd.DataFrame) -&gt; int:\n        \"\"\"Get the number of treated instances in the dataframe\"\"\"\n        return (df[self.treatment_col] == self.treatment).sum()\n\n    def _sample_normal_effect(\n        self, average_effect: float, scale: float, n: int\n    ) -&gt; np.ndarray:\n        return np.random.normal(average_effect, scale, n)\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n        return cls(\n            average_effect=config.average_effect,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            scale=config.scale,\n        )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.NormalPerturbator.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a Perturbator object from a PowerConfig object</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n    return cls(\n        average_effect=config.average_effect,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        scale=config.scale,\n    )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.NormalPerturbator.get_number_of_treated","title":"<code>get_number_of_treated(df)</code>","text":"<p>Get the number of treated instances in the dataframe</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def get_number_of_treated(self, df: pd.DataFrame) -&gt; int:\n    \"\"\"Get the number of treated instances in the dataframe\"\"\"\n    return (df[self.treatment_col] == self.treatment).sum()\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.NormalPerturbator.get_scale","title":"<code>get_scale(average_effect)</code>","text":"<p>Get the scale of the normal distribution. If scale is not provided, the variance is abs(average_effect). Raises a ValueError if scale is not positive.</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def get_scale(self, average_effect: float) -&gt; float:\n    \"\"\"Get the scale of the normal distribution. If scale is not provided, the\n    variance is abs(average_effect). Raises a ValueError if scale is not positive.\n    \"\"\"\n    scale = abs(average_effect) if self._scale is None else self._scale\n    if scale &lt;= 0:\n        raise ValueError(f\"scale must be positive, got {scale}\")\n    return scale\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.NormalPerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Perturbate with a normal effect with mean average_effect and std abs(average_effect).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the dataframe to perturbate.</p> required <code>average_effect</code> <code>Optional[float]</code> <p>the average effect. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the perturbated dataframe.</p> <p>Usage:</p> <pre><code>from cluster_experiments.perturbator import NormalPerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = NormalPerturbator()\nperturbator.perturbate(df, average_effect=1)\n</code></pre> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Perturbate with a normal effect with mean average_effect and\n    std abs(average_effect).\n\n    Arguments:\n        df (pd.DataFrame): the dataframe to perturbate.\n        average_effect (Optional[float], optional): the average effect. Defaults to None.\n\n    Returns:\n        pd.DataFrame: the perturbated dataframe.\n\n    Usage:\n\n    ```python\n    from cluster_experiments.perturbator import NormalPerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = NormalPerturbator()\n    perturbator.perturbate(df, average_effect=1)\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    scale = self.get_scale(average_effect)\n    n = self.get_number_of_treated(df)\n    sampled_effect = self._sample_normal_effect(average_effect, scale, n)\n    df = self.apply_additive_effect(df, sampled_effect)\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.Perturbator","title":"<code>Perturbator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract perturbator. Perturbators are used to simulate a fictitious effect when running a power analysis.</p> <p>The idea is that, when running a power analysis, we split our instances according to a RandomSplitter, and the instances that got the treatment, are perturbated with a fictional effect via the Perturbator.</p> <p>In order to create your own perturbator, you should create a derived class that implements the perturbate method. The perturbate method should add the average effect in the desired way and return the dataframe with the extra average effect, without affecting the initial dataframe. Keep in mind to use <code>df = df.copy()</code> in the first line of the perturbate method.</p> <p>Parameters:</p> Name Type Description Default <code>average_effect</code> <code>Optional[float]</code> <p>The average effect of the treatment</p> <code>None</code> <code>target_col</code> <code>str</code> <p>name of the target_col to use as the outcome</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>The name of the column that contains the treatment</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group</p> <code>'B'</code> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class Perturbator(ABC):\n    \"\"\"\n    Abstract perturbator. Perturbators are used to simulate a fictitious effect when running a power analysis.\n\n    The idea is that, when running a power analysis, we split our instances according to a RandomSplitter, and the\n    instances that got the treatment, are perturbated with a fictional effect via the Perturbator.\n\n    In order to create your own perturbator, you should create a derived class that implements the perturbate method.\n    The perturbate method should add the average effect in the desired way and return the dataframe with the extra average effect,\n    without affecting the initial dataframe. Keep in mind to use `df = df.copy()` in the first line of the perturbate method.\n\n    Arguments:\n        average_effect: The average effect of the treatment\n        target_col: name of the target_col to use as the outcome\n        treatment_col: The name of the column that contains the treatment\n        treatment: name of the treatment to use as the treated group\n\n    \"\"\"\n\n    def __init__(\n        self,\n        average_effect: Optional[float] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n    ):\n        self.average_effect = average_effect\n        self.target_col = target_col\n        self.treatment_col = treatment_col\n        self.treatment = treatment\n        self.treated_query = f\"{self.treatment_col} == '{self.treatment}'\"\n\n    def get_average_effect(self, average_effect: Optional[float] = None) -&gt; float:\n        average_effect = (\n            average_effect if average_effect is not None else self.average_effect\n        )\n        if average_effect is None:\n            raise ValueError(\n                \"average_effect must be provided, either in the constructor or in the method call\"\n            )\n        return average_effect\n\n    @abstractmethod\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Method to perturbate a dataframe\"\"\"\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n        return cls(\n            average_effect=config.average_effect,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n        )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.Perturbator.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a Perturbator object from a PowerConfig object</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n    return cls(\n        average_effect=config.average_effect,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n    )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.Perturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>  <code>abstractmethod</code>","text":"<p>Method to perturbate a dataframe</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>@abstractmethod\ndef perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Method to perturbate a dataframe\"\"\"\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.RelativeMixedPerturbator","title":"<code>RelativeMixedPerturbator</code>","text":"<p>               Bases: <code>Perturbator</code></p> <p>RelativeMixedPerturbator is a Perturbator that applies a multiplicative effect to the target column of treated instances, modifying their values by a relative factor depending on whether they are positive or negative.</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class RelativeMixedPerturbator(Perturbator):\n    \"\"\"\n    RelativeMixedPerturbator is a Perturbator that applies a multiplicative effect to the target column\n    of treated instances, modifying their values by a relative factor depending on whether they are positive\n    or negative.\n    \"\"\"\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply the multiplicative effect to the target column of the treated instances. The effect is calculated\n        as a factor of (1 + effect) for non-negative values, and (1 - effect) for negative values.\n\n        Usage:\n\n        ```python\n        from cluster_experiments.perturbator import RelativeMixedPerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, -2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = RelativeMixedPerturbator()\n        perturbator.perturbate(df, average_effect=0.1)\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        df = self.apply_multiplicative_effect(df, average_effect)\n        return df\n\n    def apply_multiplicative_effect(\n        self, df: pd.DataFrame, effect: Union[float, np.ndarray]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply the relative multiplicative effect to the target column, adjusting values based on whether\n        they are positive or negative.\n        \"\"\"\n        mask = df[self.treatment_col] == self.treatment\n        original_values = df.loc[mask, self.target_col]\n\n        # Calculate new values: apply (1 + effect) for non-negative, (1 - effect) for negative values\n        new_values = np.where(\n            original_values &gt;= 0,\n            original_values * (1 + effect),\n            original_values * (1 - effect),\n        )\n\n        # The round is required to avoid float imprecision\n        df.loc[mask, self.target_col] = np.round(new_values, 2)\n\n        return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.RelativeMixedPerturbator.apply_multiplicative_effect","title":"<code>apply_multiplicative_effect(df, effect)</code>","text":"<p>Apply the relative multiplicative effect to the target column, adjusting values based on whether they are positive or negative.</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def apply_multiplicative_effect(\n    self, df: pd.DataFrame, effect: Union[float, np.ndarray]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the relative multiplicative effect to the target column, adjusting values based on whether\n    they are positive or negative.\n    \"\"\"\n    mask = df[self.treatment_col] == self.treatment\n    original_values = df.loc[mask, self.target_col]\n\n    # Calculate new values: apply (1 + effect) for non-negative, (1 - effect) for negative values\n    new_values = np.where(\n        original_values &gt;= 0,\n        original_values * (1 + effect),\n        original_values * (1 - effect),\n    )\n\n    # The round is required to avoid float imprecision\n    df.loc[mask, self.target_col] = np.round(new_values, 2)\n\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.RelativeMixedPerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Apply the multiplicative effect to the target column of the treated instances. The effect is calculated as a factor of (1 + effect) for non-negative values, and (1 - effect) for negative values.</p> <p>Usage:</p> <pre><code>from cluster_experiments.perturbator import RelativeMixedPerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, -2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = RelativeMixedPerturbator()\nperturbator.perturbate(df, average_effect=0.1)\n</code></pre> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the multiplicative effect to the target column of the treated instances. The effect is calculated\n    as a factor of (1 + effect) for non-negative values, and (1 - effect) for negative values.\n\n    Usage:\n\n    ```python\n    from cluster_experiments.perturbator import RelativeMixedPerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, -2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = RelativeMixedPerturbator()\n    perturbator.perturbate(df, average_effect=0.1)\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    df = self.apply_multiplicative_effect(df, average_effect)\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.RelativePositivePerturbator","title":"<code>RelativePositivePerturbator</code>","text":"<p>               Bases: <code>Perturbator</code></p> <p>A Perturbator for continuous, positively-defined targets applies a simulated effect multiplicatively for the treated samples, ie. proportional to the target value for each sample. The number of samples with 0 as target remains unchanged.</p> <pre><code>target -&gt; target * (1 + average_effect), where -1 &lt; average_effect &lt; inf\n                                           and target &gt; 0 for all samples\n</code></pre> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class RelativePositivePerturbator(Perturbator):\n    \"\"\"\n    A Perturbator for continuous, positively-defined targets\n    applies a simulated effect multiplicatively for the treated samples, ie.\n    proportional to the target value for each sample. The number of samples with 0\n    as target remains unchanged.\n\n    ```\n    target -&gt; target * (1 + average_effect), where -1 &lt; average_effect &lt; inf\n                                               and target &gt; 0 for all samples\n    ```\n    \"\"\"\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Usage:\n        ```python\n        from cluster_experiments.perturbator import RelativePositivePerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = RelativePositivePerturbator()\n        # Increase target metric by 50%\n        perturbator.perturbate(df, average_effect=0.5)\n        # returns pd.DataFrame({\"target\": [1, 3, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        self.check_relative_positive_effect(df, average_effect)\n        df = self.apply_multiplicative_effect(df, average_effect)\n        return df\n\n    def check_relative_positive_effect(\n        self, df: pd.DataFrame, average_effect: float\n    ) -&gt; None:\n        self.check_average_effect_greater_than(average_effect, x=-1)\n        self.check_target_is_not_negative(df)\n        self.check_target_is_not_constant_zero(df, average_effect)\n\n    def check_target_is_not_constant_zero(\n        self, df: pd.DataFrame, average_effect: float\n    ) -&gt; Optional[NoReturn]:\n        treatment_zeros = (\n            (df[self.treatment_col] != self.treatment) | (df[self.target_col] == 0)\n        ).mean()\n        if 1.0 == treatment_zeros:\n            raise ValueError(\n                f\"All treatment samples have {self.target_col} = 0, relative effect \"\n                f\"{average_effect} will have no effect\"\n            )\n\n    def check_target_is_not_negative(self, df: pd.DataFrame) -&gt; Optional[NoReturn]:\n        if any(df[self.target_col] &lt; 0):\n            raise ValueError(\n                f\"All {self.target_col} values need to be positive or 0, \"\n                f\"got {df[self.target_col].min()}\"\n            )\n\n    def check_average_effect_greater_than(\n        self, average_effect: float, x: float\n    ) -&gt; Optional[NoReturn]:\n        if average_effect &lt;= x:\n            raise ValueError(\n                f\"Simulated effect needs to be greater than {x*100:.0f}%, got \"\n                f\"{average_effect*100:.1f}%\"\n            )\n\n    def apply_multiplicative_effect(\n        self, df: pd.DataFrame, effect: Union[float, np.ndarray]\n    ) -&gt; pd.DataFrame:\n        df.loc[df[self.treatment_col] == self.treatment, self.target_col] *= 1 + effect\n        return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.RelativePositivePerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Usage: <pre><code>from cluster_experiments.perturbator import RelativePositivePerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = RelativePositivePerturbator()\n# Increase target metric by 50%\nperturbator.perturbate(df, average_effect=0.5)\n# returns pd.DataFrame({\"target\": [1, 3, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n</code></pre></p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Usage:\n    ```python\n    from cluster_experiments.perturbator import RelativePositivePerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = RelativePositivePerturbator()\n    # Increase target metric by 50%\n    perturbator.perturbate(df, average_effect=0.5)\n    # returns pd.DataFrame({\"target\": [1, 3, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    self.check_relative_positive_effect(df, average_effect)\n    df = self.apply_multiplicative_effect(df, average_effect)\n    return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.SegmentedBetaRelativePerturbator","title":"<code>SegmentedBetaRelativePerturbator</code>","text":"<p>               Bases: <code>BetaRelativePositivePerturbator</code></p> <p>A stochastic Perturbator for continuous targets that applies a sampled effect from the Beta distribution. It applies the effect multiplicatively and based on given segments. For each segment, the average segment effect is sampled from a beta distribution with support in (0, 1). Within each segment, the individual effects are sampled from a beta distribution with mean equal to the segment average effect and support in (range_min, range_max).</p> <p>The number of samples with 0 as target remains unchanged.</p> <p>For additional details and recommendations on the parameters, see the documentation for the <code>BetaRelativePerturbator</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>average_effect</code> <code>Optional[float]</code> <p>the average effect of the treatment. Defaults to None.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>name of the target_col to use as the outcome. Defaults to \"target\".</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>the name of the column that contains the treatment. Defaults to \"treatment\".</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>name of the treatment to use as the treated group. Defaults to \"B\".</p> <code>'B'</code> <code>scale</code> <code>Optional[float]</code> <p>the scale of the effect distribution. Defaults to None.</p> <code>None</code> <code>range_min</code> <code>float</code> <p>the minimum value of the target range, must be &gt;-1. Defaults to -0.8, which allows for up to 5x decreases of the target.</p> <code>None</code> <code>range_max</code> <code>float</code> <p>the maximum value of the target range. Defaults to 4, which allows for up to 5x increases of the target.</p> <code>None</code> <code>segment_cols</code> <code>Optional[List[str]]</code> <p>the columns to use for segmenting. Defaults to None.</p> required Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class SegmentedBetaRelativePerturbator(BetaRelativePositivePerturbator):\n    \"\"\"\n    A stochastic Perturbator for continuous targets that applies a sampled\n    effect from the Beta distribution. It applies the effect multiplicatively\n    and based on given segments.\n    For each segment, the average segment effect is sampled from a beta\n    distribution with support in (0, 1). Within each segment, the individual\n    effects are sampled from a beta distribution with mean equal to the segment\n    average effect and support in (range_min, range_max).\n\n    The number of samples with 0 as target remains unchanged.\n\n    For additional details and recommendations on the parameters, see the\n    documentation for the `BetaRelativePerturbator` class.\n\n    Arguments:\n        average_effect (Optional[float], optional): the average effect of the treatment. Defaults to None.\n        target_col (str, optional): name of the target_col to use as the outcome. Defaults to \"target\".\n        treatment_col (str, optional): the name of the column that contains the treatment. Defaults to \"treatment\".\n        treatment (str, optional): name of the treatment to use as the treated group. Defaults to \"B\".\n        scale (Optional[float], optional): the scale of the effect distribution. Defaults to None.\n        range_min (float, optional): the minimum value of the target range, must be &gt;-1.\n            Defaults to -0.8, which allows for up to 5x decreases of the target.\n        range_max (float, optional): the maximum value of the target range.\n            Defaults to 4, which allows for up to 5x increases of the target.\n        segment_cols (Optional[List[str]], optional): the columns to use for segmenting. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        segment_cols: List[str],\n        average_effect: Optional[float] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        scale: Optional[float] = None,\n        range_min: Optional[float] = None,\n        range_max: Optional[float] = None,\n    ):\n        super().__init__(average_effect, target_col, treatment_col, treatment, scale)\n        self._range_min = range_min or -0.8\n        self._range_max = range_max or 4\n        self._segment_cols = segment_cols\n        self.segment_col = self._get_segment_col_name(segment_cols)\n\n    @staticmethod\n    def _get_segment_col_name(segment_cols: List[str]):\n        if not isinstance(segment_cols, list):\n            raise ValueError(\n                f\"segment_cols must be of type List[str], got type {type(segment_cols)}\"\n            )\n        return \"_cluster_\" + \"_\".join(segment_cols)\n\n    def _set_segment_col_values(self, df: pd.DataFrame):\n        if self.segment_col in df.columns:\n            raise ValueError(\n                f\"Cannot use {self.segment_col=} as perturbator clustering \"\n                f\"column, as it already exists in the input dataframe!\"\n            )\n        return df.copy().assign(\n            **{self.segment_col: df[self._segment_cols].astype(str).sum(axis=1)}\n        )\n\n    def get_cluster_perturbator_fixed_params(\n        self, average_effect: Optional[float] = None\n    ) -&gt; Dict[str, Any]:\n        average_effect = self.get_average_effect(average_effect)\n        self.check_average_effect_greater_than(average_effect, x=0)\n        self.check_average_effect_less_than(average_effect, x=1)\n        scale = self.get_scale(average_effect)\n        return {\n            \"average_effect\": average_effect,\n            \"scale\": scale,\n        }\n\n    def get_cluster_perturbator(self, **kwargs) -&gt; Perturbator:\n        sampled_effect = self._sample_beta_effect(\n            kwargs[\"average_effect\"], kwargs[\"scale\"], 1\n        )\n        cluster_perturbator = BetaRelativePerturbator(\n            average_effect=sampled_effect,\n            target_col=self.target_col,\n            treatment_col=self.treatment_col,\n            treatment=self.treatment,\n            range_min=self._range_min,\n            range_max=self._range_max,\n        )\n        return cluster_perturbator\n\n    def perturbate(\n        self,\n        df: pd.DataFrame,\n        average_effect: Optional[float] = None,\n    ) -&gt; pd.DataFrame:\n        df = df.copy().reset_index(drop=True)\n        df = self._set_segment_col_values(df)\n\n        cluster_perturbator_params = self.get_cluster_perturbator_fixed_params(\n            average_effect\n        )\n        df_perturbed = pd.concat(\n            [\n                self.get_cluster_perturbator(**cluster_perturbator_params).perturbate(\n                    df=df[df[self.segment_col] == cluster].copy()\n                )\n                for cluster in df[self.segment_col].unique()\n            ]\n        )\n        return df_perturbed.drop(columns=self.segment_col).reset_index(drop=True)\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n        return cls(\n            average_effect=config.average_effect,\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            scale=config.scale,\n            range_min=config.range_min,\n            range_max=config.range_max,\n            segment_cols=config.segment_cols,\n        )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.SegmentedBetaRelativePerturbator.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a Perturbator object from a PowerConfig object</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a Perturbator object from a PowerConfig object\"\"\"\n    return cls(\n        average_effect=config.average_effect,\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        scale=config.scale,\n        range_min=config.range_min,\n        range_max=config.range_max,\n        segment_cols=config.segment_cols,\n    )\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.UniformPerturbator","title":"<code>UniformPerturbator</code>","text":"<p>               Bases: <code>Perturbator</code></p> <p>UniformPerturbator is a Perturbator that adds a constant effect to the target column of the treated instances.</p> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>class UniformPerturbator(Perturbator):\n    \"\"\"\n    UniformPerturbator is a Perturbator that adds a constant effect to the target column of the treated instances.\n    \"\"\"\n\n    def __init__(\n        self,\n        average_effect: Optional[float] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n    ):\n        super().__init__(average_effect, target_col, treatment_col, treatment)\n        logging.warning(\n            \"UniformPerturbator is deprecated and will be removed in future versions. \"\n            \"Use ConstantPerturbator instead.\"\n        )\n\n    def perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Usage:\n\n        ```python\n        from cluster_experiments.perturbator import UniformPerturbator\n        import pandas as pd\n        df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n        perturbator = UniformPerturbator()\n        perturbator.perturbate(df, average_effect=1)\n        ```\n        \"\"\"\n        df = df.copy().reset_index(drop=True)\n        average_effect = self.get_average_effect(average_effect)\n        df = self.apply_additive_effect(df, average_effect)\n        return df\n\n    def apply_additive_effect(\n        self, df: pd.DataFrame, effect: Union[float, np.ndarray]\n    ) -&gt; pd.DataFrame:\n        df.loc[df[self.treatment_col] == self.treatment, self.target_col] += effect\n        return df\n</code></pre>"},{"location":"api/perturbator.html#cluster_experiments.perturbator.UniformPerturbator.perturbate","title":"<code>perturbate(df, average_effect=None)</code>","text":"<p>Usage:</p> <pre><code>from cluster_experiments.perturbator import UniformPerturbator\nimport pandas as pd\ndf = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\nperturbator = UniformPerturbator()\nperturbator.perturbate(df, average_effect=1)\n</code></pre> Source code in <code>cluster_experiments/perturbator.py</code> <pre><code>def perturbate(\n    self, df: pd.DataFrame, average_effect: Optional[float] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Usage:\n\n    ```python\n    from cluster_experiments.perturbator import UniformPerturbator\n    import pandas as pd\n    df = pd.DataFrame({\"target\": [1, 2, 3], \"treatment\": [\"A\", \"B\", \"A\"]})\n    perturbator = UniformPerturbator()\n    perturbator.perturbate(df, average_effect=1)\n    ```\n    \"\"\"\n    df = df.copy().reset_index(drop=True)\n    average_effect = self.get_average_effect(average_effect)\n    df = self.apply_additive_effect(df, average_effect)\n    return df\n</code></pre>"},{"location":"api/power_analysis.html","title":"<code>from cluster_experiments.power_analysis import *</code>","text":""},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis","title":"<code>NormalPowerAnalysis</code>","text":"<p>Class used to run Power analysis, using the central limit theorem to estimate power based on standard errors of the estimator, and the fact that the coefficients of a regression are normally distributed. It does so by running simulations. In each simulation: 1. Assign treatment to dataframe randomly 2. Add pre-experiment data if needed 3. Get standard error from analysis</p> <p>Finally it returns the power of the analysis by counting how many times the effect was detected.</p> <p>Parameters:</p> Name Type Description Default <code>splitter</code> <code>RandomSplitter</code> <p>RandomSplitter class to randomly assign treatment to dataframe.</p> required <code>analysis</code> <code>ExperimentAnalysis</code> <p>ExperimentAnalysis class to use for analysis.</p> required <code>cupac_model</code> <code>Optional[BaseEstimator]</code> <p>Sklearn estimator class to add pre-experiment data to dataframe. If None, no pre-experiment data will be added.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>Name of the column with the outcome variable.</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>value of treatment_col considered to be treatment (not control)</p> <code>'B'</code> <code>control</code> <code>str</code> <p>value of treatment_col considered to be control (not treatment)</p> <code>'A'</code> <code>n_simulations</code> <code>int</code> <p>Number of simulations to run.</p> <code>100</code> <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>0.05</code> <code>features_cupac_model</code> <code>Optional[List[str]]</code> <p>Covariates to be used in cupac model</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Optional. Seed to use for the splitter.</p> <code>None</code> <p>Usage: <pre><code>from datetime import date\n\nimport numpy as np\nimport pandas as pd\nfrom cluster_experiments.experiment_analysis import GeeExperimentAnalysis\nfrom cluster_experiments.power_analysis import NormalPowerAnalysis\nfrom cluster_experiments.random_splitter import ClusteredSplitter\n\nN = 1_000\nusers = [f\"User {i}\" for i in range(1000)]\nclusters = [f\"Cluster {i}\" for i in range(100)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)]\ndf = pd.DataFrame(\n    {\n        \"cluster\": np.random.choice(clusters, size=N),\n        \"target\": np.random.normal(0, 1, size=N),\n        \"user\": np.random.choice(users, size=N),\n        \"date\": np.random.choice(dates, size=N),\n    }\n)\n\nexperiment_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(15, 32)]\nsw = ClusteredSplitter(\n    cluster_cols=[\"cluster\", \"date\"],\n)\n\nanalysis = GeeExperimentAnalysis(\n    cluster_cols=[\"cluster\", \"date\"],\n)\n\npw = NormalPowerAnalysis(\n    splitter=sw, analysis=analysis, n_simulations=50\n)\n\npower = pw.power_analysis(df, average_effect=0.1)\nprint(f\"{power = }\")\n</code></pre></p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>class NormalPowerAnalysis:\n    \"\"\"\n    Class used to run Power analysis, using the central limit theorem to estimate power based on standard errors of the estimator,\n    and the fact that the coefficients of a regression are normally distributed.\n    It does so by running simulations. In each simulation:\n    1. Assign treatment to dataframe randomly\n    2. Add pre-experiment data if needed\n    3. Get standard error from analysis\n\n    Finally it returns the power of the analysis by counting how many times the effect was detected.\n\n    Args:\n        splitter: RandomSplitter class to randomly assign treatment to dataframe.\n        analysis: ExperimentAnalysis class to use for analysis.\n        cupac_model: Sklearn estimator class to add pre-experiment data to dataframe. If None, no pre-experiment data will be added.\n        target_col: Name of the column with the outcome variable.\n        treatment_col: Name of the column with the treatment variable.\n        treatment: value of treatment_col considered to be treatment (not control)\n        control: value of treatment_col considered to be control (not treatment)\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n        features_cupac_model: Covariates to be used in cupac model\n        seed: Optional. Seed to use for the splitter.\n\n    Usage:\n    ```python\n    from datetime import date\n\n    import numpy as np\n    import pandas as pd\n    from cluster_experiments.experiment_analysis import GeeExperimentAnalysis\n    from cluster_experiments.power_analysis import NormalPowerAnalysis\n    from cluster_experiments.random_splitter import ClusteredSplitter\n\n    N = 1_000\n    users = [f\"User {i}\" for i in range(1000)]\n    clusters = [f\"Cluster {i}\" for i in range(100)]\n    dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)]\n    df = pd.DataFrame(\n        {\n            \"cluster\": np.random.choice(clusters, size=N),\n            \"target\": np.random.normal(0, 1, size=N),\n            \"user\": np.random.choice(users, size=N),\n            \"date\": np.random.choice(dates, size=N),\n        }\n    )\n\n    experiment_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(15, 32)]\n    sw = ClusteredSplitter(\n        cluster_cols=[\"cluster\", \"date\"],\n    )\n\n    analysis = GeeExperimentAnalysis(\n        cluster_cols=[\"cluster\", \"date\"],\n    )\n\n    pw = NormalPowerAnalysis(\n        splitter=sw, analysis=analysis, n_simulations=50\n    )\n\n    power = pw.power_analysis(df, average_effect=0.1)\n    print(f\"{power = }\")\n    ```\n    \"\"\"\n\n    VALID_AGG_FUNCS = (\n        \"sum\",\n        \"mean\",\n        \"median\",\n        \"min\",\n        \"max\",\n        \"count\",\n        \"std\",\n        \"var\",\n        \"nunique\",\n        \"first\",\n        \"last\",\n    )\n\n    def __init__(\n        self,\n        splitter: RandomSplitter,\n        analysis: ExperimentAnalysis,\n        cupac_model: Optional[BaseEstimator] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        control: str = \"A\",\n        n_simulations: int = 100,\n        alpha: float = 0.05,\n        features_cupac_model: Optional[List[str]] = None,\n        scale_col: Optional[str] = None,\n        seed: Optional[int] = None,\n        hypothesis: str = \"two-sided\",\n        time_col: Optional[str] = None,\n    ):\n        self.splitter = splitter\n        self.analysis = analysis\n        self.n_simulations = n_simulations\n        self.target_col = target_col\n        self.treatment = treatment\n        self.control = control\n        self.treatment_col = treatment_col\n        self.alpha = alpha\n        self.hypothesis = hypothesis\n        self.time_col = time_col\n        self.scale_col = scale_col\n\n        self.cupac_handler = CupacHandler(\n            cupac_model=cupac_model,\n            target_col=target_col,\n            features_cupac_model=features_cupac_model,\n            scale_col=scale_col,\n        )\n        if seed is not None:\n            random.seed(seed)  # seed for splitter\n            np.random.seed(seed)  # numpy seed\n            # may need to seed other stochasticity sources if added\n\n        self.check_inputs()\n\n    def _split(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Split dataframe.\n        Args:\n            df: Dataframe with outcome variable\n        \"\"\"\n        treatment_df = self.splitter.assign_treatment_df(df)\n        self.log_nulls(treatment_df)\n        treatment_df = treatment_df.query(\n            f\"{self.treatment_col}.notnull()\", engine=\"python\"\n        ).query(\n            f\"{self.treatment_col}.isin(['{self.treatment}', '{self.control}'])\",\n            engine=\"python\",\n        )\n        return treatment_df\n\n    def _get_standard_error(\n        self,\n        df: pd.DataFrame,\n        n_simulations: int,\n        verbose: bool,\n    ) -&gt; Generator[float, None, None]:\n        for _ in tqdm(range(n_simulations), disable=not verbose):\n            split_df = self._split(df)\n            yield self.analysis.get_standard_error(split_df)\n\n    def _normal_power_calculation(\n        self, alpha: float, std_error: float, average_effect: float\n    ) -&gt; float:\n        \"\"\"Returns the power of the analysis using the normal distribution.\n        Arguments:\n            alpha: significance level\n            std_error: standard error of the analysis\n            average_effect: effect size of the analysis\n        \"\"\"\n        if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.LESS:\n            z_alpha = norm.ppf(alpha)\n            return float(norm.cdf(z_alpha - average_effect / std_error))\n\n        if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.GREATER:\n            z_alpha = norm.ppf(1 - alpha)\n            return 1 - float(norm.cdf(z_alpha - average_effect / std_error))\n\n        if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.TWO_SIDED:\n            z_alpha = norm.ppf(1 - alpha / 2)\n            norm_cdf_right = norm.cdf(z_alpha - average_effect / std_error)\n            norm_cdf_left = norm.cdf(-z_alpha - average_effect / std_error)\n            return float(norm_cdf_left + (1 - norm_cdf_right))\n\n        raise ValueError(f\"{self.analysis.hypothesis} is not a valid HypothesisEntries\")\n\n    def _normal_mde_calculation(\n        self, alpha: float, std_error: float, power: float\n    ) -&gt; float:\n        \"\"\"\n        Returns the minimum detectable effect of the analysis using the normal distribution.\n        Args:\n            alpha: Significance level.\n            std_error: Standard error of the analysis.\n            power: Power of the analysis.\n        \"\"\"\n        if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.LESS:\n            z_alpha = norm.ppf(alpha)\n            z_beta = norm.ppf(1 - power)\n        elif HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.GREATER:\n            z_alpha = norm.ppf(1 - alpha)\n            z_beta = norm.ppf(power)\n        elif HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.TWO_SIDED:\n            # we are neglecting norm_cdf_left\n            z_alpha = norm.ppf(1 - alpha / 2)\n            z_beta = norm.ppf(power)\n        else:\n            raise ValueError(\n                f\"{self.analysis.hypothesis} is not a valid HypothesisEntries\"\n            )\n\n        return float(z_alpha + z_beta) * std_error\n\n    def _get_time_col(self) -&gt; str:\n        if self.time_col is None:\n            raise ValueError(\n                \"Time column not specified. You must provide `time_col` when initializing NormalPowerAnalysis.\"\n            )\n        return self.time_col\n\n    def mde_power_line(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        powers: Iterable[float] = (),\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n    ) -&gt; Dict[float, float]:\n        \"\"\"\n        Returns the minimum detectable effect of the analysis.\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            power: Power of the analysis.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        alpha = self.alpha if alpha is None else alpha\n        std_error = self._get_average_standard_error(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            n_simulations=n_simulations,\n        )\n        return {\n            power: self._normal_mde_calculation(\n                alpha=alpha, std_error=std_error, power=power\n            )\n            for power in powers\n        }\n\n    def mde(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        power: float = 0.8,\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n    ) -&gt; float:\n        \"\"\"\n        Returns the minimum detectable effect of the analysis.\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            power: Power of the analysis.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        return self.mde_power_line(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            powers=[power],\n            n_simulations=n_simulations,\n            alpha=alpha,\n        )[power]\n\n    def _get_average_standard_error(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        n_simulations: Optional[int] = None,\n    ) -&gt; float:\n        \"\"\"\n        Gets standard error to be used in normal power calculation.\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effects: Average effects to test.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        n_simulations = self.n_simulations if n_simulations is None else n_simulations\n\n        df = df.copy()\n        df = self.cupac_handler.add_covariates(df, pre_experiment_df)\n\n        std_errors = list(self._get_standard_error(df, n_simulations, verbose))\n        std_error_mean = float(np.mean(std_errors))\n\n        return std_error_mean\n\n    def run_average_standard_error(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        n_simulations: Optional[int] = None,\n        experiment_length: Iterable[int] = (),\n    ) -&gt; Generator[Tuple[float, int], None, None]:\n        \"\"\"\n        Run power analysis by simulation, using standard errors from the analysis.\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            n_simulations: Number of simulations to run.\n            experiment_length: Length of the experiment in days.\n        \"\"\"\n        n_simulations = self.n_simulations if n_simulations is None else n_simulations\n        time_col = self._get_time_col()\n\n        for n_days in experiment_length:\n            df_time = df.copy()\n            experiment_start = df_time[time_col].min()\n            df_time = df_time.loc[\n                df_time[time_col] &lt; experiment_start + pd.Timedelta(days=n_days)\n            ]\n            std_error_mean = self._get_average_standard_error(\n                df=df_time,\n                pre_experiment_df=pre_experiment_df,\n                verbose=verbose,\n                n_simulations=n_simulations,\n            )\n            yield std_error_mean, n_days\n\n    def power_time_line(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effects: Iterable[float] = (),\n        experiment_length: Iterable[int] = (),\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n    ) -&gt; List[Dict]:\n        \"\"\"\n        Run power analysis by simulation, using standard errors from the analysis.\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effects: Average effects to test.\n            experiment_length: Length of the experiment in days.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        alpha = self.alpha if alpha is None else alpha\n\n        results = []\n        for std_error_mean, n_days in self.run_average_standard_error(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            n_simulations=n_simulations,\n            experiment_length=experiment_length,\n        ):\n            for effect in average_effects:\n                power = self._normal_power_calculation(\n                    alpha=alpha, std_error=std_error_mean, average_effect=effect\n                )\n                results.append(\n                    {\"effect\": effect, \"power\": power, \"experiment_length\": n_days}\n                )\n\n        return results\n\n    def mde_time_line(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        powers: Iterable[float] = (),\n        experiment_length: Iterable[int] = (),\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n    ) -&gt; List[Dict]:\n        alpha = self.alpha if alpha is None else alpha\n\n        results = []\n        for std_error_mean, n_days in self.run_average_standard_error(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            n_simulations=n_simulations,\n            experiment_length=experiment_length,\n        ):\n            for power in powers:\n                mde = self._normal_mde_calculation(\n                    alpha=alpha, std_error=std_error_mean, power=power\n                )\n                results.append(\n                    {\"power\": power, \"mde\": mde, \"experiment_length\": n_days}\n                )\n        return results\n\n    def mde_rolling_time_line(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        powers: Iterable[float] = (),\n        experiment_length: Iterable[int] = (),\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n        *,\n        agg_func: Literal[\n            \"sum\",\n            \"mean\",\n            \"median\",\n            \"min\",\n            \"max\",\n            \"count\",\n            \"std\",\n            \"var\",\n            \"nunique\",\n            \"first\",\n            \"last\",\n        ],\n        post_process_func: Optional[Callable[[float], float]] = None,\n    ) -&gt; List[Dict]:\n        \"\"\"\n        Computes the Minimum Detectable Effect (MDE) for varying experiment lengths\n        using a sliding time window, with optional element-wise post-processing\n        on the aggregated metric.\n\n        Args:\n            df: Input DataFrame.\n            pre_experiment_df: Optional pre-experiment DataFrame.\n            powers: Iterable of powers for MDE computation (e.g., [0.8, 0.9]).\n            experiment_length: Iterable of experiment durations in days.\n            n_simulations: Number of simulations to run (default = self.n_simulations).\n            alpha: Significance level (default = self.alpha).\n            agg_func: Aggregation function applied to the metric in each cluster window.\n            post_process_func: Optional callable applied element-wise to the aggregated metric\n                (like `Series.apply`). Must take a single scalar as input and return a scalar.\n\n        Usage:\n\n        ```python\n        import pandas as pd\n        import numpy as np\n        from cluster_experiments.random_splitter import ClusteredSplitter\n        from cluster_experiments.experiment_analysis import ClusteredOLSAnalysis\n        from cluster_experiments.power_analysis import NormalPowerAnalysis\n\n        np.random.seed(42)\n\n        # Create a synthetic dataset\n        n_customers = 10\n        n_days = 60\n\n        df = pd.DataFrame({\n            \"customer_id\": np.repeat(np.arange(1, n_customers + 1), n_days),\n            \"date\": np.tile(pd.date_range(\"2024-01-01\", periods=n_days), n_customers),\n        })\n\n        p_values = np.concatenate([\n            np.full(20, 0.1),\n            np.full(20, 0.2),\n            np.full(20, 0.3),\n        ])\n\n        p = np.tile(p_values, n_customers)\n        df[\"conversion\"] = np.random.binomial(1, p)\n\n        # Define a post-processing function\n        def flag_positive(x):\n            return 1 if x &gt; 0 else 0\n\n        splitter = ClusteredSplitter(\n            cluster_cols=['customer_id'],\n            splitter_weights=[0.5, 0.5],\n            treatments=['A', 'B'],\n        )\n\n        analysis = ClusteredOLSAnalysis(\n            cluster_cols=['customer_id'],\n            target_col='conversion',\n        )\n\n        pw = NormalPowerAnalysis(\n            splitter=splitter,\n            analysis=analysis,\n            target_col=\"conversion\",\n            treatment=\"B\",\n            control=\"A\",\n            n_simulations=100,\n            time_col=\"date\",\n        )\n\n        results = pw.mde_rolling_time_line(\n            df=df,\n            pre_experiment_df=None,\n            powers=[0.8],\n            experiment_length=[7, 14, 21, 28, 56],\n            n_simulations=5,\n            alpha=0.05,\n            agg_func=\"sum\",\n            post_process_func=flag_positive,\n        )\n        ```\n        \"\"\"\n        time_col = self._get_time_col()\n\n        if agg_func not in self.VALID_AGG_FUNCS:\n            raise ValueError(\n                f\"Invalid aggregation function `{agg_func}`. \"\n                f\"Choose one of: {', '.join(self.VALID_AGG_FUNCS)}.\"\n            )\n\n        alpha = self.alpha if alpha is None else alpha\n        n_simulations = self.n_simulations if n_simulations is None else n_simulations\n        cluster_cols = self.splitter.cluster_cols\n        results = []\n\n        experiment_start = df[time_col].min()\n\n        for n_days in experiment_length:\n            df_time_filter = df[\n                df[time_col] &lt;= experiment_start + pd.Timedelta(days=n_days)\n            ]\n\n            df_grouped = df_time_filter.groupby(cluster_cols, as_index=False)[\n                self.target_col\n            ].agg(agg_func)\n\n            if post_process_func is not None:\n                df_grouped[self.target_col] = df_grouped[self.target_col].apply(\n                    post_process_func\n                )\n\n            std_error_mean = self._get_average_standard_error(\n                df=df_grouped,\n                pre_experiment_df=pre_experiment_df,\n                n_simulations=n_simulations,\n            )\n\n            for power in powers:\n                mde_value = self._normal_mde_calculation(\n                    alpha=alpha, std_error=std_error_mean, power=power\n                )\n\n                relative_mde = mde_value / abs(df_grouped[self.target_col].mean())\n\n                results.append(\n                    {\n                        \"power\": power,\n                        \"mde\": mde_value,\n                        \"experiment_length\": n_days,\n                        \"relative_mde\": relative_mde,\n                    }\n                )\n\n        return results\n\n    def power_line(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effects: Iterable[float] = (),\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n    ) -&gt; Dict[float, float]:\n        \"\"\"\n        Run power analysis by simulation, using standard errors from the analysis.\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effects: Average effects to test.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        alpha = self.alpha if alpha is None else alpha\n\n        std_error_mean = self._get_average_standard_error(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            n_simulations=n_simulations,\n        )\n\n        return {\n            effect: self._normal_power_calculation(\n                alpha=alpha, std_error=std_error_mean, average_effect=effect\n            )\n            for effect in average_effects\n        }\n\n    def power_analysis(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effect: float = 0.0,\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n    ) -&gt; float:\n        \"\"\"\n        Run power analysis by simulation, using standard errors from the analysis.\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effect: Average effect of treatment.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        return self.power_line(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            average_effects=[average_effect],\n            n_simulations=n_simulations,\n            alpha=alpha,\n        )[average_effect]\n\n    def log_nulls(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Warns about dropping nulls in treatment column\"\"\"\n        n_nulls = len(df.query(f\"{self.treatment_col}.isnull()\", engine=\"python\"))\n        if n_nulls &gt; 0:\n            logging.warning(\n                f\"There are {n_nulls} null values in treatment, dropping them\"\n            )\n\n    @classmethod\n    def from_dict(cls, config_dict: dict) -&gt; \"NormalPowerAnalysis\":\n        \"\"\"Constructs PowerAnalysis from dictionary\"\"\"\n        config = PowerConfig(**config_dict)\n        return cls.from_config(config)\n\n    @classmethod\n    def from_config(cls, config: PowerConfig) -&gt; \"NormalPowerAnalysis\":\n        \"\"\"Constructs PowerAnalysis from PowerConfig\"\"\"\n        splitter_cls = _get_mapping_key(splitter_mapping, config.splitter)\n        analysis_cls = _get_mapping_key(analysis_mapping, config.analysis)\n        cupac_cls = _get_mapping_key(cupac_model_mapping, config.cupac_model)\n        return cls(\n            splitter=splitter_cls.from_config(config),\n            analysis=analysis_cls.from_config(config),\n            cupac_model=cupac_cls.from_config(config),\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            control=config.control,\n            n_simulations=config.n_simulations,\n            alpha=config.alpha,\n            features_cupac_model=config.features_cupac_model,\n            seed=config.seed,\n            hypothesis=config.hypothesis,\n            time_col=config.time_col,\n            scale_col=config.scale_col,\n        )\n\n    def check_treatment_col(self):\n        \"\"\"Checks consistency of treatment column\"\"\"\n        assert (\n            self.analysis.treatment_col == self.treatment_col\n        ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in PowerAnalysis ({self.treatment_col})\"\n\n        assert (\n            self.analysis.treatment_col == self.splitter.treatment_col\n        ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in splitter ({self.splitter.treatment_col})\"\n\n    def check_target_col(self):\n        assert (\n            self.analysis.target_col == self.target_col\n        ), f\"target_col in analysis ({self.analysis.target_col}) must be the same as target_col in PowerAnalysis ({self.target_col})\"\n\n    def check_treatment(self):\n        assert (\n            self.analysis.treatment == self.treatment\n        ), f\"treatment in analysis ({self.analysis.treatment}) must be the same as treatment in PowerAnalysis ({self.treatment})\"\n\n        assert (\n            self.analysis.treatment in self.splitter.treatments\n        ), f\"treatment in analysis ({self.analysis.treatment}) must be in treatments in splitter ({self.splitter.treatments})\"\n\n        assert (\n            self.control in self.splitter.treatments\n        ), f\"control in power analysis ({self.control}) must be in treatments in splitter ({self.splitter.treatments})\"\n\n    def check_covariates(self):\n        if hasattr(self.analysis, \"covariates\"):\n            cupac_in_covariates = (\n                self.cupac_handler.cupac_outcome_name in self.analysis.covariates\n            )\n\n            assert cupac_in_covariates or not self.cupac_handler.is_cupac, (\n                f\"covariates in analysis must contain {self.cupac_handler.cupac_outcome_name} if cupac_model is not None. \"\n                f\"If you want to use cupac_model, you must add the cupac outcome to the covariates of the analysis \"\n                f\"You may want to do covariates=['{self.cupac_handler.cupac_outcome_name}'] in your analysis method or your config\"\n            )\n\n            if hasattr(self.splitter, \"cluster_cols\"):\n                if set(self.analysis.covariates).intersection(\n                    set(self.splitter.cluster_cols)\n                ):\n                    logging.warning(\n                        f\"covariates in analysis ({self.analysis.covariates}) are also cluster_cols in splitter ({self.splitter.cluster_cols}). \"\n                        f\"Be specially careful when using switchback splitters, since the time splitter column is being overriden\"\n                    )\n\n    def check_clusters(self):\n        has_analysis_clusters = hasattr(self.analysis, \"cluster_cols\")\n        has_splitter_clusters = hasattr(self.splitter, \"cluster_cols\")\n        not_cluster_cols_cond = not has_analysis_clusters or not has_splitter_clusters\n        assert (\n            not_cluster_cols_cond\n            or self.analysis.cluster_cols == self.splitter.cluster_cols\n        ), f\"cluster_cols in analysis ({self.analysis.cluster_cols}) must be the same as cluster_cols in splitter ({self.splitter.cluster_cols})\"\n\n        assert (\n            has_splitter_clusters\n            or not has_analysis_clusters\n            or not self.analysis.cluster_cols\n            or isinstance(self.splitter, RepeatedSampler)\n        ), \"analysis has cluster_cols but splitter does not.\"\n\n        assert (\n            has_analysis_clusters\n            or not has_splitter_clusters\n            or not self.splitter.cluster_cols\n        ), \"splitter has cluster_cols but analysis does not.\"\n\n        has_time_col = hasattr(self.splitter, \"time_col\")\n        assert not (\n            has_time_col\n            and has_splitter_clusters\n            and self.splitter.time_col not in self.splitter.cluster_cols\n        ), \"in switchback splitters, time_col must be in cluster_cols\"\n\n    def check_scale_col(self):\n        if self.scale_col is not None:\n            if not isinstance(self.analysis, DeltaMethodAnalysis):\n                raise ValueError(\n                    \"If scale_col is provided, the analysis method must be DeltaMethodAnalysis, since it is the only one that supports scale_col.\"\n                )\n\n    def check_inputs(self):\n        self.check_covariates()\n        self.check_treatment_col()\n        self.check_target_col()\n        self.check_treatment()\n        self.check_clusters()\n        self.check_scale_col()\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis._get_average_standard_error","title":"<code>_get_average_standard_error(df, pre_experiment_df=None, verbose=False, n_simulations=None)</code>","text":"<p>Gets standard error to be used in normal power calculation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>average_effects</code> <p>Average effects to test.</p> required <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run.</p> <code>None</code> <code>alpha</code> <p>Significance level.</p> required Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _get_average_standard_error(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    n_simulations: Optional[int] = None,\n) -&gt; float:\n    \"\"\"\n    Gets standard error to be used in normal power calculation.\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effects: Average effects to test.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    n_simulations = self.n_simulations if n_simulations is None else n_simulations\n\n    df = df.copy()\n    df = self.cupac_handler.add_covariates(df, pre_experiment_df)\n\n    std_errors = list(self._get_standard_error(df, n_simulations, verbose))\n    std_error_mean = float(np.mean(std_errors))\n\n    return std_error_mean\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis._normal_mde_calculation","title":"<code>_normal_mde_calculation(alpha, std_error, power)</code>","text":"<p>Returns the minimum detectable effect of the analysis using the normal distribution. Args:     alpha: Significance level.     std_error: Standard error of the analysis.     power: Power of the analysis.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _normal_mde_calculation(\n    self, alpha: float, std_error: float, power: float\n) -&gt; float:\n    \"\"\"\n    Returns the minimum detectable effect of the analysis using the normal distribution.\n    Args:\n        alpha: Significance level.\n        std_error: Standard error of the analysis.\n        power: Power of the analysis.\n    \"\"\"\n    if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.LESS:\n        z_alpha = norm.ppf(alpha)\n        z_beta = norm.ppf(1 - power)\n    elif HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.GREATER:\n        z_alpha = norm.ppf(1 - alpha)\n        z_beta = norm.ppf(power)\n    elif HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.TWO_SIDED:\n        # we are neglecting norm_cdf_left\n        z_alpha = norm.ppf(1 - alpha / 2)\n        z_beta = norm.ppf(power)\n    else:\n        raise ValueError(\n            f\"{self.analysis.hypothesis} is not a valid HypothesisEntries\"\n        )\n\n    return float(z_alpha + z_beta) * std_error\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis._normal_power_calculation","title":"<code>_normal_power_calculation(alpha, std_error, average_effect)</code>","text":"<p>Returns the power of the analysis using the normal distribution. Arguments:     alpha: significance level     std_error: standard error of the analysis     average_effect: effect size of the analysis</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _normal_power_calculation(\n    self, alpha: float, std_error: float, average_effect: float\n) -&gt; float:\n    \"\"\"Returns the power of the analysis using the normal distribution.\n    Arguments:\n        alpha: significance level\n        std_error: standard error of the analysis\n        average_effect: effect size of the analysis\n    \"\"\"\n    if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.LESS:\n        z_alpha = norm.ppf(alpha)\n        return float(norm.cdf(z_alpha - average_effect / std_error))\n\n    if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.GREATER:\n        z_alpha = norm.ppf(1 - alpha)\n        return 1 - float(norm.cdf(z_alpha - average_effect / std_error))\n\n    if HypothesisEntries(self.analysis.hypothesis) == HypothesisEntries.TWO_SIDED:\n        z_alpha = norm.ppf(1 - alpha / 2)\n        norm_cdf_right = norm.cdf(z_alpha - average_effect / std_error)\n        norm_cdf_left = norm.cdf(-z_alpha - average_effect / std_error)\n        return float(norm_cdf_left + (1 - norm_cdf_right))\n\n    raise ValueError(f\"{self.analysis.hypothesis} is not a valid HypothesisEntries\")\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis._split","title":"<code>_split(df)</code>","text":"<p>Split dataframe. Args:     df: Dataframe with outcome variable</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _split(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Split dataframe.\n    Args:\n        df: Dataframe with outcome variable\n    \"\"\"\n    treatment_df = self.splitter.assign_treatment_df(df)\n    self.log_nulls(treatment_df)\n    treatment_df = treatment_df.query(\n        f\"{self.treatment_col}.notnull()\", engine=\"python\"\n    ).query(\n        f\"{self.treatment_col}.isin(['{self.treatment}', '{self.control}'])\",\n        engine=\"python\",\n    )\n    return treatment_df\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.check_treatment_col","title":"<code>check_treatment_col()</code>","text":"<p>Checks consistency of treatment column</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def check_treatment_col(self):\n    \"\"\"Checks consistency of treatment column\"\"\"\n    assert (\n        self.analysis.treatment_col == self.treatment_col\n    ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in PowerAnalysis ({self.treatment_col})\"\n\n    assert (\n        self.analysis.treatment_col == self.splitter.treatment_col\n    ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in splitter ({self.splitter.treatment_col})\"\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Constructs PowerAnalysis from PowerConfig</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config: PowerConfig) -&gt; \"NormalPowerAnalysis\":\n    \"\"\"Constructs PowerAnalysis from PowerConfig\"\"\"\n    splitter_cls = _get_mapping_key(splitter_mapping, config.splitter)\n    analysis_cls = _get_mapping_key(analysis_mapping, config.analysis)\n    cupac_cls = _get_mapping_key(cupac_model_mapping, config.cupac_model)\n    return cls(\n        splitter=splitter_cls.from_config(config),\n        analysis=analysis_cls.from_config(config),\n        cupac_model=cupac_cls.from_config(config),\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        control=config.control,\n        n_simulations=config.n_simulations,\n        alpha=config.alpha,\n        features_cupac_model=config.features_cupac_model,\n        seed=config.seed,\n        hypothesis=config.hypothesis,\n        time_col=config.time_col,\n        scale_col=config.scale_col,\n    )\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Constructs PowerAnalysis from dictionary</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; \"NormalPowerAnalysis\":\n    \"\"\"Constructs PowerAnalysis from dictionary\"\"\"\n    config = PowerConfig(**config_dict)\n    return cls.from_config(config)\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.log_nulls","title":"<code>log_nulls(df)</code>","text":"<p>Warns about dropping nulls in treatment column</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def log_nulls(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Warns about dropping nulls in treatment column\"\"\"\n    n_nulls = len(df.query(f\"{self.treatment_col}.isnull()\", engine=\"python\"))\n    if n_nulls &gt; 0:\n        logging.warning(\n            f\"There are {n_nulls} null values in treatment, dropping them\"\n        )\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.mde","title":"<code>mde(df, pre_experiment_df=None, verbose=False, power=0.8, n_simulations=None, alpha=None)</code>","text":"<p>Returns the minimum detectable effect of the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>power</code> <code>float</code> <p>Power of the analysis.</p> <code>0.8</code> <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Significance level.</p> <code>None</code> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def mde(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    power: float = 0.8,\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; float:\n    \"\"\"\n    Returns the minimum detectable effect of the analysis.\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        power: Power of the analysis.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    return self.mde_power_line(\n        df=df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        powers=[power],\n        n_simulations=n_simulations,\n        alpha=alpha,\n    )[power]\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.mde_power_line","title":"<code>mde_power_line(df, pre_experiment_df=None, verbose=False, powers=(), n_simulations=None, alpha=None)</code>","text":"<p>Returns the minimum detectable effect of the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>power</code> <p>Power of the analysis.</p> required <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Significance level.</p> <code>None</code> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def mde_power_line(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    powers: Iterable[float] = (),\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; Dict[float, float]:\n    \"\"\"\n    Returns the minimum detectable effect of the analysis.\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        power: Power of the analysis.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    alpha = self.alpha if alpha is None else alpha\n    std_error = self._get_average_standard_error(\n        df=df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        n_simulations=n_simulations,\n    )\n    return {\n        power: self._normal_mde_calculation(\n            alpha=alpha, std_error=std_error, power=power\n        )\n        for power in powers\n    }\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.mde_rolling_time_line","title":"<code>mde_rolling_time_line(df, pre_experiment_df=None, powers=(), experiment_length=(), n_simulations=None, alpha=None, *, agg_func, post_process_func=None)</code>","text":"<p>Computes the Minimum Detectable Effect (MDE) for varying experiment lengths using a sliding time window, with optional element-wise post-processing on the aggregated metric.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Optional pre-experiment DataFrame.</p> <code>None</code> <code>powers</code> <code>Iterable[float]</code> <p>Iterable of powers for MDE computation (e.g., [0.8, 0.9]).</p> <code>()</code> <code>experiment_length</code> <code>Iterable[int]</code> <p>Iterable of experiment durations in days.</p> <code>()</code> <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run (default = self.n_simulations).</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Significance level (default = self.alpha).</p> <code>None</code> <code>agg_func</code> <code>Literal['sum', 'mean', 'median', 'min', 'max', 'count', 'std', 'var', 'nunique', 'first', 'last']</code> <p>Aggregation function applied to the metric in each cluster window.</p> required <code>post_process_func</code> <code>Optional[Callable[[float], float]]</code> <p>Optional callable applied element-wise to the aggregated metric (like <code>Series.apply</code>). Must take a single scalar as input and return a scalar.</p> <code>None</code> <p>Usage:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom cluster_experiments.random_splitter import ClusteredSplitter\nfrom cluster_experiments.experiment_analysis import ClusteredOLSAnalysis\nfrom cluster_experiments.power_analysis import NormalPowerAnalysis\n\nnp.random.seed(42)\n\n# Create a synthetic dataset\nn_customers = 10\nn_days = 60\n\ndf = pd.DataFrame({\n    \"customer_id\": np.repeat(np.arange(1, n_customers + 1), n_days),\n    \"date\": np.tile(pd.date_range(\"2024-01-01\", periods=n_days), n_customers),\n})\n\np_values = np.concatenate([\n    np.full(20, 0.1),\n    np.full(20, 0.2),\n    np.full(20, 0.3),\n])\n\np = np.tile(p_values, n_customers)\ndf[\"conversion\"] = np.random.binomial(1, p)\n\n# Define a post-processing function\ndef flag_positive(x):\n    return 1 if x &gt; 0 else 0\n\nsplitter = ClusteredSplitter(\n    cluster_cols=['customer_id'],\n    splitter_weights=[0.5, 0.5],\n    treatments=['A', 'B'],\n)\n\nanalysis = ClusteredOLSAnalysis(\n    cluster_cols=['customer_id'],\n    target_col='conversion',\n)\n\npw = NormalPowerAnalysis(\n    splitter=splitter,\n    analysis=analysis,\n    target_col=\"conversion\",\n    treatment=\"B\",\n    control=\"A\",\n    n_simulations=100,\n    time_col=\"date\",\n)\n\nresults = pw.mde_rolling_time_line(\n    df=df,\n    pre_experiment_df=None,\n    powers=[0.8],\n    experiment_length=[7, 14, 21, 28, 56],\n    n_simulations=5,\n    alpha=0.05,\n    agg_func=\"sum\",\n    post_process_func=flag_positive,\n)\n</code></pre> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def mde_rolling_time_line(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    powers: Iterable[float] = (),\n    experiment_length: Iterable[int] = (),\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n    *,\n    agg_func: Literal[\n        \"sum\",\n        \"mean\",\n        \"median\",\n        \"min\",\n        \"max\",\n        \"count\",\n        \"std\",\n        \"var\",\n        \"nunique\",\n        \"first\",\n        \"last\",\n    ],\n    post_process_func: Optional[Callable[[float], float]] = None,\n) -&gt; List[Dict]:\n    \"\"\"\n    Computes the Minimum Detectable Effect (MDE) for varying experiment lengths\n    using a sliding time window, with optional element-wise post-processing\n    on the aggregated metric.\n\n    Args:\n        df: Input DataFrame.\n        pre_experiment_df: Optional pre-experiment DataFrame.\n        powers: Iterable of powers for MDE computation (e.g., [0.8, 0.9]).\n        experiment_length: Iterable of experiment durations in days.\n        n_simulations: Number of simulations to run (default = self.n_simulations).\n        alpha: Significance level (default = self.alpha).\n        agg_func: Aggregation function applied to the metric in each cluster window.\n        post_process_func: Optional callable applied element-wise to the aggregated metric\n            (like `Series.apply`). Must take a single scalar as input and return a scalar.\n\n    Usage:\n\n    ```python\n    import pandas as pd\n    import numpy as np\n    from cluster_experiments.random_splitter import ClusteredSplitter\n    from cluster_experiments.experiment_analysis import ClusteredOLSAnalysis\n    from cluster_experiments.power_analysis import NormalPowerAnalysis\n\n    np.random.seed(42)\n\n    # Create a synthetic dataset\n    n_customers = 10\n    n_days = 60\n\n    df = pd.DataFrame({\n        \"customer_id\": np.repeat(np.arange(1, n_customers + 1), n_days),\n        \"date\": np.tile(pd.date_range(\"2024-01-01\", periods=n_days), n_customers),\n    })\n\n    p_values = np.concatenate([\n        np.full(20, 0.1),\n        np.full(20, 0.2),\n        np.full(20, 0.3),\n    ])\n\n    p = np.tile(p_values, n_customers)\n    df[\"conversion\"] = np.random.binomial(1, p)\n\n    # Define a post-processing function\n    def flag_positive(x):\n        return 1 if x &gt; 0 else 0\n\n    splitter = ClusteredSplitter(\n        cluster_cols=['customer_id'],\n        splitter_weights=[0.5, 0.5],\n        treatments=['A', 'B'],\n    )\n\n    analysis = ClusteredOLSAnalysis(\n        cluster_cols=['customer_id'],\n        target_col='conversion',\n    )\n\n    pw = NormalPowerAnalysis(\n        splitter=splitter,\n        analysis=analysis,\n        target_col=\"conversion\",\n        treatment=\"B\",\n        control=\"A\",\n        n_simulations=100,\n        time_col=\"date\",\n    )\n\n    results = pw.mde_rolling_time_line(\n        df=df,\n        pre_experiment_df=None,\n        powers=[0.8],\n        experiment_length=[7, 14, 21, 28, 56],\n        n_simulations=5,\n        alpha=0.05,\n        agg_func=\"sum\",\n        post_process_func=flag_positive,\n    )\n    ```\n    \"\"\"\n    time_col = self._get_time_col()\n\n    if agg_func not in self.VALID_AGG_FUNCS:\n        raise ValueError(\n            f\"Invalid aggregation function `{agg_func}`. \"\n            f\"Choose one of: {', '.join(self.VALID_AGG_FUNCS)}.\"\n        )\n\n    alpha = self.alpha if alpha is None else alpha\n    n_simulations = self.n_simulations if n_simulations is None else n_simulations\n    cluster_cols = self.splitter.cluster_cols\n    results = []\n\n    experiment_start = df[time_col].min()\n\n    for n_days in experiment_length:\n        df_time_filter = df[\n            df[time_col] &lt;= experiment_start + pd.Timedelta(days=n_days)\n        ]\n\n        df_grouped = df_time_filter.groupby(cluster_cols, as_index=False)[\n            self.target_col\n        ].agg(agg_func)\n\n        if post_process_func is not None:\n            df_grouped[self.target_col] = df_grouped[self.target_col].apply(\n                post_process_func\n            )\n\n        std_error_mean = self._get_average_standard_error(\n            df=df_grouped,\n            pre_experiment_df=pre_experiment_df,\n            n_simulations=n_simulations,\n        )\n\n        for power in powers:\n            mde_value = self._normal_mde_calculation(\n                alpha=alpha, std_error=std_error_mean, power=power\n            )\n\n            relative_mde = mde_value / abs(df_grouped[self.target_col].mean())\n\n            results.append(\n                {\n                    \"power\": power,\n                    \"mde\": mde_value,\n                    \"experiment_length\": n_days,\n                    \"relative_mde\": relative_mde,\n                }\n            )\n\n    return results\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.power_analysis","title":"<code>power_analysis(df, pre_experiment_df=None, verbose=False, average_effect=0.0, n_simulations=None, alpha=None)</code>","text":"<p>Run power analysis by simulation, using standard errors from the analysis. Args:     df: Dataframe with outcome and treatment variables.     pre_experiment_df: Dataframe with pre-experiment data.     verbose: Whether to show progress bar.     average_effect: Average effect of treatment.     n_simulations: Number of simulations to run.     alpha: Significance level.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def power_analysis(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effect: float = 0.0,\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; float:\n    \"\"\"\n    Run power analysis by simulation, using standard errors from the analysis.\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effect: Average effect of treatment.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    return self.power_line(\n        df=df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        average_effects=[average_effect],\n        n_simulations=n_simulations,\n        alpha=alpha,\n    )[average_effect]\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.power_line","title":"<code>power_line(df, pre_experiment_df=None, verbose=False, average_effects=(), n_simulations=None, alpha=None)</code>","text":"<p>Run power analysis by simulation, using standard errors from the analysis. Args:     df: Dataframe with outcome and treatment variables.     pre_experiment_df: Dataframe with pre-experiment data.     verbose: Whether to show progress bar.     average_effects: Average effects to test.     n_simulations: Number of simulations to run.     alpha: Significance level.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def power_line(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effects: Iterable[float] = (),\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; Dict[float, float]:\n    \"\"\"\n    Run power analysis by simulation, using standard errors from the analysis.\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effects: Average effects to test.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    alpha = self.alpha if alpha is None else alpha\n\n    std_error_mean = self._get_average_standard_error(\n        df=df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        n_simulations=n_simulations,\n    )\n\n    return {\n        effect: self._normal_power_calculation(\n            alpha=alpha, std_error=std_error_mean, average_effect=effect\n        )\n        for effect in average_effects\n    }\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.power_time_line","title":"<code>power_time_line(df, pre_experiment_df=None, verbose=False, average_effects=(), experiment_length=(), n_simulations=None, alpha=None)</code>","text":"<p>Run power analysis by simulation, using standard errors from the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>average_effects</code> <code>Iterable[float]</code> <p>Average effects to test.</p> <code>()</code> <code>experiment_length</code> <code>Iterable[int]</code> <p>Length of the experiment in days.</p> <code>()</code> <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Significance level.</p> <code>None</code> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def power_time_line(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effects: Iterable[float] = (),\n    experiment_length: Iterable[int] = (),\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; List[Dict]:\n    \"\"\"\n    Run power analysis by simulation, using standard errors from the analysis.\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effects: Average effects to test.\n        experiment_length: Length of the experiment in days.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    alpha = self.alpha if alpha is None else alpha\n\n    results = []\n    for std_error_mean, n_days in self.run_average_standard_error(\n        df=df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        n_simulations=n_simulations,\n        experiment_length=experiment_length,\n    ):\n        for effect in average_effects:\n            power = self._normal_power_calculation(\n                alpha=alpha, std_error=std_error_mean, average_effect=effect\n            )\n            results.append(\n                {\"effect\": effect, \"power\": power, \"experiment_length\": n_days}\n            )\n\n    return results\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.NormalPowerAnalysis.run_average_standard_error","title":"<code>run_average_standard_error(df, pre_experiment_df=None, verbose=False, n_simulations=None, experiment_length=())</code>","text":"<p>Run power analysis by simulation, using standard errors from the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run.</p> <code>None</code> <code>experiment_length</code> <code>Iterable[int]</code> <p>Length of the experiment in days.</p> <code>()</code> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def run_average_standard_error(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    n_simulations: Optional[int] = None,\n    experiment_length: Iterable[int] = (),\n) -&gt; Generator[Tuple[float, int], None, None]:\n    \"\"\"\n    Run power analysis by simulation, using standard errors from the analysis.\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        n_simulations: Number of simulations to run.\n        experiment_length: Length of the experiment in days.\n    \"\"\"\n    n_simulations = self.n_simulations if n_simulations is None else n_simulations\n    time_col = self._get_time_col()\n\n    for n_days in experiment_length:\n        df_time = df.copy()\n        experiment_start = df_time[time_col].min()\n        df_time = df_time.loc[\n            df_time[time_col] &lt; experiment_start + pd.Timedelta(days=n_days)\n        ]\n        std_error_mean = self._get_average_standard_error(\n            df=df_time,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            n_simulations=n_simulations,\n        )\n        yield std_error_mean, n_days\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis","title":"<code>PowerAnalysis</code>","text":"<p>Class used to run Power analysis. It does so by running simulations. In each simulation: 1. Assign treatment to dataframe randomly 2. Perturbate dataframe 3. Add pre-experiment data if needed 4. Run analysis</p> <p>Finally it returns the power of the analysis by counting how many times the effect was detected.</p> <p>Parameters:</p> Name Type Description Default <code>perturbator</code> <code>Perturbator</code> <p>Perturbator class to perturbate dataframe with treatment assigned.</p> required <code>splitter</code> <code>RandomSplitter</code> <p>RandomSplitter class to randomly assign treatment to dataframe.</p> required <code>analysis</code> <code>ExperimentAnalysis</code> <p>ExperimentAnalysis class to use for analysis.</p> required <code>cupac_model</code> <code>Optional[BaseEstimator]</code> <p>Sklearn estimator class to add pre-experiment data to dataframe. If None, no pre-experiment data will be added.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>Name of the column with the outcome variable.</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>value of treatment_col considered to be treatment (not control)</p> <code>'B'</code> <code>control</code> <code>str</code> <p>value of treatment_col considered to be control (not treatment)</p> <code>'A'</code> <code>n_simulations</code> <code>int</code> <p>Number of simulations to run.</p> <code>100</code> <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>0.05</code> <code>features_cupac_model</code> <code>Optional[List[str]]</code> <p>Covariates to be used in cupac model</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Optional. Seed to use for the splitter.</p> <code>None</code> <p>Usage: <pre><code>from datetime import date\n\nimport numpy as np\nimport pandas as pd\nfrom cluster_experiments.experiment_analysis import GeeExperimentAnalysis\nfrom cluster_experiments.perturbator import ConstantPerturbator\nfrom cluster_experiments.power_analysis import PowerAnalysis\nfrom cluster_experiments.random_splitter import ClusteredSplitter\n\nN = 1_000\nusers = [f\"User {i}\" for i in range(1000)]\nclusters = [f\"Cluster {i}\" for i in range(100)]\ndates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)]\ndf = pd.DataFrame(\n    {\n        \"cluster\": np.random.choice(clusters, size=N),\n        \"target\": np.random.normal(0, 1, size=N),\n        \"user\": np.random.choice(users, size=N),\n        \"date\": np.random.choice(dates, size=N),\n    }\n)\n\nexperiment_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(15, 32)]\nsw = ClusteredSplitter(\n    cluster_cols=[\"cluster\", \"date\"],\n)\n\nperturbator = ConstantPerturbator()\n\nanalysis = GeeExperimentAnalysis(\n    cluster_cols=[\"cluster\", \"date\"],\n)\n\npw = PowerAnalysis(\n    perturbator=perturbator, splitter=sw, analysis=analysis, n_simulations=50\n)\n\npower = pw.power_analysis(df, average_effect=0.1)\nprint(f\"{power = }\")\n</code></pre></p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>class PowerAnalysis:\n    \"\"\"\n    Class used to run Power analysis. It does so by running simulations. In each simulation:\n    1. Assign treatment to dataframe randomly\n    2. Perturbate dataframe\n    3. Add pre-experiment data if needed\n    4. Run analysis\n\n    Finally it returns the power of the analysis by counting how many times the effect was detected.\n\n    Args:\n        perturbator: Perturbator class to perturbate dataframe with treatment assigned.\n        splitter: RandomSplitter class to randomly assign treatment to dataframe.\n        analysis: ExperimentAnalysis class to use for analysis.\n        cupac_model: Sklearn estimator class to add pre-experiment data to dataframe. If None, no pre-experiment data will be added.\n        target_col: Name of the column with the outcome variable.\n        treatment_col: Name of the column with the treatment variable.\n        treatment: value of treatment_col considered to be treatment (not control)\n        control: value of treatment_col considered to be control (not treatment)\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n        features_cupac_model: Covariates to be used in cupac model\n        seed: Optional. Seed to use for the splitter.\n\n    Usage:\n    ```python\n    from datetime import date\n\n    import numpy as np\n    import pandas as pd\n    from cluster_experiments.experiment_analysis import GeeExperimentAnalysis\n    from cluster_experiments.perturbator import ConstantPerturbator\n    from cluster_experiments.power_analysis import PowerAnalysis\n    from cluster_experiments.random_splitter import ClusteredSplitter\n\n    N = 1_000\n    users = [f\"User {i}\" for i in range(1000)]\n    clusters = [f\"Cluster {i}\" for i in range(100)]\n    dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(1, 32)]\n    df = pd.DataFrame(\n        {\n            \"cluster\": np.random.choice(clusters, size=N),\n            \"target\": np.random.normal(0, 1, size=N),\n            \"user\": np.random.choice(users, size=N),\n            \"date\": np.random.choice(dates, size=N),\n        }\n    )\n\n    experiment_dates = [f\"{date(2022, 1, i):%Y-%m-%d}\" for i in range(15, 32)]\n    sw = ClusteredSplitter(\n        cluster_cols=[\"cluster\", \"date\"],\n    )\n\n    perturbator = ConstantPerturbator()\n\n    analysis = GeeExperimentAnalysis(\n        cluster_cols=[\"cluster\", \"date\"],\n    )\n\n    pw = PowerAnalysis(\n        perturbator=perturbator, splitter=sw, analysis=analysis, n_simulations=50\n    )\n\n    power = pw.power_analysis(df, average_effect=0.1)\n    print(f\"{power = }\")\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        perturbator: Perturbator,\n        splitter: RandomSplitter,\n        analysis: ExperimentAnalysis,\n        cupac_model: Optional[BaseEstimator] = None,\n        target_col: str = \"target\",\n        treatment_col: str = \"treatment\",\n        treatment: str = \"B\",\n        control: str = \"A\",\n        n_simulations: int = 100,\n        alpha: float = 0.05,\n        features_cupac_model: Optional[List[str]] = None,\n        scale_col: Optional[str] = None,\n        seed: Optional[int] = None,\n        hypothesis: str = \"two-sided\",\n    ):\n        self.perturbator = perturbator\n        self.splitter = splitter\n        self.analysis = analysis\n        self.n_simulations = n_simulations\n        self.target_col = target_col\n        self.treatment = treatment\n        self.control = control\n        self.treatment_col = treatment_col\n        self.alpha = alpha\n        self.hypothesis = hypothesis\n        self.scale_col = scale_col\n\n        self.cupac_handler = CupacHandler(\n            cupac_model=cupac_model,\n            target_col=target_col,\n            scale_col=scale_col,\n            features_cupac_model=features_cupac_model,\n        )\n        if seed is not None:\n            random.seed(seed)  # seed for splitter\n            np.random.seed(seed)  # seed for the binary perturbator\n            # may need to seed other stochasticity sources if added\n\n        self.check_inputs()\n\n    def _simulate_perturbed_df(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effect: Optional[float] = None,\n        n_simulations: int = 100,\n    ) -&gt; Generator[pd.DataFrame, None, None]:\n        \"\"\"Yields splitted + perturbated dataframe for each iteration of the simulation.\"\"\"\n        df = df.copy()\n        df = self.cupac_handler.add_covariates(df, pre_experiment_df)\n\n        for _ in tqdm(range(n_simulations), disable=not verbose):\n            yield self._split_and_perturbate(df, average_effect)\n\n    def simulate_pvalue(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effect: Optional[float] = None,\n        n_simulations: int = 100,\n    ) -&gt; Generator[float, None, None]:\n        \"\"\"\n        Yields p-values for each iteration of the simulation.\n        In general, this is to be used in power_analysis method. However,\n        if you're interested in the distribution of p-values, you can use this method to generate them.\n        Args:\n            df: Dataframe with outcome variable.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n            n_simulations: Number of simulations to run.\n        \"\"\"\n        for perturbed_df in self._simulate_perturbed_df(\n            df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            average_effect=average_effect,\n            n_simulations=n_simulations,\n        ):\n            yield self.analysis.get_pvalue(perturbed_df)\n\n    def running_power_analysis(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effect: Optional[float] = None,\n        n_simulations: int = 100,\n    ) -&gt; Generator[float, None, None]:\n        \"\"\"\n        Yields running power for each iteration of the simulation.\n        if you're interested in getting the power at each iteration, you can use this method to generate them.\n        Args:\n            df: Dataframe with outcome variable.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n            n_simulations: Number of simulations to run.\n        \"\"\"\n        n_rejected = 0\n        for i, perturbed_df in enumerate(\n            self._simulate_perturbed_df(\n                df,\n                pre_experiment_df=pre_experiment_df,\n                verbose=verbose,\n                average_effect=average_effect,\n                n_simulations=n_simulations,\n            )\n        ):\n            p_value = self.analysis.get_pvalue(perturbed_df)\n            n_rejected += int(p_value &lt; self.alpha)\n            yield n_rejected / (i + 1)\n\n    def simulate_point_estimate(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effect: Optional[float] = None,\n        n_simulations: int = 100,\n    ) -&gt; Generator[float, None, None]:\n        \"\"\"\n        Yields point estimates for each iteration of the simulation.\n        In general, this is to be used in power_analysis method. However,\n        if you're interested in the distribution of point estimates, you can use this method to generate them.\n\n        This is an experimental feature and it might change in the future.\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n            n_simulations: Number of simulations to run.\n        \"\"\"\n        for perturbed_df in self._simulate_perturbed_df(\n            df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            average_effect=average_effect,\n            n_simulations=n_simulations,\n        ):\n            yield self.analysis.get_point_estimate(perturbed_df)\n\n    def power_analysis(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effect: Optional[float] = None,\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n        n_jobs: int = 1,\n    ) -&gt; float:\n        \"\"\"\n        Run power analysis by simulation\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n            n_jobs: Number of jobs to run in parallel. If 1, it will run in serial.\n        \"\"\"\n        n_simulations = self.n_simulations if n_simulations is None else n_simulations\n        alpha = self.alpha if alpha is None else alpha\n\n        df = df.copy()\n        df = self.cupac_handler.add_covariates(df, pre_experiment_df)\n\n        if n_jobs == 1:\n            return self._non_parallel_loop(\n                df, average_effect, n_simulations, alpha, verbose\n            )\n        elif n_jobs &gt; 1 or n_jobs == -1:\n            return self._parallel_loop(\n                df, average_effect, n_simulations, alpha, verbose, n_jobs\n            )\n        else:\n            raise ValueError(\"n_jobs must be greater than 0, or -1.\")\n\n    def _split(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Split dataframe.\n        Args:\n            df: Dataframe with outcome variable\n        \"\"\"\n        treatment_df = self.splitter.assign_treatment_df(df)\n        self.log_nulls(treatment_df)\n        treatment_df = treatment_df.query(\n            f\"{self.treatment_col}.notnull()\", engine=\"python\"\n        ).query(\n            f\"{self.treatment_col}.isin(['{self.treatment}', '{self.control}'])\",\n            engine=\"python\",\n        )\n\n        return treatment_df\n\n    def _perturbate(\n        self, treatment_df: pd.DataFrame, average_effect: Optional[float]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Perturbate dataframe using perturbator.\n        Args:\n            df: Dataframe with outcome variable\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        \"\"\"\n\n        perturbed_df = self.perturbator.perturbate(\n            treatment_df, average_effect=average_effect\n        )\n        return perturbed_df\n\n    def _split_and_perturbate(\n        self, df: pd.DataFrame, average_effect: Optional[float]\n    ) -&gt; pd.DataFrame:\n        treatment_df = self._split(df)\n        perturbed_df = self._perturbate(\n            treatment_df=treatment_df, average_effect=average_effect\n        )\n        return perturbed_df\n\n    def _run_simulation(self, args: Tuple[pd.DataFrame, Optional[float]]) -&gt; float:\n        df, average_effect = args\n        perturbed_df = self._split_and_perturbate(df, average_effect)\n        return self.analysis.get_pvalue(perturbed_df)\n\n    def _non_parallel_loop(\n        self,\n        df: pd.DataFrame,\n        average_effect: Optional[float],\n        n_simulations: int,\n        alpha: float,\n        verbose: bool,\n    ) -&gt; float:\n        \"\"\"\n        Run power analysis by simulation in serial\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n        \"\"\"\n        n_detected_mde = 0\n        for _ in tqdm(range(n_simulations), disable=not verbose):\n            p_value = self._run_simulation((df, average_effect))\n            if verbose:\n                print(f\"p_value of simulation run: {p_value:.3f}\")\n            n_detected_mde += p_value &lt; alpha\n\n        return n_detected_mde / n_simulations\n\n    def _parallel_loop(\n        self,\n        df: pd.DataFrame,\n        average_effect: Optional[float],\n        n_simulations: int,\n        alpha: float,\n        verbose: bool,\n        n_jobs: int,\n    ) -&gt; float:\n        \"\"\"\n        Run power analysis by simulation in parallel\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n            n_jobs: Number of jobs to run in parallel.\n        \"\"\"\n        from multiprocessing import Pool, cpu_count\n\n        n_jobs = n_jobs if n_jobs != -1 else cpu_count()\n\n        n_detected_mde = 0\n        with Pool(processes=n_jobs) as pool:\n            args = [(df, average_effect) for _ in range(n_simulations)]\n            results = pool.imap_unordered(self._run_simulation, args)\n            for p_value in tqdm(results, total=n_simulations, disable=not verbose):\n                n_detected_mde += p_value &lt; alpha\n\n        return n_detected_mde / n_simulations\n\n    def power_line(\n        self,\n        df: pd.DataFrame,\n        pre_experiment_df: Optional[pd.DataFrame] = None,\n        verbose: bool = False,\n        average_effects: Iterable[float] = (),\n        n_simulations: Optional[int] = None,\n        alpha: Optional[float] = None,\n        n_jobs: int = 1,\n    ) -&gt; Dict[float, float]:\n        \"\"\"Runs power analysis with multiple average effects\n\n        Args:\n            df: Dataframe with outcome and treatment variables.\n            pre_experiment_df: Dataframe with pre-experiment data.\n            verbose: Whether to show progress bar.\n            average_effects: Average effects to test.\n            n_simulations: Number of simulations to run.\n            alpha: Significance level.\n            n_jobs: Number of jobs to run in parallel.\n\n        Returns:\n            Dictionary with average effects as keys and power as values.\n        \"\"\"\n        return {\n            effect: self.power_analysis(\n                df=df,\n                pre_experiment_df=pre_experiment_df,\n                verbose=verbose,\n                average_effect=effect,\n                n_simulations=n_simulations,\n                alpha=alpha,\n                n_jobs=n_jobs,\n            )\n            for effect in tqdm(\n                list(average_effects), disable=not verbose, desc=\"Effects loop\"\n            )\n        }\n\n    def log_nulls(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Warns about dropping nulls in treatment column\"\"\"\n        n_nulls = len(df.query(f\"{self.treatment_col}.isnull()\", engine=\"python\"))\n        if n_nulls &gt; 0:\n            logging.warning(\n                f\"There are {n_nulls} null values in treatment, dropping them\"\n            )\n\n    @classmethod\n    def from_dict(cls, config_dict: dict) -&gt; \"PowerAnalysis\":\n        \"\"\"Constructs PowerAnalysis from dictionary\"\"\"\n        config = PowerConfig(**config_dict)\n        return cls.from_config(config)\n\n    @classmethod\n    def from_config(cls, config: PowerConfig) -&gt; \"PowerAnalysis\":\n        \"\"\"Constructs PowerAnalysis from PowerConfig\"\"\"\n        perturbator_cls = _get_mapping_key(perturbator_mapping, config.perturbator)\n        splitter_cls = _get_mapping_key(splitter_mapping, config.splitter)\n        analysis_cls = _get_mapping_key(analysis_mapping, config.analysis)\n        cupac_cls = _get_mapping_key(cupac_model_mapping, config.cupac_model)\n        return cls(\n            perturbator=perturbator_cls.from_config(config),\n            splitter=splitter_cls.from_config(config),\n            analysis=analysis_cls.from_config(config),\n            cupac_model=cupac_cls.from_config(config),\n            target_col=config.target_col,\n            treatment_col=config.treatment_col,\n            treatment=config.treatment,\n            control=config.control,\n            n_simulations=config.n_simulations,\n            alpha=config.alpha,\n            features_cupac_model=config.features_cupac_model,\n            seed=config.seed,\n            hypothesis=config.hypothesis,\n            scale_col=config.scale_col,\n        )\n\n    def check_treatment_col(self):\n        \"\"\"Checks consistency of treatment column\"\"\"\n        assert (\n            self.analysis.treatment_col == self.perturbator.treatment_col\n        ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in perturbator ({self.perturbator.treatment_col})\"\n\n        assert (\n            self.analysis.treatment_col == self.treatment_col\n        ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in PowerAnalysis ({self.treatment_col})\"\n\n        assert (\n            self.analysis.treatment_col == self.splitter.treatment_col\n        ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in splitter ({self.splitter.treatment_col})\"\n\n    def check_target_col(self):\n        assert (\n            self.analysis.target_col == self.perturbator.target_col\n        ), f\"target_col in analysis ({self.analysis.target_col}) must be the same as target_col in perturbator ({self.perturbator.target_col})\"\n\n        assert (\n            self.analysis.target_col == self.target_col\n        ), f\"target_col in analysis ({self.analysis.target_col}) must be the same as target_col in PowerAnalysis ({self.target_col})\"\n\n    def check_treatment(self):\n        assert (\n            self.treatment != self.control\n        ), f\"treatment in PowerAnalysis ({self.treatment}) must not be the same as control in PowerAnalysis ({self.control})\"\n\n        assert (\n            self.analysis.treatment == self.perturbator.treatment\n        ), f\"treatment in analysis ({self.analysis.treatment}) must be the same as treatment in perturbator ({self.perturbator.treatment})\"\n\n        assert (\n            self.analysis.treatment == self.treatment\n        ), f\"treatment in analysis ({self.analysis.treatment}) must be the same as treatment in PowerAnalysis ({self.treatment})\"\n\n        assert (\n            self.analysis.treatment in self.splitter.treatments\n        ), f\"treatment in analysis ({self.analysis.treatment}) must be in treatments in splitter ({self.splitter.treatments})\"\n\n        assert (\n            self.control in self.splitter.treatments\n        ), f\"control in power analysis ({self.control}) must be in treatments in splitter ({self.splitter.treatments})\"\n\n    def check_covariates(self):\n        if hasattr(self.analysis, \"covariates\"):\n            cupac_in_covariates = (\n                self.cupac_handler.cupac_outcome_name in self.analysis.covariates\n            )\n            assert cupac_in_covariates or not self.cupac_handler.is_cupac, (\n                f\"covariates in analysis must contain {self.cupac_handler.cupac_outcome_name} if cupac_model is not None. \"\n                f\"If you want to use cupac_model, you must add the cupac outcome to the covariates of the analysis \"\n                f\"You may want to do covariates=['{self.cupac_handler.cupac_outcome_name}'] in your analysis method or your config\"\n            )\n\n            if hasattr(self.splitter, \"cluster_cols\"):\n                if set(self.analysis.covariates).intersection(\n                    set(self.splitter.cluster_cols)\n                ):\n                    logging.warning(\n                        f\"covariates in analysis ({self.analysis.covariates}) are also cluster_cols in splitter ({self.splitter.cluster_cols}). \"\n                        f\"Be specially careful when using switchback splitters, since the time splitter column is being overriden\"\n                    )\n\n    def check_clusters(self):\n        has_analysis_clusters = hasattr(self.analysis, \"cluster_cols\")\n        has_splitter_clusters = hasattr(self.splitter, \"cluster_cols\")\n        not_cluster_cols_cond = not has_analysis_clusters or not has_splitter_clusters\n        assert (\n            not_cluster_cols_cond\n            or self.analysis.cluster_cols == self.splitter.cluster_cols\n        ), f\"cluster_cols in analysis ({self.analysis.cluster_cols}) must be the same as cluster_cols in splitter ({self.splitter.cluster_cols})\"\n\n        assert (\n            has_splitter_clusters\n            or not has_analysis_clusters\n            or not self.analysis.cluster_cols\n            or isinstance(self.splitter, RepeatedSampler)\n        ), \"analysis has cluster_cols but splitter does not.\"\n\n        assert (\n            has_analysis_clusters\n            or not has_splitter_clusters\n            or not self.splitter.cluster_cols\n        ), \"splitter has cluster_cols but analysis does not.\"\n\n        has_time_col = hasattr(self.splitter, \"time_col\")\n        assert not (\n            has_time_col\n            and has_splitter_clusters\n            and self.splitter.time_col not in self.splitter.cluster_cols\n        ), \"in switchback splitters, time_col must be in cluster_cols\"\n\n    def check_scale_col(self):\n        if self.scale_col is not None:\n            if not isinstance(self.analysis, DeltaMethodAnalysis):\n                raise ValueError(\n                    \"If scale_col is provided, the analysis method must be DeltaMethodAnalysis, since it is the only one that supports scale_col.\"\n                )\n\n    def check_inputs(self):\n        self.check_covariates()\n        self.check_treatment_col()\n        self.check_target_col()\n        self.check_treatment()\n        self.check_clusters()\n        self.check_scale_col()\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis._non_parallel_loop","title":"<code>_non_parallel_loop(df, average_effect, n_simulations, alpha, verbose)</code>","text":"<p>Run power analysis by simulation in serial Args:     df: Dataframe with outcome and treatment variables.     average_effect: Average effect of treatment. If None, it will use the perturbator average effect.     n_simulations: Number of simulations to run.     alpha: Significance level.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _non_parallel_loop(\n    self,\n    df: pd.DataFrame,\n    average_effect: Optional[float],\n    n_simulations: int,\n    alpha: float,\n    verbose: bool,\n) -&gt; float:\n    \"\"\"\n    Run power analysis by simulation in serial\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n    \"\"\"\n    n_detected_mde = 0\n    for _ in tqdm(range(n_simulations), disable=not verbose):\n        p_value = self._run_simulation((df, average_effect))\n        if verbose:\n            print(f\"p_value of simulation run: {p_value:.3f}\")\n        n_detected_mde += p_value &lt; alpha\n\n    return n_detected_mde / n_simulations\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis._parallel_loop","title":"<code>_parallel_loop(df, average_effect, n_simulations, alpha, verbose, n_jobs)</code>","text":"<p>Run power analysis by simulation in parallel Args:     df: Dataframe with outcome and treatment variables.     average_effect: Average effect of treatment. If None, it will use the perturbator average effect.     n_simulations: Number of simulations to run.     alpha: Significance level.     n_jobs: Number of jobs to run in parallel.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _parallel_loop(\n    self,\n    df: pd.DataFrame,\n    average_effect: Optional[float],\n    n_simulations: int,\n    alpha: float,\n    verbose: bool,\n    n_jobs: int,\n) -&gt; float:\n    \"\"\"\n    Run power analysis by simulation in parallel\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n        n_jobs: Number of jobs to run in parallel.\n    \"\"\"\n    from multiprocessing import Pool, cpu_count\n\n    n_jobs = n_jobs if n_jobs != -1 else cpu_count()\n\n    n_detected_mde = 0\n    with Pool(processes=n_jobs) as pool:\n        args = [(df, average_effect) for _ in range(n_simulations)]\n        results = pool.imap_unordered(self._run_simulation, args)\n        for p_value in tqdm(results, total=n_simulations, disable=not verbose):\n            n_detected_mde += p_value &lt; alpha\n\n    return n_detected_mde / n_simulations\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis._perturbate","title":"<code>_perturbate(treatment_df, average_effect)</code>","text":"<p>Perturbate dataframe using perturbator. Args:     df: Dataframe with outcome variable     average_effect: Average effect of treatment. If None, it will use the perturbator average effect.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _perturbate(\n    self, treatment_df: pd.DataFrame, average_effect: Optional[float]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perturbate dataframe using perturbator.\n    Args:\n        df: Dataframe with outcome variable\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n    \"\"\"\n\n    perturbed_df = self.perturbator.perturbate(\n        treatment_df, average_effect=average_effect\n    )\n    return perturbed_df\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis._simulate_perturbed_df","title":"<code>_simulate_perturbed_df(df, pre_experiment_df=None, verbose=False, average_effect=None, n_simulations=100)</code>","text":"<p>Yields splitted + perturbated dataframe for each iteration of the simulation.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _simulate_perturbed_df(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effect: Optional[float] = None,\n    n_simulations: int = 100,\n) -&gt; Generator[pd.DataFrame, None, None]:\n    \"\"\"Yields splitted + perturbated dataframe for each iteration of the simulation.\"\"\"\n    df = df.copy()\n    df = self.cupac_handler.add_covariates(df, pre_experiment_df)\n\n    for _ in tqdm(range(n_simulations), disable=not verbose):\n        yield self._split_and_perturbate(df, average_effect)\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis._split","title":"<code>_split(df)</code>","text":"<p>Split dataframe. Args:     df: Dataframe with outcome variable</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def _split(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Split dataframe.\n    Args:\n        df: Dataframe with outcome variable\n    \"\"\"\n    treatment_df = self.splitter.assign_treatment_df(df)\n    self.log_nulls(treatment_df)\n    treatment_df = treatment_df.query(\n        f\"{self.treatment_col}.notnull()\", engine=\"python\"\n    ).query(\n        f\"{self.treatment_col}.isin(['{self.treatment}', '{self.control}'])\",\n        engine=\"python\",\n    )\n\n    return treatment_df\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.check_treatment_col","title":"<code>check_treatment_col()</code>","text":"<p>Checks consistency of treatment column</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def check_treatment_col(self):\n    \"\"\"Checks consistency of treatment column\"\"\"\n    assert (\n        self.analysis.treatment_col == self.perturbator.treatment_col\n    ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in perturbator ({self.perturbator.treatment_col})\"\n\n    assert (\n        self.analysis.treatment_col == self.treatment_col\n    ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in PowerAnalysis ({self.treatment_col})\"\n\n    assert (\n        self.analysis.treatment_col == self.splitter.treatment_col\n    ), f\"treatment_col in analysis ({self.analysis.treatment_col}) must be the same as treatment_col in splitter ({self.splitter.treatment_col})\"\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Constructs PowerAnalysis from PowerConfig</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>@classmethod\ndef from_config(cls, config: PowerConfig) -&gt; \"PowerAnalysis\":\n    \"\"\"Constructs PowerAnalysis from PowerConfig\"\"\"\n    perturbator_cls = _get_mapping_key(perturbator_mapping, config.perturbator)\n    splitter_cls = _get_mapping_key(splitter_mapping, config.splitter)\n    analysis_cls = _get_mapping_key(analysis_mapping, config.analysis)\n    cupac_cls = _get_mapping_key(cupac_model_mapping, config.cupac_model)\n    return cls(\n        perturbator=perturbator_cls.from_config(config),\n        splitter=splitter_cls.from_config(config),\n        analysis=analysis_cls.from_config(config),\n        cupac_model=cupac_cls.from_config(config),\n        target_col=config.target_col,\n        treatment_col=config.treatment_col,\n        treatment=config.treatment,\n        control=config.control,\n        n_simulations=config.n_simulations,\n        alpha=config.alpha,\n        features_cupac_model=config.features_cupac_model,\n        seed=config.seed,\n        hypothesis=config.hypothesis,\n        scale_col=config.scale_col,\n    )\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Constructs PowerAnalysis from dictionary</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; \"PowerAnalysis\":\n    \"\"\"Constructs PowerAnalysis from dictionary\"\"\"\n    config = PowerConfig(**config_dict)\n    return cls.from_config(config)\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.log_nulls","title":"<code>log_nulls(df)</code>","text":"<p>Warns about dropping nulls in treatment column</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def log_nulls(self, df: pd.DataFrame) -&gt; None:\n    \"\"\"Warns about dropping nulls in treatment column\"\"\"\n    n_nulls = len(df.query(f\"{self.treatment_col}.isnull()\", engine=\"python\"))\n    if n_nulls &gt; 0:\n        logging.warning(\n            f\"There are {n_nulls} null values in treatment, dropping them\"\n        )\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.power_analysis","title":"<code>power_analysis(df, pre_experiment_df=None, verbose=False, average_effect=None, n_simulations=None, alpha=None, n_jobs=1)</code>","text":"<p>Run power analysis by simulation Args:     df: Dataframe with outcome and treatment variables.     pre_experiment_df: Dataframe with pre-experiment data.     verbose: Whether to show progress bar.     average_effect: Average effect of treatment. If None, it will use the perturbator average effect.     n_simulations: Number of simulations to run.     alpha: Significance level.     n_jobs: Number of jobs to run in parallel. If 1, it will run in serial.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def power_analysis(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effect: Optional[float] = None,\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n    n_jobs: int = 1,\n) -&gt; float:\n    \"\"\"\n    Run power analysis by simulation\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n        n_jobs: Number of jobs to run in parallel. If 1, it will run in serial.\n    \"\"\"\n    n_simulations = self.n_simulations if n_simulations is None else n_simulations\n    alpha = self.alpha if alpha is None else alpha\n\n    df = df.copy()\n    df = self.cupac_handler.add_covariates(df, pre_experiment_df)\n\n    if n_jobs == 1:\n        return self._non_parallel_loop(\n            df, average_effect, n_simulations, alpha, verbose\n        )\n    elif n_jobs &gt; 1 or n_jobs == -1:\n        return self._parallel_loop(\n            df, average_effect, n_simulations, alpha, verbose, n_jobs\n        )\n    else:\n        raise ValueError(\"n_jobs must be greater than 0, or -1.\")\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.power_line","title":"<code>power_line(df, pre_experiment_df=None, verbose=False, average_effects=(), n_simulations=None, alpha=None, n_jobs=1)</code>","text":"<p>Runs power analysis with multiple average effects</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>average_effects</code> <code>Iterable[float]</code> <p>Average effects to test.</p> <code>()</code> <code>n_simulations</code> <code>Optional[int]</code> <p>Number of simulations to run.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>Significance level.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[float, float]</code> <p>Dictionary with average effects as keys and power as values.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def power_line(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effects: Iterable[float] = (),\n    n_simulations: Optional[int] = None,\n    alpha: Optional[float] = None,\n    n_jobs: int = 1,\n) -&gt; Dict[float, float]:\n    \"\"\"Runs power analysis with multiple average effects\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effects: Average effects to test.\n        n_simulations: Number of simulations to run.\n        alpha: Significance level.\n        n_jobs: Number of jobs to run in parallel.\n\n    Returns:\n        Dictionary with average effects as keys and power as values.\n    \"\"\"\n    return {\n        effect: self.power_analysis(\n            df=df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            average_effect=effect,\n            n_simulations=n_simulations,\n            alpha=alpha,\n            n_jobs=n_jobs,\n        )\n        for effect in tqdm(\n            list(average_effects), disable=not verbose, desc=\"Effects loop\"\n        )\n    }\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.running_power_analysis","title":"<code>running_power_analysis(df, pre_experiment_df=None, verbose=False, average_effect=None, n_simulations=100)</code>","text":"<p>Yields running power for each iteration of the simulation. if you're interested in getting the power at each iteration, you can use this method to generate them. Args:     df: Dataframe with outcome variable.     pre_experiment_df: Dataframe with pre-experiment data.     verbose: Whether to show progress bar.     average_effect: Average effect of treatment. If None, it will use the perturbator average effect.     n_simulations: Number of simulations to run.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def running_power_analysis(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effect: Optional[float] = None,\n    n_simulations: int = 100,\n) -&gt; Generator[float, None, None]:\n    \"\"\"\n    Yields running power for each iteration of the simulation.\n    if you're interested in getting the power at each iteration, you can use this method to generate them.\n    Args:\n        df: Dataframe with outcome variable.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        n_simulations: Number of simulations to run.\n    \"\"\"\n    n_rejected = 0\n    for i, perturbed_df in enumerate(\n        self._simulate_perturbed_df(\n            df,\n            pre_experiment_df=pre_experiment_df,\n            verbose=verbose,\n            average_effect=average_effect,\n            n_simulations=n_simulations,\n        )\n    ):\n        p_value = self.analysis.get_pvalue(perturbed_df)\n        n_rejected += int(p_value &lt; self.alpha)\n        yield n_rejected / (i + 1)\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.simulate_point_estimate","title":"<code>simulate_point_estimate(df, pre_experiment_df=None, verbose=False, average_effect=None, n_simulations=100)</code>","text":"<p>Yields point estimates for each iteration of the simulation. In general, this is to be used in power_analysis method. However, if you're interested in the distribution of point estimates, you can use this method to generate them.</p> <p>This is an experimental feature and it might change in the future.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with outcome and treatment variables.</p> required <code>pre_experiment_df</code> <code>Optional[DataFrame]</code> <p>Dataframe with pre-experiment data.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to show progress bar.</p> <code>False</code> <code>average_effect</code> <code>Optional[float]</code> <p>Average effect of treatment. If None, it will use the perturbator average effect.</p> <code>None</code> <code>n_simulations</code> <code>int</code> <p>Number of simulations to run.</p> <code>100</code> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def simulate_point_estimate(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effect: Optional[float] = None,\n    n_simulations: int = 100,\n) -&gt; Generator[float, None, None]:\n    \"\"\"\n    Yields point estimates for each iteration of the simulation.\n    In general, this is to be used in power_analysis method. However,\n    if you're interested in the distribution of point estimates, you can use this method to generate them.\n\n    This is an experimental feature and it might change in the future.\n\n    Args:\n        df: Dataframe with outcome and treatment variables.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        n_simulations: Number of simulations to run.\n    \"\"\"\n    for perturbed_df in self._simulate_perturbed_df(\n        df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        average_effect=average_effect,\n        n_simulations=n_simulations,\n    ):\n        yield self.analysis.get_point_estimate(perturbed_df)\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysis.simulate_pvalue","title":"<code>simulate_pvalue(df, pre_experiment_df=None, verbose=False, average_effect=None, n_simulations=100)</code>","text":"<p>Yields p-values for each iteration of the simulation. In general, this is to be used in power_analysis method. However, if you're interested in the distribution of p-values, you can use this method to generate them. Args:     df: Dataframe with outcome variable.     pre_experiment_df: Dataframe with pre-experiment data.     verbose: Whether to show progress bar.     average_effect: Average effect of treatment. If None, it will use the perturbator average effect.     n_simulations: Number of simulations to run.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>def simulate_pvalue(\n    self,\n    df: pd.DataFrame,\n    pre_experiment_df: Optional[pd.DataFrame] = None,\n    verbose: bool = False,\n    average_effect: Optional[float] = None,\n    n_simulations: int = 100,\n) -&gt; Generator[float, None, None]:\n    \"\"\"\n    Yields p-values for each iteration of the simulation.\n    In general, this is to be used in power_analysis method. However,\n    if you're interested in the distribution of p-values, you can use this method to generate them.\n    Args:\n        df: Dataframe with outcome variable.\n        pre_experiment_df: Dataframe with pre-experiment data.\n        verbose: Whether to show progress bar.\n        average_effect: Average effect of treatment. If None, it will use the perturbator average effect.\n        n_simulations: Number of simulations to run.\n    \"\"\"\n    for perturbed_df in self._simulate_perturbed_df(\n        df,\n        pre_experiment_df=pre_experiment_df,\n        verbose=verbose,\n        average_effect=average_effect,\n        n_simulations=n_simulations,\n    ):\n        yield self.analysis.get_pvalue(perturbed_df)\n</code></pre>"},{"location":"api/power_analysis.html#cluster_experiments.power_analysis.PowerAnalysisWithPreExperimentData","title":"<code>PowerAnalysisWithPreExperimentData</code>","text":"<p>               Bases: <code>PowerAnalysis</code></p> <p>This is intended to work mainly for diff-in-diff or synthetic control-like estimators, and NOT for cases of CUPED/CUPAC. Same as PowerAnalysis, but allowing a perturbation only at experiment period and keeping pre-experiment df intact. Using this class, the pre experiment df is also available when the class is instantiated.</p> Source code in <code>cluster_experiments/power_analysis.py</code> <pre><code>class PowerAnalysisWithPreExperimentData(PowerAnalysis):\n    \"\"\"\n    This is intended to work mainly for diff-in-diff or synthetic control-like estimators, and NOT for cases of CUPED/CUPAC.\n    Same as PowerAnalysis, but allowing a perturbation only at experiment period and keeping pre-experiment df intact.\n    Using this class, the pre experiment df is also available when the class is instantiated.\n    \"\"\"\n\n    def _perturbate(\n        self, treatment_df: pd.DataFrame, average_effect: Optional[float]\n    ) -&gt; pd.DataFrame:\n        if not hasattr(self.analysis, \"_split_pre_experiment_df\"):\n            raise AttributeError(\n                \"The PowerAnalysisWithPreExperimentData is intended to work mainly for diff-in-diff or synthetic control-like estimators.\"\n                \"For other cases use the PowerAnalysis\"\n            )\n\n        df, pre_experiment_df = self.analysis._split_pre_experiment_df(treatment_df)\n\n        perturbed_df = self.perturbator.perturbate(df, average_effect=average_effect)\n\n        return pd.concat([perturbed_df, pre_experiment_df])\n</code></pre>"},{"location":"api/power_config.html","title":"<code>from cluster_experiments.power_config import *</code>","text":""},{"location":"api/power_config.html#cluster_experiments.power_config.PowerConfig","title":"<code>PowerConfig</code>  <code>dataclass</code>","text":"<p>Dataclass to create a power analysis from.</p> <p>Parameters:</p> Name Type Description Default <code>splitter</code> <code>str</code> <p>Splitter object to use</p> required <code>perturbator</code> <code>str</code> <p>Perturbator object to use, defaults to \"\" for normal power analysis</p> <code>''</code> <code>analysis</code> <code>str</code> <p>ExperimentAnalysis object to use</p> required <code>washover</code> <code>str</code> <p>Washover object to use, defaults to \"\"</p> <code>''</code> <code>cupac_model</code> <code>str</code> <p>CUPAC model to use</p> <code>''</code> <code>n_simulations</code> <code>int</code> <p>number of simulations to run</p> <code>100</code> <code>cluster_cols</code> <code>Optional[List[str]]</code> <p>list of columns to use as clusters</p> <code>None</code> <code>target_col</code> <code>str</code> <p>column to use as target</p> <code>'target'</code> <code>treatment_col</code> <code>str</code> <p>column to use as treatment</p> <code>'treatment'</code> <code>treatment</code> <code>str</code> <p>what value of treatment_col should be considered as treatment</p> <code>'B'</code> <code>control</code> <code>str</code> <p>what value of treatment_col should be considered as control</p> <code>'A'</code> <code>strata_cols</code> <code>Optional[List[str]]</code> <p>columns to stratify with</p> <code>None</code> <code>splitter_weights</code> <code>Optional[List[float]]</code> <p>weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments</p> <code>None</code> <code>switch_frequency</code> <code>Optional[str]</code> <p>how often to switch treatments</p> <code>None</code> <code>time_col</code> <code>Optional[str]</code> <p>column to use as time in switchback splitter</p> <code>None</code> <code>washover_time_delta</code> <code>Optional[Union[timedelta, int]]</code> <p>optional, int indicating the washover time in minutes or datetime.timedelta object</p> <code>None</code> <code>covariates</code> <code>Optional[List[str]]</code> <p>list of columns to use as covariates</p> <code>None</code> <code>average_effect</code> <code>Optional[float]</code> <p>average effect to use in the perturbator</p> <code>None</code> <code>scale</code> <code>Optional[float]</code> <p>scale to use in stochastic perturbators</p> <code>None</code> <code>range_min</code> <code>Optional[float]</code> <p>minimum value of the target range for relative beta perturbator, must be &gt;-1</p> <code>None</code> <code>range_max</code> <code>Optional[float]</code> <p>maximum value of the target range for relative beta perturbator</p> <code>None</code> <code>reduce_variance</code> <code>Optional[bool]</code> <p>whether to reduce variance in the BetaRelative perturbator</p> <code>None</code> <code>segment_cols</code> <code>Optional[List[str]]</code> <p>list of segmentation columns for segmented perturbator</p> <code>None</code> <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments to use</p> <code>None</code> <code>alpha</code> <code>float</code> <p>alpha value to use in the power analysis</p> <code>0.05</code> <code>agg_col</code> <code>str</code> <p>column to use for aggregation in the CUPAC model</p> <code>''</code> <code>smoothing_factor</code> <code>float</code> <p>smoothing value to use in the CUPAC model</p> <code>20</code> <code>features_cupac_model</code> <code>Optional[List[str]]</code> <p>list of features to use in the CUPAC model</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>seed to make the power analysis reproducible</p> <code>None</code> <p>Usage:</p> <pre><code>from cluster_experiments.power_config import PowerConfig\nfrom cluster_experiments.power_analysis import PowerAnalysis, NormalPowerAnalysis\n\np = PowerConfig(\n    analysis=\"gee\",\n    splitter=\"clustered_balance\",\n    perturbator=\"constant\",\n    cluster_cols=[\"city\"],\n    n_simulations=100,\n    alpha=0.05,\n)\npower_analysis = PowerAnalysis.from_config(p)\n\nnormal_power_analysis = NormalPowerAnalysis.from_config(p)\n</code></pre> Source code in <code>cluster_experiments/power_config.py</code> <pre><code>@dataclass(eq=True)\nclass PowerConfig:\n    \"\"\"\n    Dataclass to create a power analysis from.\n\n    Arguments:\n        splitter: Splitter object to use\n        perturbator: Perturbator object to use, defaults to \"\" for normal power analysis\n        analysis: ExperimentAnalysis object to use\n        washover: Washover object to use, defaults to \"\"\n        cupac_model: CUPAC model to use\n        n_simulations: number of simulations to run\n        cluster_cols: list of columns to use as clusters\n        target_col: column to use as target\n        treatment_col: column to use as treatment\n        treatment: what value of treatment_col should be considered as treatment\n        control: what value of treatment_col should be considered as control\n        strata_cols: columns to stratify with\n        splitter_weights: weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments\n        switch_frequency: how often to switch treatments\n        time_col: column to use as time in switchback splitter\n        washover_time_delta: optional, int indicating the washover time in minutes or datetime.timedelta object\n        covariates: list of columns to use as covariates\n        average_effect: average effect to use in the perturbator\n        scale: scale to use in stochastic perturbators\n        range_min: minimum value of the target range for relative beta perturbator, must be &gt;-1\n        range_max: maximum value of the target range for relative beta perturbator\n        reduce_variance: whether to reduce variance in the BetaRelative perturbator\n        segment_cols: list of segmentation columns for segmented perturbator\n        treatments: list of treatments to use\n        alpha: alpha value to use in the power analysis\n        agg_col: column to use for aggregation in the CUPAC model\n        smoothing_factor: smoothing value to use in the CUPAC model\n        features_cupac_model: list of features to use in the CUPAC model\n        seed: seed to make the power analysis reproducible\n\n    Usage:\n\n    ```python\n    from cluster_experiments.power_config import PowerConfig\n    from cluster_experiments.power_analysis import PowerAnalysis, NormalPowerAnalysis\n\n    p = PowerConfig(\n        analysis=\"gee\",\n        splitter=\"clustered_balance\",\n        perturbator=\"constant\",\n        cluster_cols=[\"city\"],\n        n_simulations=100,\n        alpha=0.05,\n    )\n    power_analysis = PowerAnalysis.from_config(p)\n\n    normal_power_analysis = NormalPowerAnalysis.from_config(p)\n    ```\n    \"\"\"\n\n    # mappings\n    splitter: str\n    analysis: str\n    perturbator: str = \"\"\n    washover: str = \"\"\n\n    # Needed\n    cluster_cols: Optional[List[str]] = None\n\n    # optional mappings\n    cupac_model: str = \"\"\n    scale_col: Optional[str] = None\n\n    # Shared\n    target_col: str = \"target\"\n    treatment_col: str = \"treatment\"\n    treatment: str = \"B\"\n\n    # Perturbator\n    average_effect: Optional[float] = None\n    scale: Optional[float] = None\n    range_min: Optional[float] = None\n    range_max: Optional[float] = None\n    reduce_variance: Optional[bool] = None\n    segment_cols: Optional[List[str]] = None\n\n    # Splitter\n    treatments: Optional[List[str]] = None\n    strata_cols: Optional[List[str]] = None\n    splitter_weights: Optional[List[float]] = None\n    switch_frequency: Optional[str] = None\n    # Switchback\n    time_col: Optional[str] = None\n    washover_time_delta: Optional[Union[datetime.timedelta, int]] = None\n\n    # Analysis\n    covariates: Optional[List[str]] = None\n    hypothesis: str = \"two-sided\"\n    cov_type: Optional[\n        Literal[\n            \"nonrobust\",\n            \"fixed scale\",\n            \"HC0\",\n            \"HC1\",\n            \"HC2\",\n            \"HC3\",\n            \"HAC\",\n            \"hac-panel\",\n            \"hac-groupsum\",\n            \"cluster\",\n        ]\n    ] = None\n    add_covariate_interaction: bool = False\n    relative_effect: bool = False\n\n    # Power analysis\n    n_simulations: int = 100\n    alpha: float = 0.05\n    control: str = \"A\"\n\n    # Cupac\n    agg_col: str = \"\"\n    smoothing_factor: float = 20\n    features_cupac_model: Optional[List[str]] = None\n\n    seed: Optional[int] = None\n\n    def __post_init__(self):\n        if \"switchback\" not in self.splitter:\n            if self._are_different(self.switch_frequency, None):\n                self._set_and_log(\"switch_frequency\", None, \"splitter\")\n            if self._are_different(self.washover_time_delta, None):\n                self._set_and_log(\"washover_time_delta\", None, \"splitter\")\n            if self._are_different(self.washover, \"\"):\n                self._set_and_log(\"washover\", \"\", \"splitter\")\n            # an exception is made when we have no perturbator (normal power analysis)\n            if self._are_different(self.time_col, None) and self.perturbator != \"\":\n                self._set_and_log(\"time_col\", None, \"splitter\")\n\n        if self.perturbator not in {\"normal\", \"beta_relative_positive\"}:\n            if self._are_different(self.scale, None):\n                self._set_and_log(\"scale\", None, \"perturbator\")\n\n        if self.perturbator not in {\"beta_relative\", \"segmented_beta_relative\"}:\n            if self._are_different(self.range_min, None):\n                self._set_and_log(\"range_min\", None, \"perturbator\")\n            if self._are_different(self.range_max, None):\n                self._set_and_log(\"range_max\", None, \"perturbator\")\n            if self._are_different(self.reduce_variance, None):\n                self._set_and_log(\"reduce_variance\", None, \"perturbator\")\n\n        if self.perturbator not in {\"segmented_beta_relative\"}:\n            if self._are_different(self.segment_cols, None):\n                self._set_and_log(\"segment_cols\", None, \"perturbator\")\n\n        if \"stratified\" not in self.splitter and \"paired_ttest\" not in self.analysis:\n            if self._are_different(self.strata_cols, None):\n                self._set_and_log(\"strata_cols\", None, \"splitter\")\n\n        if \"stratified\" in self.splitter or \"balanced\" in self.splitter:\n            if self._are_different(self.splitter_weights, None):\n                self._set_and_log(\"splitter_weights\", None, \"splitter\")\n\n        if self.cupac_model != \"mean_cupac_model\":\n            if self._are_different(self.agg_col, \"\"):\n                self._set_and_log(\"agg_col\", \"\", \"cupac_model\")\n            if self._are_different(self.smoothing_factor, 20):\n                self._set_and_log(\"smoothing_factor\", 20, \"cupac_model\")\n        # for now, features_cupac_model are not used\n        if self._are_different(self.features_cupac_model, None):\n            self._set_and_log(\"features_cupac_model\", None, \"cupac_model\")\n\n        if \"ttest\" in self.analysis:\n            if self._are_different(self.covariates, None):\n                self._set_and_log(\"covariates\", None, \"analysis\")\n\n        if \"segmented\" in self.perturbator:\n            self._raise_error_if_missing(\"segment_cols\", \"perturbator\")\n\n        if \"delta\" not in self.analysis:\n            if self.scale_col is not None:\n                self._raise_error_if_missing(\"scale_col\", \"analysis\")\n\n        if self.relative_effect:\n            if analysis_mapping[self.analysis] not in {\n                OLSAnalysis,\n                ClusteredOLSAnalysis,\n            }:\n                raise ValueError(\n                    \"relative_effect only works for OLSAnalysis, ClusteredOLSAnalysis\"\n                )\n\n    def _are_different(self, arg1, arg2) -&gt; bool:\n        return arg1 != arg2\n\n    def _set_and_log(self, attr, value, other_attr):\n        logging.warning(\n            f\"{attr} = {getattr(self, attr)} has no effect with {other_attr} = {getattr(self, other_attr)}. Overriding {attr} to {value}.\"\n        )\n        setattr(self, attr, value)\n\n    def _raise_error_if_missing(self, attr, other_attr):\n        if getattr(self, attr) is None:\n            raise MissingArgumentError(\n                f\"{attr} is required when using {other_attr} = {getattr(self, other_attr)}.\"\n            )\n\n    def _raise_error_if_present(self, attr, other_attr):\n        if getattr(self, attr) is None:\n            raise UnexpectedArgumentError(\n                f\"{attr} is not expected when using {other_attr} = {getattr(self, other_attr)}.\"\n            )\n</code></pre>"},{"location":"api/random_splitter.html","title":"<code>from cluster_experiments.random_splitter import *</code>","text":""},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedClusteredSplitter","title":"<code>BalancedClusteredSplitter</code>","text":"<p>               Bases: <code>ClusteredSplitter</code></p> <p>Like ClusteredSplitter, but ensures that treatments are balanced among clusters. That is, if we have 25 clusters and 2 treatments, 13 clusters should have treatment A and 12 clusters should have treatment B.</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class BalancedClusteredSplitter(ClusteredSplitter):\n    \"\"\"Like ClusteredSplitter, but ensures that treatments are balanced among clusters. That is, if we have\n    25 clusters and 2 treatments, 13 clusters should have treatment A and 12 clusters should have treatment B.\n    \"\"\"\n\n    def sample_treatment(\n        self,\n        cluster_df: pd.DataFrame,\n    ) -&gt; List[str]:\n        \"\"\"\n        Samples treatments for each cluster\n\n        Arguments:\n            cluster_df: dataframe to assign treatments to\n        \"\"\"\n        n_clusters = len(cluster_df)\n        n_treatments = len(self.treatments)\n        n_per_treatment = n_clusters // n_treatments\n        n_extra = n_clusters % n_treatments\n        treatments = []\n        for i in range(n_treatments):\n            treatments += [self.treatments[i]] * (n_per_treatment + (i &lt; n_extra))\n        random.shuffle(treatments)\n        return treatments\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedClusteredSplitter.sample_treatment","title":"<code>sample_treatment(cluster_df)</code>","text":"<p>Samples treatments for each cluster</p> <p>Parameters:</p> Name Type Description Default <code>cluster_df</code> <code>DataFrame</code> <p>dataframe to assign treatments to</p> required Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def sample_treatment(\n    self,\n    cluster_df: pd.DataFrame,\n) -&gt; List[str]:\n    \"\"\"\n    Samples treatments for each cluster\n\n    Arguments:\n        cluster_df: dataframe to assign treatments to\n    \"\"\"\n    n_clusters = len(cluster_df)\n    n_treatments = len(self.treatments)\n    n_per_treatment = n_clusters // n_treatments\n    n_extra = n_clusters % n_treatments\n    treatments = []\n    for i in range(n_treatments):\n        treatments += [self.treatments[i]] * (n_per_treatment + (i &lt; n_extra))\n    random.shuffle(treatments)\n    return treatments\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.BalancedSwitchbackSplitter","title":"<code>BalancedSwitchbackSplitter</code>","text":"<p>               Bases: <code>BalancedClusteredSplitter</code>, <code>SwitchbackSplitter</code></p> <p>Like SwitchbackSplitter, but ensures that treatments are balanced among clusters. That is, if we have 25 clusters and 2 treatments, 13 clusters should have treatment A and 12 clusters should have treatment B.</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class BalancedSwitchbackSplitter(BalancedClusteredSplitter, SwitchbackSplitter):\n    \"\"\"\n    Like SwitchbackSplitter, but ensures that treatments are balanced among clusters. That is, if we have\n    25 clusters and 2 treatments, 13 clusters should have treatment A and 12 clusters should have treatment B.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.ClusteredSplitter","title":"<code>ClusteredSplitter</code>","text":"<p>               Bases: <code>RandomSplitter</code></p> <p>Splits randomly using clusters</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>List[str]</code> <p>List of columns to use as clusters</p> required <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>splitter_weights</code> <code>Optional[List[float]]</code> <p>weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments</p> <code>None</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.random_splitter import ClusteredSplitter\nsplitter = ClusteredSplitter(cluster_cols=[\"city\"])\ndf = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\"]})\ndf = splitter.assign_treatment_df(df)\nprint(df)\n</code></pre></p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class ClusteredSplitter(RandomSplitter):\n    \"\"\"\n    Splits randomly using clusters\n\n    Arguments:\n        cluster_cols: List of columns to use as clusters\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n        splitter_weights: weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments\n\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.random_splitter import ClusteredSplitter\n    splitter = ClusteredSplitter(cluster_cols=[\"city\"])\n    df = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\"]})\n    df = splitter.assign_treatment_df(df)\n    print(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: List[str],\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n        splitter_weights: Optional[List[float]] = None,\n    ) -&gt; None:\n        self.treatments = treatments or [\"A\", \"B\"]\n        self.cluster_cols = cluster_cols\n        self.treatment_col = treatment_col\n        self.splitter_weights = splitter_weights\n\n    def assign_treatment_df(\n        self,\n        df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Takes a df, randomizes treatments and adds the treatment column to the dataframe\n\n        Arguments:\n            df: dataframe to assign treatments to\n        \"\"\"\n        df = df.copy()\n\n        # raise error if any nulls in cluster_cols\n        if df[self.cluster_cols].isnull().values.any():\n            raise ValueError(\n                f\"Null values found in cluster_cols: {self.cluster_cols}. \"\n                \"Please remove nulls before running the splitter.\"\n            )\n\n        clusters_df = df.loc[:, self.cluster_cols].drop_duplicates()\n        clusters_df[self.treatment_col] = self.sample_treatment(clusters_df)\n        df = df.merge(clusters_df, on=self.cluster_cols, how=\"left\")\n        return df\n\n    def sample_treatment(\n        self,\n        cluster_df: pd.DataFrame,\n    ) -&gt; List[str]:\n        \"\"\"\n        Samples treatments for each cluster\n\n        Arguments:\n            cluster_df: dataframe to assign treatments to\n        \"\"\"\n        return random.choices(\n            self.treatments, k=len(cluster_df), weights=self.splitter_weights\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.ClusteredSplitter.assign_treatment_df","title":"<code>assign_treatment_df(df)</code>","text":"<p>Takes a df, randomizes treatments and adds the treatment column to the dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to assign treatments to</p> required Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def assign_treatment_df(\n    self,\n    df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Takes a df, randomizes treatments and adds the treatment column to the dataframe\n\n    Arguments:\n        df: dataframe to assign treatments to\n    \"\"\"\n    df = df.copy()\n\n    # raise error if any nulls in cluster_cols\n    if df[self.cluster_cols].isnull().values.any():\n        raise ValueError(\n            f\"Null values found in cluster_cols: {self.cluster_cols}. \"\n            \"Please remove nulls before running the splitter.\"\n        )\n\n    clusters_df = df.loc[:, self.cluster_cols].drop_duplicates()\n    clusters_df[self.treatment_col] = self.sample_treatment(clusters_df)\n    df = df.merge(clusters_df, on=self.cluster_cols, how=\"left\")\n    return df\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.ClusteredSplitter.sample_treatment","title":"<code>sample_treatment(cluster_df)</code>","text":"<p>Samples treatments for each cluster</p> <p>Parameters:</p> Name Type Description Default <code>cluster_df</code> <code>DataFrame</code> <p>dataframe to assign treatments to</p> required Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def sample_treatment(\n    self,\n    cluster_df: pd.DataFrame,\n) -&gt; List[str]:\n    \"\"\"\n    Samples treatments for each cluster\n\n    Arguments:\n        cluster_df: dataframe to assign treatments to\n    \"\"\"\n    return random.choices(\n        self.treatments, k=len(cluster_df), weights=self.splitter_weights\n    )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.FixedSizeClusteredSplitter","title":"<code>FixedSizeClusteredSplitter</code>","text":"<p>               Bases: <code>ClusteredSplitter</code></p> <p>This class  represents a splitter that splits clusters into treatment groups with a predefined number of treatment clusters. This is particularly useful for synthetic control analysis, where we only want 1 cluster ( unit) to be in treatment group and the rest in control The cluster that receives treatment remains random.</p> <p>Attributes:</p> Name Type Description <code>cluster_cols</code> <code>List[str]</code> <p>List of columns to use as clusters.</p> <code>n_treatment_clusters</code> <code>int</code> <p>The predefined number of treatment clusters.</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class FixedSizeClusteredSplitter(ClusteredSplitter):\n    \"\"\"\n    This class  represents a splitter that splits clusters into treatment groups with a predefined number of\n    treatment clusters. This is particularly useful for synthetic control analysis, where we only want 1 cluster (\n    unit) to be in treatment group and the rest in control The cluster that receives treatment remains random.\n\n    Attributes:\n        cluster_cols (List[str]): List of columns to use as clusters.\n        n_treatment_clusters (int): The predefined number of treatment clusters.\n\n    \"\"\"\n\n    def __init__(self, cluster_cols: List[str], n_treatment_clusters: int):\n        super().__init__(cluster_cols=cluster_cols)\n        self.n_treatment_clusters = n_treatment_clusters\n\n    def sample_treatment(\n        self,\n        cluster_df: pd.DataFrame,\n    ) -&gt; List[str]:\n        \"\"\"\n        Samples treatments for each cluster.\n\n        Args:\n            cluster_df (pd.DataFrame): Dataframe to assign treatments to.\n\n        Returns:\n            List[str]: A list of treatments for each cluster.\n        \"\"\"\n        n_control_treatment = [\n            len(cluster_df) - self.n_treatment_clusters,\n            self.n_treatment_clusters,\n        ]\n\n        sample_treatment = [\n            treatment\n            for treatment, count in zip(self.treatments, n_control_treatment)\n            for _ in range(count)\n        ]\n        random.shuffle(sample_treatment)\n        return sample_treatment\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.FixedSizeClusteredSplitter.sample_treatment","title":"<code>sample_treatment(cluster_df)</code>","text":"<p>Samples treatments for each cluster.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_df</code> <code>DataFrame</code> <p>Dataframe to assign treatments to.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of treatments for each cluster.</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def sample_treatment(\n    self,\n    cluster_df: pd.DataFrame,\n) -&gt; List[str]:\n    \"\"\"\n    Samples treatments for each cluster.\n\n    Args:\n        cluster_df (pd.DataFrame): Dataframe to assign treatments to.\n\n    Returns:\n        List[str]: A list of treatments for each cluster.\n    \"\"\"\n    n_control_treatment = [\n        len(cluster_df) - self.n_treatment_clusters,\n        self.n_treatment_clusters,\n    ]\n\n    sample_treatment = [\n        treatment\n        for treatment, count in zip(self.treatments, n_control_treatment)\n        for _ in range(count)\n    ]\n    random.shuffle(sample_treatment)\n    return sample_treatment\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.NonClusteredSplitter","title":"<code>NonClusteredSplitter</code>","text":"<p>               Bases: <code>RandomSplitter</code></p> <p>Splits randomly without clusters</p> <p>Parameters:</p> Name Type Description Default <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.random_splitter import NonClusteredSplitter\nsplitter = NonClusteredSplitter(\n    treatments=[\"A\", \"B\"],\n)\ndf = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\"]})\ndf = splitter.assign_treatment_df(df)\nprint(df)\n</code></pre></p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class NonClusteredSplitter(RandomSplitter):\n    \"\"\"\n    Splits randomly without clusters\n\n    Arguments:\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.random_splitter import NonClusteredSplitter\n    splitter = NonClusteredSplitter(\n        treatments=[\"A\", \"B\"],\n    )\n    df = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\"]})\n    df = splitter.assign_treatment_df(df)\n    print(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n        splitter_weights: Optional[List[float]] = None,\n    ) -&gt; None:\n        self.treatments = treatments or [\"A\", \"B\"]\n        self.treatment_col = treatment_col\n        self.splitter_weights = splitter_weights\n\n    def assign_treatment_df(\n        self,\n        df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Takes a df, randomizes treatments and adds the treatment column to the dataframe\n\n        Arguments:\n            df: dataframe to assign treatments to\n        \"\"\"\n        df = df.copy()\n        df[self.treatment_col] = random.choices(\n            self.treatments, k=len(df), weights=self.splitter_weights\n        )\n        return df\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a NonClusteredSplitter from a PowerConfig\"\"\"\n        return cls(\n            treatments=config.treatments,\n            treatment_col=config.treatment_col,\n            splitter_weights=config.splitter_weights,\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.NonClusteredSplitter.assign_treatment_df","title":"<code>assign_treatment_df(df)</code>","text":"<p>Takes a df, randomizes treatments and adds the treatment column to the dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to assign treatments to</p> required Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def assign_treatment_df(\n    self,\n    df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Takes a df, randomizes treatments and adds the treatment column to the dataframe\n\n    Arguments:\n        df: dataframe to assign treatments to\n    \"\"\"\n    df = df.copy()\n    df[self.treatment_col] = random.choices(\n        self.treatments, k=len(df), weights=self.splitter_weights\n    )\n    return df\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.NonClusteredSplitter.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a NonClusteredSplitter from a PowerConfig</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a NonClusteredSplitter from a PowerConfig\"\"\"\n    return cls(\n        treatments=config.treatments,\n        treatment_col=config.treatment_col,\n        splitter_weights=config.splitter_weights,\n    )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter","title":"<code>RandomSplitter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class to split instances in a switchback or clustered way. It can be used to create a calendar/split of clusters or to run a power analysis.</p> <p>In order to create your own RandomSplitter, you should write your own assign_treatment_df method, that takes a dataframe as an input and returns the same dataframe with the treatment_col column.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>Optional[List[str]]</code> <p>List of columns to use as clusters</p> <code>None</code> <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>splitter_weights</code> <code>Optional[List[float]]</code> <p>weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments</p> <code>None</code> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class RandomSplitter(ABC):\n    \"\"\"\n    Abstract class to split instances in a switchback or clustered way. It can be used to create a calendar/split of clusters\n    or to run a power analysis.\n\n    In order to create your own RandomSplitter, you should write your own assign_treatment_df method, that takes a dataframe as an input and returns the same dataframe with the treatment_col column.\n\n    Arguments:\n        cluster_cols: List of columns to use as clusters\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n        splitter_weights: weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments\n\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: Optional[List[str]] = None,\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n        splitter_weights: Optional[List[float]] = None,\n    ) -&gt; None:\n        self.treatments = treatments or [\"A\", \"B\"]\n        self.cluster_cols = cluster_cols or []\n        self.treatment_col = treatment_col\n        self.splitter_weights = splitter_weights\n\n    @abstractmethod\n    def assign_treatment_df(\n        self,\n        df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Takes a df, randomizes treatments and adds the treatment column to the dataframe\n\n        Arguments:\n            df: dataframe to assign treatments to\n        \"\"\"\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a RandomSplitter from a PowerConfig\"\"\"\n        return cls(\n            treatments=config.treatments,\n            cluster_cols=config.cluster_cols,\n            treatment_col=config.treatment_col,\n            splitter_weights=config.splitter_weights,\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter.assign_treatment_df","title":"<code>assign_treatment_df(df)</code>  <code>abstractmethod</code>","text":"<p>Takes a df, randomizes treatments and adds the treatment column to the dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to assign treatments to</p> required Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>@abstractmethod\ndef assign_treatment_df(\n    self,\n    df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Takes a df, randomizes treatments and adds the treatment column to the dataframe\n\n    Arguments:\n        df: dataframe to assign treatments to\n    \"\"\"\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RandomSplitter.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a RandomSplitter from a PowerConfig</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a RandomSplitter from a PowerConfig\"\"\"\n    return cls(\n        treatments=config.treatments,\n        cluster_cols=config.cluster_cols,\n        treatment_col=config.treatment_col,\n        splitter_weights=config.splitter_weights,\n    )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RepeatedSampler","title":"<code>RepeatedSampler</code>","text":"<p>               Bases: <code>RandomSplitter</code></p> <p>Doesn't actually split the data, but repeatedly samples (i.e. duplicates) all rows for all treatments. This is useful for backtesting, where we assume to have access to all counterfactuals.</p> <p>Parameters:</p> Name Type Description Default <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.random_splitter import RepeatedSampler\nsplitter = RepeatedSampler(\n    treatments=[\"A\", \"B\"],\n)\ndf = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\"]})\ndf = splitter.assign_treatment_df(df)\nprint(df)\n</code></pre></p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class RepeatedSampler(RandomSplitter):\n    \"\"\"\n    Doesn't actually split the data, but repeatedly samples (i.e. duplicates) all rows for all treatments.\n    This is useful for backtesting, where we assume to have access to all counterfactuals.\n\n    Arguments:\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.random_splitter import RepeatedSampler\n    splitter = RepeatedSampler(\n        treatments=[\"A\", \"B\"],\n    )\n    df = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\"]})\n    df = splitter.assign_treatment_df(df)\n    print(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n    ) -&gt; None:\n        self.treatments = treatments or [\"A\", \"B\"]\n        self.treatment_col = treatment_col\n\n    def assign_treatment_df(\n        self,\n        df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        df = df.copy()\n\n        dfs = []\n        for treatment in self.treatments:\n            df_treat = df.copy().assign(**{self.treatment_col: treatment})\n            dfs.append(df_treat)\n\n        return pd.concat(dfs).reset_index(drop=True)\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a RepeatedSampler from a PowerConfig\"\"\"\n        return cls(\n            treatments=config.treatments,\n            treatment_col=config.treatment_col,\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.RepeatedSampler.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a RepeatedSampler from a PowerConfig</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a RepeatedSampler from a PowerConfig\"\"\"\n    return cls(\n        treatments=config.treatments,\n        treatment_col=config.treatment_col,\n    )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.StratifiedClusteredSplitter","title":"<code>StratifiedClusteredSplitter</code>","text":"<p>               Bases: <code>RandomSplitter</code></p> <p>Splits randomly with clusters, ensuring a balanced allocation of treatment groups across clusters and strata. To be used, for example, when having days as clusters and days of the week as stratus. This splitter will make sure that we won't have all Sundays in treatment and no Sundays in control.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_cols</code> <code>Optional[List[str]]</code> <p>List of columns to use as clusters</p> <code>None</code> <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>strata_cols</code> <code>Optional[List[str]]</code> <p>List of columns to use as strata</p> <code>None</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.random_splitter import StratifiedClusteredSplitter\nsplitter = StratifiedClusteredSplitter(cluster_cols=[\"city\"],strata_cols=[\"country\"])\ndf = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\",\"D\"], \"country\":[\"C1\",\"C2\",\"C2\",\"C1\"]})\ndf = splitter.assign_treatment_df(df)\nprint(df)\n</code></pre></p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class StratifiedClusteredSplitter(RandomSplitter):\n    \"\"\"\n    Splits randomly with clusters, ensuring a balanced allocation of treatment groups across clusters and strata.\n    To be used, for example, when having days as clusters and days of the week as stratus. This splitter will make sure\n    that we won't have all Sundays in treatment and no Sundays in control.\n\n    Arguments:\n        cluster_cols: List of columns to use as clusters\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n        strata_cols: List of columns to use as strata\n\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.random_splitter import StratifiedClusteredSplitter\n    splitter = StratifiedClusteredSplitter(cluster_cols=[\"city\"],strata_cols=[\"country\"])\n    df = pd.DataFrame({\"city\": [\"A\", \"B\", \"C\",\"D\"], \"country\":[\"C1\",\"C2\",\"C2\",\"C1\"]})\n    df = splitter.assign_treatment_df(df)\n    print(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        cluster_cols: Optional[List[str]] = None,\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n        strata_cols: Optional[List[str]] = None,\n    ) -&gt; None:\n        super().__init__(\n            cluster_cols=cluster_cols,\n            treatments=treatments,\n            treatment_col=treatment_col,\n        )\n        if not strata_cols or strata_cols == [\"\"]:\n            raise ValueError(\n                f\"Splitter {self.__class__.__name__} requires strata_cols,\"\n                f\" got {strata_cols = }\"\n            )\n        self.strata_cols = strata_cols\n\n    def assign_treatment_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df.copy()\n        df_unique_shuffled = (\n            df.loc[:, list(set(self.cluster_cols + self.strata_cols))]\n            .drop_duplicates()\n            .sample(frac=1)\n            .reset_index(drop=True)\n        )\n\n        # check that, for a given cluster, there is only 1 strata\n        for strata_col in self.strata_cols:\n            if (\n                df_unique_shuffled.groupby(self.cluster_cols)[strata_col]\n                .nunique()\n                .max()\n                &gt; 1\n            ):\n                raise ValueError(\n                    f\"There are multiple values in {strata_col} for the same cluster item \\n\"\n                    \"You cannot stratify on this column\",\n                )\n\n        # random shuffling\n        random_sorted_treatments = list(np.random.permutation(self.treatments))\n\n        df_unique_shuffled[self.treatment_col] = (\n            df_unique_shuffled.groupby(self.strata_cols, as_index=False)\n            .cumcount()\n            .mod(len(random_sorted_treatments))\n            .map(dict(enumerate(random_sorted_treatments)))\n        )\n\n        df = df.merge(\n            df_unique_shuffled, on=self.cluster_cols + self.strata_cols, how=\"left\"\n        )\n\n        return df\n\n    @classmethod\n    def from_config(cls, config):\n        \"\"\"Creates a StratifiedClusteredSplitter from a PowerConfig\"\"\"\n        return cls(\n            treatments=config.treatments,\n            cluster_cols=config.cluster_cols,\n            strata_cols=config.strata_cols,\n            treatment_col=config.treatment_col,\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.StratifiedClusteredSplitter.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a StratifiedClusteredSplitter from a PowerConfig</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>@classmethod\ndef from_config(cls, config):\n    \"\"\"Creates a StratifiedClusteredSplitter from a PowerConfig\"\"\"\n    return cls(\n        treatments=config.treatments,\n        cluster_cols=config.cluster_cols,\n        strata_cols=config.strata_cols,\n        treatment_col=config.treatment_col,\n    )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.StratifiedSwitchbackSplitter","title":"<code>StratifiedSwitchbackSplitter</code>","text":"<p>               Bases: <code>StratifiedClusteredSplitter</code>, <code>SwitchbackSplitter</code></p> <p>Splits randomly with clusters, ensuring a balanced allocation of treatment groups across clusters and strata. To be used, for example, when having days as clusters and days of the week as stratus. This splitter will make sure that we won't have all Sundays in treatment and no Sundays in control.</p> <p>It can be created using the time_col and switch_frequency arguments, just like the SwitchbackSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>time_col</code> <code>str</code> <p>Name of the column with the time variable.</p> <code>'date'</code> <code>switch_frequency</code> <code>str</code> <p>Frequency of the switchback. Must be a string (e.g. \"1D\")</p> <code>'1D'</code> <code>cluster_cols</code> <code>Optional[List[str]]</code> <p>List of columns to use as clusters</p> <code>None</code> <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>splitter_weights</code> <code>Optional[List[float]]</code> <p>List of weights for the treatments. If None, all treatments will have the same weight.</p> <code>None</code> <code>strata_cols</code> <code>Optional[List[str]]</code> <p>List of columns to use as strata</p> <code>None</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.random_splitter import StratifiedSwitchbackSplitter\nsplitter = StratifiedSwitchbackSplitter(time_col=\"date\",switch_frequency=\"1D\",strata_cols=[\"country\"], cluster_cols=[\"country\", \"date\"])\ndf = pd.DataFrame({\"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\",\"2020-01-04\"], \"country\":[\"C1\",\"C2\",\"C2\",\"C1\"]})\ndf = splitter.assign_treatment_df(df)\nprint(df)\n</code></pre></p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class StratifiedSwitchbackSplitter(StratifiedClusteredSplitter, SwitchbackSplitter):\n    \"\"\"\n    Splits randomly with clusters, ensuring a balanced allocation of treatment groups across clusters and strata.\n    To be used, for example, when having days as clusters and days of the week as stratus. This splitter will make sure\n    that we won't have all Sundays in treatment and no Sundays in control.\n\n    It can be created using the time_col and switch_frequency arguments, just like the SwitchbackSplitter.\n\n    Arguments:\n        time_col: Name of the column with the time variable.\n        switch_frequency: Frequency of the switchback. Must be a string (e.g. \"1D\")\n        cluster_cols: List of columns to use as clusters\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n        splitter_weights: List of weights for the treatments. If None, all treatments will have the same weight.\n        strata_cols: List of columns to use as strata\n\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.random_splitter import StratifiedSwitchbackSplitter\n    splitter = StratifiedSwitchbackSplitter(time_col=\"date\",switch_frequency=\"1D\",strata_cols=[\"country\"], cluster_cols=[\"country\", \"date\"])\n    df = pd.DataFrame({\"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\",\"2020-01-04\"], \"country\":[\"C1\",\"C2\",\"C2\",\"C1\"]})\n    df = splitter.assign_treatment_df(df)\n    print(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_col: str = \"date\",\n        switch_frequency: str = \"1D\",\n        cluster_cols: Optional[List[str]] = None,\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n        splitter_weights: Optional[List[float]] = None,\n        washover: Optional[Washover] = None,\n        strata_cols: Optional[List[str]] = None,\n    ) -&gt; None:\n        # Inherit init from SwitchbackSplitter\n        SwitchbackSplitter.__init__(\n            self,\n            time_col=time_col,\n            switch_frequency=switch_frequency,\n            cluster_cols=cluster_cols,\n            treatments=treatments,\n            treatment_col=treatment_col,\n            splitter_weights=splitter_weights,\n            washover=washover,\n        )\n        self.strata_cols = strata_cols or [\"strata\"]\n\n    def assign_treatment_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df.copy()\n        df = self._prepare_switchback_df(df)\n        df = StratifiedClusteredSplitter.assign_treatment_df(self, df)\n        return self.washover.washover(\n            df=df,\n            treatment_col=self.treatment_col,\n            truncated_time_col=self.time_col,\n            cluster_cols=self.cluster_cols,\n        )\n\n    @classmethod\n    def from_config(cls, config) -&gt; \"StratifiedSwitchbackSplitter\":\n        \"\"\"Creates a StratifiedSwitchbackSplitter from a PowerConfig\"\"\"\n        washover_cls = _get_mapping_key(washover_mapping, config.washover)\n        return cls(\n            treatments=config.treatments,\n            cluster_cols=config.cluster_cols,\n            strata_cols=config.strata_cols,\n            treatment_col=config.treatment_col,\n            time_col=config.time_col,\n            switch_frequency=config.switch_frequency,\n            splitter_weights=config.splitter_weights,\n            washover=washover_cls.from_config(config),\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.StratifiedSwitchbackSplitter.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a StratifiedSwitchbackSplitter from a PowerConfig</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>@classmethod\ndef from_config(cls, config) -&gt; \"StratifiedSwitchbackSplitter\":\n    \"\"\"Creates a StratifiedSwitchbackSplitter from a PowerConfig\"\"\"\n    washover_cls = _get_mapping_key(washover_mapping, config.washover)\n    return cls(\n        treatments=config.treatments,\n        cluster_cols=config.cluster_cols,\n        strata_cols=config.strata_cols,\n        treatment_col=config.treatment_col,\n        time_col=config.time_col,\n        switch_frequency=config.switch_frequency,\n        splitter_weights=config.splitter_weights,\n        washover=washover_cls.from_config(config),\n    )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.SwitchbackSplitter","title":"<code>SwitchbackSplitter</code>","text":"<p>               Bases: <code>ClusteredSplitter</code></p> <p>Splits randomly using clusters and time column</p> <p>It is a clustered splitter but one of the cluster columns is obtained by truncating the time column to the switch frequency.</p> <p>Parameters:</p> Name Type Description Default <code>time_col</code> <code>Optional[str]</code> <p>Name of the column with the time variable.</p> <code>None</code> <code>switch_frequency</code> <code>Optional[str]</code> <p>Frequency to switch treatments. Uses pandas frequency aliases (https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)</p> <code>None</code> <code>cluster_cols</code> <code>Optional[List[str]]</code> <p>List of columns to use as clusters</p> <code>None</code> <code>treatments</code> <code>Optional[List[str]]</code> <p>list of treatments</p> <code>None</code> <code>treatment_col</code> <code>str</code> <p>Name of the column with the treatment variable.</p> <code>'treatment'</code> <code>splitter_weights</code> <code>Optional[List[float]]</code> <p>weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments</p> <code>None</code> <p>Usage: <pre><code>import pandas as pd\nfrom cluster_experiments.random_splitter import SwitchbackSplitter\nsplitter = SwitchbackSplitter(time_col=\"date\", switch_frequency=\"1D\", cluster_cols=[\"date\"])\ndf = pd.DataFrame({\"date\": pd.date_range(\"2020-01-01\", \"2020-01-03\")})\ndf = splitter.assign_treatment_df(df)\nprint(df)\n</code></pre></p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>class SwitchbackSplitter(ClusteredSplitter):\n    \"\"\"\n    Splits randomly using clusters and time column\n\n    It is a clustered splitter but one of the cluster columns is obtained by truncating the time column to the switch frequency.\n\n    Arguments:\n        time_col: Name of the column with the time variable.\n        switch_frequency: Frequency to switch treatments. Uses pandas frequency aliases (https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)\n        cluster_cols: List of columns to use as clusters\n        treatments: list of treatments\n        treatment_col: Name of the column with the treatment variable.\n        splitter_weights: weights to use for the splitter, should have the same length as treatments, each weight should correspond to an element in treatments\n\n    Usage:\n    ```python\n    import pandas as pd\n    from cluster_experiments.random_splitter import SwitchbackSplitter\n    splitter = SwitchbackSplitter(time_col=\"date\", switch_frequency=\"1D\", cluster_cols=[\"date\"])\n    df = pd.DataFrame({\"date\": pd.date_range(\"2020-01-01\", \"2020-01-03\")})\n    df = splitter.assign_treatment_df(df)\n    print(df)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_col: Optional[str] = None,\n        switch_frequency: Optional[str] = None,\n        cluster_cols: Optional[List[str]] = None,\n        treatments: Optional[List[str]] = None,\n        treatment_col: str = \"treatment\",\n        splitter_weights: Optional[List[float]] = None,\n        washover: Optional[Washover] = None,\n    ) -&gt; None:\n        self.time_col = time_col or \"date\"\n        self.switch_frequency = switch_frequency or \"1D\"\n        self.cluster_cols = cluster_cols or []\n        self.treatments = treatments or [\"A\", \"B\"]\n        self.treatment_col = treatment_col\n        self.splitter_weights = splitter_weights\n        self.washover = washover or EmptyWashover()\n        self._check_clusters()\n\n    def _check_clusters(self):\n        \"\"\"Check if time_col is in cluster_cols\"\"\"\n        assert (\n            self.time_col in self.cluster_cols\n        ), \"in switchback splitters, time_col must be in cluster_cols\"\n\n    def _get_time_col_cluster(self, df: pd.DataFrame) -&gt; pd.Series:\n        df = df.copy()\n        df[self.time_col] = pd.to_datetime(df[self.time_col])\n        # Given the switch frequency, truncate the time column to the switch frequency\n        # Using pandas frequency aliases: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n        if \"W\" in self.switch_frequency or \"M\" in self.switch_frequency:\n            return df[self.time_col].dt.to_period(self.switch_frequency).dt.start_time\n        return df[self.time_col].dt.floor(self.switch_frequency)\n\n    def _prepare_switchback_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df = df.copy()\n        # Build time_col switchback column\n        # Overwriting column, this is the worst! If we use the column as a covariate, we're screwed. Needs improvement\n        df[_original_time_column(self.time_col)] = df[self.time_col]\n        df[self.time_col] = self._get_time_col_cluster(df)\n        return df\n\n    def assign_treatment_df(\n        self,\n        df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Creates the switchback column, adds it to cluster_cols and then calls ClusteredSplitter assign_treatment_df\n\n        Arguments:\n            df: dataframe to assign treatments to\n        \"\"\"\n        df = df.copy()\n        df = self._prepare_switchback_df(df)\n        df = super().assign_treatment_df(df)\n        df = self.washover.washover(\n            df,\n            truncated_time_col=self.time_col,\n            treatment_col=self.treatment_col,\n            cluster_cols=self.cluster_cols,\n        )\n        return df\n\n    @classmethod\n    def from_config(cls, config) -&gt; \"SwitchbackSplitter\":\n        washover_cls = _get_mapping_key(washover_mapping, config.washover)\n        return cls(\n            time_col=config.time_col,\n            switch_frequency=config.switch_frequency,\n            cluster_cols=config.cluster_cols,\n            treatments=config.treatments,\n            treatment_col=config.treatment_col,\n            splitter_weights=config.splitter_weights,\n            washover=washover_cls.from_config(config),\n        )\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.SwitchbackSplitter._check_clusters","title":"<code>_check_clusters()</code>","text":"<p>Check if time_col is in cluster_cols</p> Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def _check_clusters(self):\n    \"\"\"Check if time_col is in cluster_cols\"\"\"\n    assert (\n        self.time_col in self.cluster_cols\n    ), \"in switchback splitters, time_col must be in cluster_cols\"\n</code></pre>"},{"location":"api/random_splitter.html#cluster_experiments.random_splitter.SwitchbackSplitter.assign_treatment_df","title":"<code>assign_treatment_df(df)</code>","text":"<p>Creates the switchback column, adds it to cluster_cols and then calls ClusteredSplitter assign_treatment_df</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to assign treatments to</p> required Source code in <code>cluster_experiments/random_splitter.py</code> <pre><code>def assign_treatment_df(\n    self,\n    df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates the switchback column, adds it to cluster_cols and then calls ClusteredSplitter assign_treatment_df\n\n    Arguments:\n        df: dataframe to assign treatments to\n    \"\"\"\n    df = df.copy()\n    df = self._prepare_switchback_df(df)\n    df = super().assign_treatment_df(df)\n    df = self.washover.washover(\n        df,\n        truncated_time_col=self.time_col,\n        treatment_col=self.treatment_col,\n        cluster_cols=self.cluster_cols,\n    )\n    return df\n</code></pre>"},{"location":"api/variant.html","title":"<code>from cluster_experiments.inference.variant import *</code>","text":""},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant","title":"<code>Variant</code>  <code>dataclass</code>","text":"<p>A class used to represent a Variant with a name and a control flag.</p>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant--attributes","title":"Attributes","text":"<p>name : str     The name of the variant is_control : bool     A boolean indicating if the variant is a control variant</p> Source code in <code>cluster_experiments/inference/variant.py</code> <pre><code>@dataclass\nclass Variant:\n    \"\"\"\n    A class used to represent a Variant with a name and a control flag.\n\n    Attributes\n    ----------\n    name : str\n        The name of the variant\n    is_control : bool\n        A boolean indicating if the variant is a control variant\n    \"\"\"\n\n    name: str\n    is_control: bool\n\n    def __post_init__(self):\n        \"\"\"\n        Validates the inputs after initialization.\n        \"\"\"\n        self._validate_inputs()\n\n    def _validate_inputs(self):\n        \"\"\"\n        Validates the inputs for the Variant class.\n\n        Raises\n        ------\n        TypeError\n            If the name is not a string or if is_control is not a boolean.\n        \"\"\"\n        if not isinstance(self.name, str):\n            raise TypeError(\"Variant name must be a string\")\n        if not isinstance(self.is_control, bool):\n            raise TypeError(\"Variant is_control must be a boolean\")\n\n    @classmethod\n    def from_metrics_config(cls, config: dict) -&gt; \"Variant\":\n        \"\"\"\n        Creates a Variant object from a configuration dictionary.\n\n        Parameters\n        ----------\n        config : dict\n            A dictionary containing the configuration for the Variant\n\n        Returns\n        -------\n        Variant\n            A Variant object\n        \"\"\"\n        return cls(name=config[\"name\"], is_control=config[\"is_control\"])\n</code></pre>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validates the inputs after initialization.</p> Source code in <code>cluster_experiments/inference/variant.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Validates the inputs after initialization.\n    \"\"\"\n    self._validate_inputs()\n</code></pre>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant._validate_inputs","title":"<code>_validate_inputs()</code>","text":"<p>Validates the inputs for the Variant class.</p>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant._validate_inputs--raises","title":"Raises","text":"<p>TypeError     If the name is not a string or if is_control is not a boolean.</p> Source code in <code>cluster_experiments/inference/variant.py</code> <pre><code>def _validate_inputs(self):\n    \"\"\"\n    Validates the inputs for the Variant class.\n\n    Raises\n    ------\n    TypeError\n        If the name is not a string or if is_control is not a boolean.\n    \"\"\"\n    if not isinstance(self.name, str):\n        raise TypeError(\"Variant name must be a string\")\n    if not isinstance(self.is_control, bool):\n        raise TypeError(\"Variant is_control must be a boolean\")\n</code></pre>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant.from_metrics_config","title":"<code>from_metrics_config(config)</code>  <code>classmethod</code>","text":"<p>Creates a Variant object from a configuration dictionary.</p>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant.from_metrics_config--parameters","title":"Parameters","text":"<p>config : dict     A dictionary containing the configuration for the Variant</p>"},{"location":"api/variant.html#cluster_experiments.inference.variant.Variant.from_metrics_config--returns","title":"Returns","text":"<p>Variant     A Variant object</p> Source code in <code>cluster_experiments/inference/variant.py</code> <pre><code>@classmethod\ndef from_metrics_config(cls, config: dict) -&gt; \"Variant\":\n    \"\"\"\n    Creates a Variant object from a configuration dictionary.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing the configuration for the Variant\n\n    Returns\n    -------\n    Variant\n        A Variant object\n    \"\"\"\n    return cls(name=config[\"name\"], is_control=config[\"is_control\"])\n</code></pre>"},{"location":"api/washover.html","title":"<code>from cluster_experiments.washover import *</code>","text":""},{"location":"api/washover.html#cluster_experiments.washover.ConstantWashover","title":"<code>ConstantWashover</code>","text":"<p>               Bases: <code>Washover</code></p> <p>Constant washover - we drop all rows in the washover period when there is a switch where the treatment is different.</p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>class ConstantWashover(Washover):\n    \"\"\"Constant washover - we drop all rows in the washover period when\n    there is a switch where the treatment is different.\"\"\"\n\n    def __init__(self, washover_time_delta: datetime.timedelta):\n        self.washover_time_delta = washover_time_delta\n\n    def washover(\n        self,\n        df: pd.DataFrame,\n        truncated_time_col: str,\n        treatment_col: str,\n        cluster_cols: List[str],\n        original_time_col: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Constant washover - we drop all rows in the washover period when\n        there is a switch where the treatment is different.\n\n        Args:\n            df (pd.DataFrame): Input dataframe.\n            truncated_time_col (str): Name of the truncated time column.\n            treatment_col (str): Name of the treatment column.\n            cluster_cols (List[str]): List of clusters of experiment.\n            original_time_col (Optional[str], optional): Name of the original time column.\n\n        Returns:\n            pd.DataFrame: Same dataframe as input without the rows in the washover period.\n\n        Usage:\n        ```python\n        import numpy as np\n        import pandas as pd\n        from datetime import datetime, timedelta\n\n        from cluster_experiments import ConstantWashover\n\n        np.random.seed(42)\n\n        num_rows = 10\n\n        def random_timestamp(start_time, end_time):\n            time_delta = end_time - start_time\n            random_seconds = np.random.randint(0, time_delta.total_seconds())\n            return start_time + timedelta(seconds=random_seconds)\n\n        def generate_data(start_time, end_time, treatment):\n            data = {\n                'order_id': np.random.randint(10**9, 10**10, size=num_rows),\n                'city_code': 'VAL',\n                'activation_time_local': [random_timestamp(start_time, end_time) for _ in range(num_rows)],\n                'bin_start_time_local': start_time,\n                'treatment': treatment\n            }\n            return pd.DataFrame(data)\n\n        start_times = [datetime(2024, 1, 22, 9, 0), datetime(2024, 1, 22, 11, 0),\n                    datetime(2024, 1, 22, 13, 0), datetime(2024, 1, 22, 15, 0)]\n\n        treatments = ['control', 'variation', 'variation', 'control']\n\n        dataframes = [generate_data(start, start + timedelta(hours=2), treatment) for start, treatment in zip(start_times, treatments)]\n\n        df = pd.concat(dataframes).sort_values(by='activation_time_local').reset_index(drop=True)\n\n        ## Define washover with 30 min duration\n        washover = ConstantWashover(washover_time_delta=timedelta(minutes=30))\n\n        ## Apply washover to the dataframe, the orders with activation time within the first 30 minutes after every change in the treatment column, clustering by city and 2h time bin, will be dropped\n        df_analysis_washover = washover.washover(\n            df=df,\n            truncated_time_col='bin_start_time_local',\n            treatment_col='treatment',\n            cluster_cols=['city_code','bin_start_time_local'],\n            original_time_col='activation_time_local',\n        )\n        ```\n        \"\"\"\n        # Set original time column\n        original_time_col = (\n            original_time_col\n            if original_time_col\n            else _original_time_column(truncated_time_col)\n        )\n\n        # Validate columns\n        self._validate_columns(df, truncated_time_col, cluster_cols, original_time_col)\n\n        # Cluster columns that do not involve time\n        non_time_cols = list(set(cluster_cols) - set([truncated_time_col]))\n        # For each cluster, we need to check if treatment has changed wrt last time\n        df_agg = df.sort_values([original_time_col]).copy()\n        df_agg = df_agg.drop_duplicates(subset=cluster_cols + [treatment_col])\n\n        if non_time_cols:\n            df_agg[\"__changed\"] = (\n                df_agg.groupby(non_time_cols)[treatment_col].shift(1)\n                != df_agg[treatment_col]\n            )\n        else:\n            df_agg[\"__changed\"] = (\n                df_agg[treatment_col].shift(1) != df_agg[treatment_col]\n            )\n        df_agg = df_agg.loc[:, cluster_cols + [\"__changed\"]]\n        return (\n            df.merge(df_agg, on=cluster_cols, how=\"inner\")\n            .assign(\n                __time_since_switch=lambda x: x[original_time_col].astype(\n                    \"datetime64[ns]\"\n                )\n                - x[truncated_time_col].astype(\"datetime64[ns]\"),\n                __after_washover=lambda x: x[\"__time_since_switch\"]\n                &gt; self.washover_time_delta,\n            )\n            # add not changed in query\n            .query(\"__after_washover or not __changed\")\n            .drop(columns=[\"__time_since_switch\", \"__after_washover\", \"__changed\"])\n        )\n\n    @classmethod\n    def from_config(cls, config) -&gt; \"Washover\":\n        if not config.washover_time_delta:\n            raise ValueError(\n                f\"Washover time delta must be specified for ConstantWashover, while it is {config.washover_time_delta = }\"\n            )\n\n        washover_time_delta = config.washover_time_delta\n        if isinstance(washover_time_delta, int):\n            washover_time_delta = datetime.timedelta(minutes=config.washover_time_delta)\n        return cls(washover_time_delta=washover_time_delta)\n</code></pre>"},{"location":"api/washover.html#cluster_experiments.washover.ConstantWashover.washover","title":"<code>washover(df, truncated_time_col, treatment_col, cluster_cols, original_time_col=None)</code>","text":"<p>Constant washover - we drop all rows in the washover period when there is a switch where the treatment is different.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>truncated_time_col</code> <code>str</code> <p>Name of the truncated time column.</p> required <code>treatment_col</code> <code>str</code> <p>Name of the treatment column.</p> required <code>cluster_cols</code> <code>List[str]</code> <p>List of clusters of experiment.</p> required <code>original_time_col</code> <code>Optional[str]</code> <p>Name of the original time column.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Same dataframe as input without the rows in the washover period.</p> <p>Usage: <pre><code>import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nfrom cluster_experiments import ConstantWashover\n\nnp.random.seed(42)\n\nnum_rows = 10\n\ndef random_timestamp(start_time, end_time):\n    time_delta = end_time - start_time\n    random_seconds = np.random.randint(0, time_delta.total_seconds())\n    return start_time + timedelta(seconds=random_seconds)\n\ndef generate_data(start_time, end_time, treatment):\n    data = {\n        'order_id': np.random.randint(10**9, 10**10, size=num_rows),\n        'city_code': 'VAL',\n        'activation_time_local': [random_timestamp(start_time, end_time) for _ in range(num_rows)],\n        'bin_start_time_local': start_time,\n        'treatment': treatment\n    }\n    return pd.DataFrame(data)\n\nstart_times = [datetime(2024, 1, 22, 9, 0), datetime(2024, 1, 22, 11, 0),\n            datetime(2024, 1, 22, 13, 0), datetime(2024, 1, 22, 15, 0)]\n\ntreatments = ['control', 'variation', 'variation', 'control']\n\ndataframes = [generate_data(start, start + timedelta(hours=2), treatment) for start, treatment in zip(start_times, treatments)]\n\ndf = pd.concat(dataframes).sort_values(by='activation_time_local').reset_index(drop=True)\n\n## Define washover with 30 min duration\nwashover = ConstantWashover(washover_time_delta=timedelta(minutes=30))\n\n## Apply washover to the dataframe, the orders with activation time within the first 30 minutes after every change in the treatment column, clustering by city and 2h time bin, will be dropped\ndf_analysis_washover = washover.washover(\n    df=df,\n    truncated_time_col='bin_start_time_local',\n    treatment_col='treatment',\n    cluster_cols=['city_code','bin_start_time_local'],\n    original_time_col='activation_time_local',\n)\n</code></pre></p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>def washover(\n    self,\n    df: pd.DataFrame,\n    truncated_time_col: str,\n    treatment_col: str,\n    cluster_cols: List[str],\n    original_time_col: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Constant washover - we drop all rows in the washover period when\n    there is a switch where the treatment is different.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        truncated_time_col (str): Name of the truncated time column.\n        treatment_col (str): Name of the treatment column.\n        cluster_cols (List[str]): List of clusters of experiment.\n        original_time_col (Optional[str], optional): Name of the original time column.\n\n    Returns:\n        pd.DataFrame: Same dataframe as input without the rows in the washover period.\n\n    Usage:\n    ```python\n    import numpy as np\n    import pandas as pd\n    from datetime import datetime, timedelta\n\n    from cluster_experiments import ConstantWashover\n\n    np.random.seed(42)\n\n    num_rows = 10\n\n    def random_timestamp(start_time, end_time):\n        time_delta = end_time - start_time\n        random_seconds = np.random.randint(0, time_delta.total_seconds())\n        return start_time + timedelta(seconds=random_seconds)\n\n    def generate_data(start_time, end_time, treatment):\n        data = {\n            'order_id': np.random.randint(10**9, 10**10, size=num_rows),\n            'city_code': 'VAL',\n            'activation_time_local': [random_timestamp(start_time, end_time) for _ in range(num_rows)],\n            'bin_start_time_local': start_time,\n            'treatment': treatment\n        }\n        return pd.DataFrame(data)\n\n    start_times = [datetime(2024, 1, 22, 9, 0), datetime(2024, 1, 22, 11, 0),\n                datetime(2024, 1, 22, 13, 0), datetime(2024, 1, 22, 15, 0)]\n\n    treatments = ['control', 'variation', 'variation', 'control']\n\n    dataframes = [generate_data(start, start + timedelta(hours=2), treatment) for start, treatment in zip(start_times, treatments)]\n\n    df = pd.concat(dataframes).sort_values(by='activation_time_local').reset_index(drop=True)\n\n    ## Define washover with 30 min duration\n    washover = ConstantWashover(washover_time_delta=timedelta(minutes=30))\n\n    ## Apply washover to the dataframe, the orders with activation time within the first 30 minutes after every change in the treatment column, clustering by city and 2h time bin, will be dropped\n    df_analysis_washover = washover.washover(\n        df=df,\n        truncated_time_col='bin_start_time_local',\n        treatment_col='treatment',\n        cluster_cols=['city_code','bin_start_time_local'],\n        original_time_col='activation_time_local',\n    )\n    ```\n    \"\"\"\n    # Set original time column\n    original_time_col = (\n        original_time_col\n        if original_time_col\n        else _original_time_column(truncated_time_col)\n    )\n\n    # Validate columns\n    self._validate_columns(df, truncated_time_col, cluster_cols, original_time_col)\n\n    # Cluster columns that do not involve time\n    non_time_cols = list(set(cluster_cols) - set([truncated_time_col]))\n    # For each cluster, we need to check if treatment has changed wrt last time\n    df_agg = df.sort_values([original_time_col]).copy()\n    df_agg = df_agg.drop_duplicates(subset=cluster_cols + [treatment_col])\n\n    if non_time_cols:\n        df_agg[\"__changed\"] = (\n            df_agg.groupby(non_time_cols)[treatment_col].shift(1)\n            != df_agg[treatment_col]\n        )\n    else:\n        df_agg[\"__changed\"] = (\n            df_agg[treatment_col].shift(1) != df_agg[treatment_col]\n        )\n    df_agg = df_agg.loc[:, cluster_cols + [\"__changed\"]]\n    return (\n        df.merge(df_agg, on=cluster_cols, how=\"inner\")\n        .assign(\n            __time_since_switch=lambda x: x[original_time_col].astype(\n                \"datetime64[ns]\"\n            )\n            - x[truncated_time_col].astype(\"datetime64[ns]\"),\n            __after_washover=lambda x: x[\"__time_since_switch\"]\n            &gt; self.washover_time_delta,\n        )\n        # add not changed in query\n        .query(\"__after_washover or not __changed\")\n        .drop(columns=[\"__time_since_switch\", \"__after_washover\", \"__changed\"])\n    )\n</code></pre>"},{"location":"api/washover.html#cluster_experiments.washover.EmptyWashover","title":"<code>EmptyWashover</code>","text":"<p>               Bases: <code>Washover</code></p> <p>No washover - assumes no spill-over effects from one treatment to another.</p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>class EmptyWashover(Washover):\n    \"\"\"No washover - assumes no spill-over effects from one treatment to another.\"\"\"\n\n    def washover(\n        self,\n        df: pd.DataFrame,\n        truncated_time_col: str,\n        treatment_col: str,\n        cluster_cols: List[str],\n        original_time_col: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"No washover - returns the same dataframe as input.\n\n        Args:\n            df (pd.DataFrame): Input dataframe.\n            truncated_time_col (str): Name of the truncated time column.\n            treatment_col (str): Name of the treatment column.\n            cluster_cols (List[str]): List of clusters of experiment.\n            original_time_col (Optional[str], optional): Name of the original time column.\n\n        Returns:\n            pd.DataFrame: Same dataframe as input.\n\n        Usage:\n        ```python\n        from cluster_experiments import SwitchbackSplitter\n        from cluster_experiments import EmptyWashover\n\n        washover = EmptyWashover()\n\n        n = 10\n        df = pd.DataFrame(\n            {\n                # Random time each minute in 2022-01-01, length 10\n                \"time\": pd.date_range(\"2022-01-01\", \"2022-01-02\", freq=\"1min\")[\n                    np.random.randint(24 * 60, size=n)\n                ],\n                \"city\": random.choices([\"TGN\", \"NYC\", \"LON\", \"REU\"], k=n),\n            }\n        )\n\n\n        splitter = SwitchbackSplitter(\n            washover=washover,\n            time_col=\"time\",\n            cluster_cols=[\"city\", \"time\"],\n            treatment_col=\"treatment\",\n            switch_frequency=\"30T\",\n        )\n\n        out_df = splitter.assign_treatment_df(df=washover_split_df)\n        ```\n        \"\"\"\n        return df\n</code></pre>"},{"location":"api/washover.html#cluster_experiments.washover.EmptyWashover.washover","title":"<code>washover(df, truncated_time_col, treatment_col, cluster_cols, original_time_col=None)</code>","text":"<p>No washover - returns the same dataframe as input.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>truncated_time_col</code> <code>str</code> <p>Name of the truncated time column.</p> required <code>treatment_col</code> <code>str</code> <p>Name of the treatment column.</p> required <code>cluster_cols</code> <code>List[str]</code> <p>List of clusters of experiment.</p> required <code>original_time_col</code> <code>Optional[str]</code> <p>Name of the original time column.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Same dataframe as input.</p> <p>Usage: <pre><code>from cluster_experiments import SwitchbackSplitter\nfrom cluster_experiments import EmptyWashover\n\nwashover = EmptyWashover()\n\nn = 10\ndf = pd.DataFrame(\n    {\n        # Random time each minute in 2022-01-01, length 10\n        \"time\": pd.date_range(\"2022-01-01\", \"2022-01-02\", freq=\"1min\")[\n            np.random.randint(24 * 60, size=n)\n        ],\n        \"city\": random.choices([\"TGN\", \"NYC\", \"LON\", \"REU\"], k=n),\n    }\n)\n\n\nsplitter = SwitchbackSplitter(\n    washover=washover,\n    time_col=\"time\",\n    cluster_cols=[\"city\", \"time\"],\n    treatment_col=\"treatment\",\n    switch_frequency=\"30T\",\n)\n\nout_df = splitter.assign_treatment_df(df=washover_split_df)\n</code></pre></p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>def washover(\n    self,\n    df: pd.DataFrame,\n    truncated_time_col: str,\n    treatment_col: str,\n    cluster_cols: List[str],\n    original_time_col: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"No washover - returns the same dataframe as input.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        truncated_time_col (str): Name of the truncated time column.\n        treatment_col (str): Name of the treatment column.\n        cluster_cols (List[str]): List of clusters of experiment.\n        original_time_col (Optional[str], optional): Name of the original time column.\n\n    Returns:\n        pd.DataFrame: Same dataframe as input.\n\n    Usage:\n    ```python\n    from cluster_experiments import SwitchbackSplitter\n    from cluster_experiments import EmptyWashover\n\n    washover = EmptyWashover()\n\n    n = 10\n    df = pd.DataFrame(\n        {\n            # Random time each minute in 2022-01-01, length 10\n            \"time\": pd.date_range(\"2022-01-01\", \"2022-01-02\", freq=\"1min\")[\n                np.random.randint(24 * 60, size=n)\n            ],\n            \"city\": random.choices([\"TGN\", \"NYC\", \"LON\", \"REU\"], k=n),\n        }\n    )\n\n\n    splitter = SwitchbackSplitter(\n        washover=washover,\n        time_col=\"time\",\n        cluster_cols=[\"city\", \"time\"],\n        treatment_col=\"treatment\",\n        switch_frequency=\"30T\",\n    )\n\n    out_df = splitter.assign_treatment_df(df=washover_split_df)\n    ```\n    \"\"\"\n    return df\n</code></pre>"},{"location":"api/washover.html#cluster_experiments.washover.Washover","title":"<code>Washover</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class to model washovers in the switchback splitter.</p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>class Washover(ABC):\n    \"\"\"Abstract class to model washovers in the switchback splitter.\"\"\"\n\n    def _validate_columns(\n        self,\n        df: pd.DataFrame,\n        truncated_time_col: str,\n        cluster_cols: List[str],\n        original_time_col: str,\n    ):\n        \"\"\"Validate that all the columns required for the washover are present in the dataframe.\n\n        Args:\n            df (pd.DataFrame): Input dataframe.\n            truncated_time_col (str): Name of the truncated time column.\n            cluster_cols (List[str]): List of clusters of experiment.\n            original_time_col (str): Name of the original time column.\n\n        Returns:\n            None: This method does not return any data; it only performs validation.\n\n        \"\"\"\n        if original_time_col not in df.columns:\n            raise ValueError(\n                f\"{original_time_col = } is not in the dataframe columns and/or not specified as an input.\"\n            )\n        if truncated_time_col not in cluster_cols:\n            raise ValueError(f\"{truncated_time_col = } is not in the cluster columns.\")\n        for col in cluster_cols:\n            if col not in df.columns:\n                raise ValueError(f\"{col = } cluster is not in the dataframe columns.\")\n\n    @abstractmethod\n    def washover(\n        self,\n        df: pd.DataFrame,\n        truncated_time_col: str,\n        treatment_col: str,\n        cluster_cols: List[str],\n        original_time_col: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Abstract method to add washvover to the dataframe.\"\"\"\n\n    @classmethod\n    def from_config(cls, config) -&gt; \"Washover\":\n        return cls()\n</code></pre>"},{"location":"api/washover.html#cluster_experiments.washover.Washover._validate_columns","title":"<code>_validate_columns(df, truncated_time_col, cluster_cols, original_time_col)</code>","text":"<p>Validate that all the columns required for the washover are present in the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>truncated_time_col</code> <code>str</code> <p>Name of the truncated time column.</p> required <code>cluster_cols</code> <code>List[str]</code> <p>List of clusters of experiment.</p> required <code>original_time_col</code> <code>str</code> <p>Name of the original time column.</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>This method does not return any data; it only performs validation.</p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>def _validate_columns(\n    self,\n    df: pd.DataFrame,\n    truncated_time_col: str,\n    cluster_cols: List[str],\n    original_time_col: str,\n):\n    \"\"\"Validate that all the columns required for the washover are present in the dataframe.\n\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        truncated_time_col (str): Name of the truncated time column.\n        cluster_cols (List[str]): List of clusters of experiment.\n        original_time_col (str): Name of the original time column.\n\n    Returns:\n        None: This method does not return any data; it only performs validation.\n\n    \"\"\"\n    if original_time_col not in df.columns:\n        raise ValueError(\n            f\"{original_time_col = } is not in the dataframe columns and/or not specified as an input.\"\n        )\n    if truncated_time_col not in cluster_cols:\n        raise ValueError(f\"{truncated_time_col = } is not in the cluster columns.\")\n    for col in cluster_cols:\n        if col not in df.columns:\n            raise ValueError(f\"{col = } cluster is not in the dataframe columns.\")\n</code></pre>"},{"location":"api/washover.html#cluster_experiments.washover.Washover.washover","title":"<code>washover(df, truncated_time_col, treatment_col, cluster_cols, original_time_col=None)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to add washvover to the dataframe.</p> Source code in <code>cluster_experiments/washover.py</code> <pre><code>@abstractmethod\ndef washover(\n    self,\n    df: pd.DataFrame,\n    truncated_time_col: str,\n    treatment_col: str,\n    cluster_cols: List[str],\n    original_time_col: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Abstract method to add washvover to the dataframe.\"\"\"\n</code></pre>"},{"location":"examples/cluster_randomization.html","title":"Cluster Randomization Example","text":"In\u00a0[10]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom cluster_experiments import AnalysisPlan\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n</pre> import pandas as pd import numpy as np from cluster_experiments import AnalysisPlan  # Set random seed for reproducibility np.random.seed(42)  In\u00a0[11]: Copied! <pre># Define parameters\nn_stores = 50  # Number of stores (clusters)\ntransactions_per_store = 100  # Average transactions per store\n\n# Step 1: Randomly assign stores to treatment\nstores = pd.DataFrame({\n    'store_id': range(n_stores),\n    'variant': np.random.choice(['control', 'treatment'], n_stores),\n})\n\n# Step 2: Generate transaction-level data\ntransactions = []\nfor _, store in stores.iterrows():\n    n_transactions = np.random.poisson(transactions_per_store)\n    \n    # Base purchase amount\n    base_amount = 50\n    \n    # Treatment effect: +$5 average purchase\n    treatment_effect = 5 if store['variant'] == 'treatment' else 0\n    \n    # Store-level random effect (intra-cluster correlation)\n    store_effect = np.random.normal(0, 10)\n    \n    # Generate transactions\n    store_transactions = pd.DataFrame({\n        'store_id': store['store_id'],\n        'variant': store['variant'],\n        'purchase_amount': np.random.normal(\n            base_amount + treatment_effect + store_effect, \n            20, \n            n_transactions\n        ).clip(min=0)  # No negative purchases\n    })\n    \n    transactions.append(store_transactions)\n\ndata = pd.concat(transactions, ignore_index=True)\n\nprint(f\"Total transactions: {len(data):,}\")\nprint(f\"Stores in control: {(stores['variant'] == 'control').sum()}\")\nprint(f\"Stores in treatment: {(stores['variant'] == 'treatment').sum()}\")\nprint(f\"\\nFirst few rows:\")\ndata.head()\n</pre> # Define parameters n_stores = 50  # Number of stores (clusters) transactions_per_store = 100  # Average transactions per store  # Step 1: Randomly assign stores to treatment stores = pd.DataFrame({     'store_id': range(n_stores),     'variant': np.random.choice(['control', 'treatment'], n_stores), })  # Step 2: Generate transaction-level data transactions = [] for _, store in stores.iterrows():     n_transactions = np.random.poisson(transactions_per_store)          # Base purchase amount     base_amount = 50          # Treatment effect: +$5 average purchase     treatment_effect = 5 if store['variant'] == 'treatment' else 0          # Store-level random effect (intra-cluster correlation)     store_effect = np.random.normal(0, 10)          # Generate transactions     store_transactions = pd.DataFrame({         'store_id': store['store_id'],         'variant': store['variant'],         'purchase_amount': np.random.normal(             base_amount + treatment_effect + store_effect,              20,              n_transactions         ).clip(min=0)  # No negative purchases     })          transactions.append(store_transactions)  data = pd.concat(transactions, ignore_index=True)  print(f\"Total transactions: {len(data):,}\") print(f\"Stores in control: {(stores['variant'] == 'control').sum()}\") print(f\"Stores in treatment: {(stores['variant'] == 'treatment').sum()}\") print(f\"\\nFirst few rows:\") data.head()  <pre>Total transactions: 5,055\nStores in control: 23\nStores in treatment: 27\n\nFirst few rows:\n</pre> Out[11]: store_id variant purchase_amount 0 0 control 83.479541 1 0 control 78.039264 2 0 control 65.286167 3 0 control 63.589803 4 0 control 94.543677 In\u00a0[12]: Copied! <pre># Naive analysis without clustering\nnaive_plan = AnalysisPlan.from_metrics_dict({\n    'metrics': [\n        {\n            'alias': 'purchase_amount',\n            'name': 'purchase_amount',\n            'metric_type': 'simple'\n        },\n    ],\n    'variants': [\n        {'name': 'control', 'is_control': True},\n        {'name': 'treatment', 'is_control': False},\n    ],\n    'variant_col': 'variant',\n    'analysis_type': 'ols',  # Standard OLS (WRONG for clustered data!)\n})\n\nnaive_results = naive_plan.analyze(data).to_dataframe()\nprint(\"=== Naive Analysis (Ignoring Clusters) ===\")\nprint(f\"Treatment Effect: ${naive_results.iloc[0]['ate']:.2f}\")\nprint(f\"Standard Error: ${naive_results.iloc[0]['std_error']:.2f}\")\nprint(f\"P-value: {naive_results.iloc[0]['p_value']:.4f}\")\nprint(f\"95% CI: [${naive_results.iloc[0]['ate_ci_lower']:.2f}, ${naive_results.iloc[0]['ate_ci_upper']:.2f}]\")\n</pre> # Naive analysis without clustering naive_plan = AnalysisPlan.from_metrics_dict({     'metrics': [         {             'alias': 'purchase_amount',             'name': 'purchase_amount',             'metric_type': 'simple'         },     ],     'variants': [         {'name': 'control', 'is_control': True},         {'name': 'treatment', 'is_control': False},     ],     'variant_col': 'variant',     'analysis_type': 'ols',  # Standard OLS (WRONG for clustered data!) })  naive_results = naive_plan.analyze(data).to_dataframe() print(\"=== Naive Analysis (Ignoring Clusters) ===\") print(f\"Treatment Effect: ${naive_results.iloc[0]['ate']:.2f}\") print(f\"Standard Error: ${naive_results.iloc[0]['std_error']:.2f}\") print(f\"P-value: {naive_results.iloc[0]['p_value']:.4f}\") print(f\"95% CI: [${naive_results.iloc[0]['ate_ci_lower']:.2f}, ${naive_results.iloc[0]['ate_ci_upper']:.2f}]\")  <pre>=== Naive Analysis (Ignoring Clusters) ===\nTreatment Effect: $4.26\nStandard Error: $0.63\nP-value: 0.0000\n95% CI: [$3.03, $5.48]\n</pre> In\u00a0[13]: Copied! <pre># Correct analysis with clustered standard errors\nclustered_plan = AnalysisPlan.from_metrics_dict({\n    'metrics': [\n        {\n            'alias': 'purchase_amount',\n            'name': 'purchase_amount',\n            'metric_type': 'simple'\n        },\n    ],\n    'variants': [\n        {'name': 'control', 'is_control': True},\n        {'name': 'treatment', 'is_control': False},\n    ],\n    'variant_col': 'variant',\n    'analysis_type': 'clustered_ols',  # Clustered OLS (CORRECT!)\n    'analysis_config': {\n        'cluster_cols': ['store_id']  # Specify the clustering variable\n    }\n})\n\nclustered_results = clustered_plan.analyze(data).to_dataframe()\nprint(\"=== Correct Analysis (With Clustering) ===\")\nprint(f\"Treatment Effect: ${clustered_results.iloc[0]['ate']:.2f}\")\nprint(f\"Standard Error: ${clustered_results.iloc[0]['std_error']:.2f}\")\nprint(f\"P-value: {clustered_results.iloc[0]['p_value']:.4f}\")\nprint(f\"95% CI: [${clustered_results.iloc[0]['ate_ci_lower']:.2f}, ${clustered_results.iloc[0]['ate_ci_upper']:.2f}]\")\n</pre> # Correct analysis with clustered standard errors clustered_plan = AnalysisPlan.from_metrics_dict({     'metrics': [         {             'alias': 'purchase_amount',             'name': 'purchase_amount',             'metric_type': 'simple'         },     ],     'variants': [         {'name': 'control', 'is_control': True},         {'name': 'treatment', 'is_control': False},     ],     'variant_col': 'variant',     'analysis_type': 'clustered_ols',  # Clustered OLS (CORRECT!)     'analysis_config': {         'cluster_cols': ['store_id']  # Specify the clustering variable     } })  clustered_results = clustered_plan.analyze(data).to_dataframe() print(\"=== Correct Analysis (With Clustering) ===\") print(f\"Treatment Effect: ${clustered_results.iloc[0]['ate']:.2f}\") print(f\"Standard Error: ${clustered_results.iloc[0]['std_error']:.2f}\") print(f\"P-value: {clustered_results.iloc[0]['p_value']:.4f}\") print(f\"95% CI: [${clustered_results.iloc[0]['ate_ci_lower']:.2f}, ${clustered_results.iloc[0]['ate_ci_upper']:.2f}]\")  <pre>=== Correct Analysis (With Clustering) ===\nTreatment Effect: $4.26\nStandard Error: $3.04\nP-value: 0.1610\n95% CI: [$-1.70, $10.21]\n</pre> In\u00a0[14]: Copied! <pre>comparison = pd.DataFrame({\n    'Method': ['Naive (OLS)', 'Correct (Clustered OLS)'],\n    'Treatment Effect': [\n        f\"${naive_results.iloc[0]['ate']:.2f}\",\n        f\"${clustered_results.iloc[0]['ate']:.2f}\"\n    ],\n    'Standard Error': [\n        f\"${naive_results.iloc[0]['std_error']:.2f}\",\n        f\"${clustered_results.iloc[0]['std_error']:.2f}\"\n    ],\n    'P-value': [\n        f\"{naive_results.iloc[0]['p_value']:.4f}\",\n        f\"{clustered_results.iloc[0]['p_value']:.4f}\"\n    ],\n    '95% CI': [\n        f\"[${naive_results.iloc[0]['ate_ci_lower']:.2f}, ${naive_results.iloc[0]['ate_ci_upper']:.2f}]\",\n        f\"[${clustered_results.iloc[0]['ate_ci_lower']:.2f}, ${clustered_results.iloc[0]['ate_ci_upper']:.2f}]\"\n    ]\n})\n\nprint(\"\\n=== Comparison ===\")\nprint(comparison.to_string(index=False))\nprint(\"\\nNotice: The clustered standard errors are LARGER, reflecting the\")\nprint(\"additional uncertainty from intra-cluster correlation.\")\n</pre> comparison = pd.DataFrame({     'Method': ['Naive (OLS)', 'Correct (Clustered OLS)'],     'Treatment Effect': [         f\"${naive_results.iloc[0]['ate']:.2f}\",         f\"${clustered_results.iloc[0]['ate']:.2f}\"     ],     'Standard Error': [         f\"${naive_results.iloc[0]['std_error']:.2f}\",         f\"${clustered_results.iloc[0]['std_error']:.2f}\"     ],     'P-value': [         f\"{naive_results.iloc[0]['p_value']:.4f}\",         f\"{clustered_results.iloc[0]['p_value']:.4f}\"     ],     '95% CI': [         f\"[${naive_results.iloc[0]['ate_ci_lower']:.2f}, ${naive_results.iloc[0]['ate_ci_upper']:.2f}]\",         f\"[${clustered_results.iloc[0]['ate_ci_lower']:.2f}, ${clustered_results.iloc[0]['ate_ci_upper']:.2f}]\"     ] })  print(\"\\n=== Comparison ===\") print(comparison.to_string(index=False)) print(\"\\nNotice: The clustered standard errors are LARGER, reflecting the\") print(\"additional uncertainty from intra-cluster correlation.\")  <pre>\n=== Comparison ===\n                 Method Treatment Effect Standard Error P-value           95% CI\n            Naive (OLS)            $4.26          $0.63  0.0000   [$3.03, $5.48]\nCorrect (Clustered OLS)            $4.26          $3.04  0.1610 [$-1.70, $10.21]\n\nNotice: The clustered standard errors are LARGER, reflecting the\nadditional uncertainty from intra-cluster correlation.\n</pre>"},{"location":"examples/cluster_randomization.html#cluster-randomization-example","title":"Cluster Randomization Example\u00b6","text":"<p>This notebook demonstrates how to analyze a cluster-randomized experiment where randomization occurs at the group level (e.g., stores, cities, schools) rather than at the individual level.</p>"},{"location":"examples/cluster_randomization.html#why-cluster-randomization","title":"Why Cluster Randomization?\u00b6","text":"<p>Cluster randomization is necessary when:</p> <ol> <li>Spillover Effects: Treatment of one individual affects others (e.g., testing driver incentives in ride-sharing)</li> <li>Operational Constraints: You can't randomize at the individual level (e.g., testing a store layout)</li> <li>Cost Efficiency: It's cheaper to randomize groups than individuals</li> </ol>"},{"location":"examples/cluster_randomization.html#key-consideration","title":"Key Consideration\u00b6","text":"<p>With cluster randomization, you need to account for intra-cluster correlation - observations within the same cluster are more similar than observations from different clusters. This requires using clustered standard errors or cluster-level analysis methods.</p>"},{"location":"examples/cluster_randomization.html#setup","title":"Setup\u00b6","text":""},{"location":"examples/cluster_randomization.html#1-simulate-cluster-randomized-experiment","title":"1. Simulate Cluster-Randomized Experiment\u00b6","text":"<p>Let's simulate an experiment where we test a promotional campaign across different stores. Each store is randomly assigned to control or treatment, and we observe multiple transactions per store.</p>"},{"location":"examples/cluster_randomization.html#2-naive-analysis-wrong","title":"2. Naive Analysis (WRONG!)\u00b6","text":"<p>First, let's see what happens if we ignore the clustering and use standard OLS. This is wrong because it doesn't account for intra-cluster correlation and will give you incorrect standard errors (typically too small, leading to false positives).</p>"},{"location":"examples/cluster_randomization.html#3-correct-analysis-with-clustered-standard-errors","title":"3. Correct Analysis with Clustered Standard Errors\u00b6","text":"<p>Now let's do the correct analysis by accounting for the clustering. We'll use <code>clustered_ols</code> which computes cluster-robust standard errors.</p>"},{"location":"examples/cluster_randomization.html#4-compare-results","title":"4. Compare Results\u00b6","text":"<p>Let's compare the two approaches side by side:</p>"},{"location":"examples/cluster_randomization.html#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>Always account for clustering in your analysis when randomization happens at the cluster level</li> <li>Clustered standard errors are typically larger than naive standard errors</li> <li>Ignoring clustering leads to overstated confidence - you might claim significance when there isn't any</li> <li>Use <code>clustered_ols</code> analysis type and specify <code>cluster_cols</code> in the analysis config</li> </ol>"},{"location":"examples/cluster_randomization.html#when-to-use-clustering","title":"When to Use Clustering\u00b6","text":"<p>Use clustered analysis when:</p> <ul> <li>\u2705 Randomization is at the group level (stores, cities, schools)</li> <li>\u2705 There are spillover effects between individuals</li> <li>\u2705 Observations within groups are more similar than across groups</li> </ul> <p>Don't use clustering when:</p> <ul> <li>\u274c Randomization is truly at the individual level</li> <li>\u274c There's no reason to believe observations are correlated within groups</li> </ul>"},{"location":"examples/simple_ab_test.html","title":"Simple A/B Test Example","text":"In\u00a0[3]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom cluster_experiments import AnalysisPlan\n\n# random seed\nnp.random.seed(42)\n</pre> import pandas as pd import numpy as np import warnings warnings.filterwarnings('ignore')  from cluster_experiments import AnalysisPlan  # random seed np.random.seed(42)  In\u00a0[4]: Copied! <pre>n_users = 2000\n\n# Create base data\ndata = pd.DataFrame({\n    'user_id': range(n_users),\n    'variant': np.random.choice(['control', 'treatment'], n_users),\n    'visits': np.random.poisson(10, n_users),  # Number of visits\n})\n\n# Simulate conversions (more likely for treatment)\ndata['converted'] = (\n    np.random.binomial(1, 0.10, n_users) |  # Base conversion rate\n    (data['variant'] == 'treatment') &amp; np.random.binomial(1, 0.03, n_users)  # +3% for treatment\n).astype(int)\n\n# Simulate revenue (higher for converters and treatment)\ndata['revenue'] = 0.0\nconverters = data['converted'] == 1\ndata.loc[converters, 'revenue'] = np.random.gamma(shape=2, scale=25, size=converters.sum())\n\n# Treatment group gets slightly higher revenue\ntreatment_converters = (data['variant'] == 'treatment') &amp; converters\ndata.loc[treatment_converters, 'revenue'] *= 1.15\n\nprint(f\"Dataset shape: {data.shape}\")\nprint(f\"\\nFirst few rows:\")\ndata.head(10)\n</pre> n_users = 2000  # Create base data data = pd.DataFrame({     'user_id': range(n_users),     'variant': np.random.choice(['control', 'treatment'], n_users),     'visits': np.random.poisson(10, n_users),  # Number of visits })  # Simulate conversions (more likely for treatment) data['converted'] = (     np.random.binomial(1, 0.10, n_users) |  # Base conversion rate     (data['variant'] == 'treatment') &amp; np.random.binomial(1, 0.03, n_users)  # +3% for treatment ).astype(int)  # Simulate revenue (higher for converters and treatment) data['revenue'] = 0.0 converters = data['converted'] == 1 data.loc[converters, 'revenue'] = np.random.gamma(shape=2, scale=25, size=converters.sum())  # Treatment group gets slightly higher revenue treatment_converters = (data['variant'] == 'treatment') &amp; converters data.loc[treatment_converters, 'revenue'] *= 1.15  print(f\"Dataset shape: {data.shape}\") print(f\"\\nFirst few rows:\") data.head(10)  <pre>Dataset shape: (2000, 5)\n\nFirst few rows:\n</pre> Out[4]: user_id variant visits converted revenue 0 0 control 7 1 90.366149 1 1 treatment 14 0 0.000000 2 2 control 13 0 0.000000 3 3 control 7 0 0.000000 4 4 control 16 0 0.000000 5 5 treatment 7 0 0.000000 6 6 control 15 0 0.000000 7 7 control 12 0 0.000000 8 8 control 16 0 0.000000 9 9 treatment 8 0 0.000000 In\u00a0[5]: Copied! <pre>from cluster_experiments import (\n    AnalysisPlan, SimpleMetric, RatioMetric, \n    Variant, HypothesisTest\n)\n\n# Define metrics by type\nsimple_metrics = {\n    \"conversions\": \"converted\",  # alias: column_name\n    \"revenue\": \"revenue\"\n}\n\nratio_metrics = {\n    \"conversion_rate\": {\n        \"numerator\": \"converted\",\n        \"denominator\": \"visits\"\n    }\n}\n\n# Define variants\nvariants = [\n    Variant(\"control\", is_control=True),\n    Variant(\"treatment\", is_control=False)\n]\n\n# Build hypothesis tests\nhypothesis_tests = []\n\n# Ratio metrics: use delta method\nfor alias, config in ratio_metrics.items():\n    metric = RatioMetric(\n        alias=alias,\n        numerator_name=config[\"numerator\"],\n        denominator_name=config[\"denominator\"]\n    )\n    hypothesis_tests.append(\n        HypothesisTest(\n            metric=metric,\n            analysis_type=\"delta\",\n            analysis_config={\n                \"scale_col\": metric.denominator_name,\n                \"cluster_cols\": [\"user_id\"]\n            }\n        )\n    )\n\n# Simple metrics: use OLS\nfor alias, column_name in simple_metrics.items():\n    metric = SimpleMetric(alias=alias, name=column_name)\n    hypothesis_tests.append(\n        HypothesisTest(\n            metric=metric,\n            analysis_type=\"ols\"\n        )\n    )\n\n# Create analysis plan\nanalysis_plan = AnalysisPlan(\n    tests=hypothesis_tests,\n    variants=variants,\n    variant_col='variant'\n)\n\nprint(\"Analysis plan created successfully!\")\n</pre> from cluster_experiments import (     AnalysisPlan, SimpleMetric, RatioMetric,      Variant, HypothesisTest )  # Define metrics by type simple_metrics = {     \"conversions\": \"converted\",  # alias: column_name     \"revenue\": \"revenue\" }  ratio_metrics = {     \"conversion_rate\": {         \"numerator\": \"converted\",         \"denominator\": \"visits\"     } }  # Define variants variants = [     Variant(\"control\", is_control=True),     Variant(\"treatment\", is_control=False) ]  # Build hypothesis tests hypothesis_tests = []  # Ratio metrics: use delta method for alias, config in ratio_metrics.items():     metric = RatioMetric(         alias=alias,         numerator_name=config[\"numerator\"],         denominator_name=config[\"denominator\"]     )     hypothesis_tests.append(         HypothesisTest(             metric=metric,             analysis_type=\"delta\",             analysis_config={                 \"scale_col\": metric.denominator_name,                 \"cluster_cols\": [\"user_id\"]             }         )     )  # Simple metrics: use OLS for alias, column_name in simple_metrics.items():     metric = SimpleMetric(alias=alias, name=column_name)     hypothesis_tests.append(         HypothesisTest(             metric=metric,             analysis_type=\"ols\"         )     )  # Create analysis plan analysis_plan = AnalysisPlan(     tests=hypothesis_tests,     variants=variants,     variant_col='variant' )  print(\"Analysis plan created successfully!\")  <pre>Analysis plan created successfully!\n</pre> In\u00a0[6]: Copied! <pre># Run analysis\nresults = analysis_plan.analyze(data)\n\n# View results as a dataframe\nresults_df = results.to_dataframe()\nprint(\"\\n=== Experiment Results ===\")\nresults_df\n</pre> # Run analysis results = analysis_plan.analyze(data)  # View results as a dataframe results_df = results.to_dataframe() print(\"\\n=== Experiment Results ===\") results_df  <pre>\n=== Experiment Results ===\n</pre> Out[6]: metric_alias control_variant_name treatment_variant_name control_variant_mean treatment_variant_mean analysis_type ate ate_ci_lower ate_ci_upper p_value std_error dimension_name dimension_value alpha 0 conversion_rate control treatment 0.009972 0.011912 delta 0.001940 -0.000825 0.004706 0.169006 0.001411 __total_dimension total 0.05 1 conversions control treatment 0.100394 0.117886 ols 0.017492 -0.009874 0.044859 0.210285 0.013963 __total_dimension total 0.05 2 revenue control treatment 5.451515 7.359327 ols 1.907812 -0.130488 3.946112 0.066581 1.039968 __total_dimension total 0.05"},{"location":"examples/simple_ab_test.html#simple-ab-test-example","title":"Simple A/B Test Example\u00b6","text":"<p>This notebook demonstrates a basic A/B test analysis using <code>cluster-experiments</code>.</p>"},{"location":"examples/simple_ab_test.html#overview","title":"Overview\u00b6","text":"<p>We'll simulate an experiment where we test a new feature's impact on:</p> <ul> <li>Conversions (simple metric): Whether a user made a purchase</li> <li>Conversion Rate (ratio metric): Conversions per visit</li> <li>Revenue (simple metric): Total revenue generated</li> </ul>"},{"location":"examples/simple_ab_test.html#setup","title":"Setup\u00b6","text":""},{"location":"examples/simple_ab_test.html#1-generate-simulated-experiment-data","title":"1. Generate Simulated Experiment Data\u00b6","text":"<p>Let's create a dataset with control and treatment groups.</p>"},{"location":"examples/simple_ab_test.html#2-define-analysis-plan","title":"2. Define Analysis Plan\u00b6","text":"<p>Now let's define our analysis plan with multiple metrics:</p> <ul> <li>conversions: Simple metric counting total conversions</li> <li>conversion_rate: Ratio metric (conversions / visits)</li> <li>revenue: Simple metric for total revenue</li> </ul>"},{"location":"examples/simple_ab_test.html#3-run-analysis","title":"3. Run Analysis\u00b6","text":"<p>Let's run the analysis and generate a comprehensive scorecard.</p>"},{"location":"examples/simple_ab_test.html#summary","title":"Summary\u00b6","text":"<p>This example demonstrated:</p> <ol> <li>\u2705 Data Simulation: Creating realistic experiment data</li> <li>\u2705 Multiple Metric Types: Analyzing both simple and ratio metrics</li> <li>\u2705 Easy Configuration: Using dictionary-based analysis plan setup</li> <li>\u2705 Comprehensive Results: Getting treatment effects, confidence intervals, and p-values</li> </ol>"},{"location":"examples/simple_ab_test.html#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Try the CUPAC example to learn about variance reduction</li> <li>Explore cluster randomization for handling correlated units</li> <li>Learn about switchback experiments for time-based designs</li> </ul>"}]}